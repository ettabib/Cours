\section{Application aux endomorphismes continus et aux matrices}

\subsection{Calcul fonctionnel et premières applications}

Soit $E$ un $K$-espace vectoriel normé complet. Si $u$ est un endomorphisme continu de $E$, on pose $\|u\| = \sup_{x \neq 0} \frac{\|u(x)\|}{\|x\|}$. On sait que $\forall x \in E, \|u(x)\| \leq \|u\| \|x\|$.

Soit $\mathcal{L}(E)$ l'algèbre des endomorphismes continus sur $E$. On sait que $(\mathcal{L}(E),\|.\|)$ est un espace vectoriel normé complet et que $\forall u,v \in \mathcal{L}(E), \|v \circ u\| \leq \|v\| \|u\|$.
En particulier, par une récurrence évidente sur $n$, on a

$\forall n \in \mathbb{N}, \forall u \in \mathcal{L}(E), \|u^n\| \leq \|u\|^n$

\begin{prop}
Soit $\sum a_n z^n$ une série entière à coefficients dans $K$ de rayon de convergence $R > 0$ et soit $u \in \mathcal{L}(E)$ tel que $\|u\| < R$. Alors la série $\sum a_n u^n$ est absolument convergente.
\end{prop}

\begin{proof}
On a $\|a_n u^n\| \leq |a_n| \|u\|^n$ et comme $\|u\| < R$, la série $\sum |a_n| \|u\|^n$ est convergente.
\end{proof}

\begin{rem}
Bien entendu, en introduisant la somme $\sum_{n=0}^{+\infty} a_n u^n$, on espère que bon nombre des propriétés formelles de la somme $S(z) = \sum_{n=0}^{+\infty} a_n z^n$, valables pour $z \in D(0,R)$, se transmettront à $\sum_{n=0}^{+\infty} a_n u^n$.
\end{rem}

Donnons une première application de ce calcul fonctionnel qui généralise l'identité $(1-z)\sum_{n=0}^{+\infty} z^n = 1$ :

\begin{prop}
\index{automorphisme continu}
L'ensemble des automorphismes continus de $E$ est un ouvert de $\mathcal{L}(E)$.
\end{prop}

\begin{proof}
Soit $u \in \mathcal{L}(E)$ tel que $\|u\| < 1$. D'après la proposition précédente, la série $\sum_{n \geq 0} u^n$ converge absolument. Soit $s$ sa somme. On a

$(\mathrm{Id}_E - u) \cdot \left(\sum_{n=0}^N u^n\right) = \left(\sum_{n=0}^N u^n\right) \cdot (\mathrm{Id}_E - u) = \mathrm{Id}_E - u^{n+1}$

En faisant tendre $n$ vers $+\infty$, on a $(\mathrm{Id}_E - u) \cdot s = s \cdot (\mathrm{Id}_E - u) = \mathrm{Id}_E$, ce qui montre que $\mathrm{Id}_E - u$ est un automorphisme continu de $E$ d'inverse $s$. Soit maintenant $v$ un automorphisme continu de $E$ et $u \in \mathcal{L}(E)$. On écrit $v + u = v \cdot (\mathrm{Id}_E + v^{-1} \cdot u)$. D'après les préliminaires, $\mathrm{Id}_E + v^{-1} \cdot u$ (et donc $v + u$) est un automorphisme continu de $E$ dès que $\|v^{-1} \cdot u\| < 1$ et donc dès que $\|u\| < \frac{1}{\|v^{-1}\|}$.
On en déduit que la boule $B(v, \frac{1}{\|v^{-1}\|})$ est contenue dans l'ensemble des automorphismes continus de $E$, qui est donc ouvert.
\end{proof}

\subsection{Exponentielle d'un endomorphisme ou d'une matrice}

\begin{de}
\index{exponentielle d'un endomorphisme}
Si $u \in \mathcal{L}(E)$, on pose $\exp(u) = \sum_{n=0}^{+\infty} \frac{u^n}{n!}$ (série absolument convergente)
\end{de}

\begin{proof}
La série entière $\sum_{n \geq 0} \frac{z^n}{n!}$ étant de rayon de convergence infinie, la série $\sum_{n \geq 0} \frac{u^n}{n!}$ est absolument convergente quelle que soit la norme de $u \in \mathcal{L}(E)$.
\end{proof}

\begin{rem}
\index{exponentielle d'une matrice}
De même, si $A \in M_p(K)$, on définit de la même façon $\exp(A) = e^A = \sum_{n=0}^{+\infty} \frac{A^n}{n!}$. On a bien entendu $\mathrm{Mat}(\exp(u),\mathcal{E}) = \exp(\mathrm{Mat}(u,\mathcal{E}))$ si $\mathcal{E}$ est une base de $E$ de dimension finie.
\end{rem}

\begin{prop}
\begin{enumerate}
\item Pour tout automorphisme continu $v$ de $E$, on a $\exp(v^{-1} \cdot u \cdot v) = v^{-1} \cdot \exp(u) \cdot v$
\item Si $u,v \in \mathcal{L}(E)$ commutent, alors $\exp(u + v) = \exp(u) \cdot \exp(v) = \exp(v) \cdot \exp(u)$ ; en particulier, pour tout $u \in \mathcal{L}(E)$, $\exp(u)$ est un automorphisme continu de $E$ et $(\exp(u))^{-1} = \exp(-u)$
\item L'application $\mathbb{R} \mapsto \mathcal{L}(E), t \mapsto \exp(tu)$ est de classe $C^{\infty}$ et on a

$\forall n \in \mathbb{N}, \frac{d^n}{dt^n} \exp(tu) = u^n \cdot \exp(tu) = \exp(tu) \cdot u^n$
\end{enumerate}
\end{prop}

\begin{proof}
(i) On a $\sum_{n=0}^N \frac{(v^{-1} \cdot u \cdot v)^n}{n!} = \sum_{n=0}^N \frac{v^{-1} \cdot u^n \cdot v}{n!} = v^{-1} \cdot \left(\sum_{n=0}^N \frac{u^n}{n!}\right) \cdot v$ et en faisant tendre $N$ vers $+\infty$, on obtient $\exp(v^{-1} \cdot u \cdot v) = v^{-1} \cdot \exp(u) \cdot v$.

(ii) Si $u,v \in \mathcal{L}(E)$ commutent, on pose $a_n = \frac{u^n}{n!}$ et $b_n = \frac{v^n}{n!}$. Ces séries sont absolument convergentes. On peut donc faire le produit de Cauchy de ces deux séries et on a alors $c_n = \sum_{k=0}^n \frac{1}{k!(n-k)!} u^k v^{n-k} = \frac{1}{n!}(u+v)^n$ d'après la formule du binôme (car $u$ et $v$ commutent). On a donc

$\sum_{n=0}^{+\infty} \frac{(u+v)^n}{n!} = \left(\sum_{n=0}^{+\infty} \frac{u^n}{n!}\right) \cdot \left(\sum_{n=0}^{+\infty} \frac{v^n}{n!}\right)$

formule dans laquelle on peut également échanger $u$ et $v$. On a alors bien entendu $\exp(u) \cdot \exp(-u) = \exp(-u) \cdot \exp(u) = \exp(u-u) = \exp(0) = \mathrm{Id}_E$, ce qui montre que $\exp(u)$ est un automorphisme continu de $E$ et que $(\exp(u))^{-1} = \exp(-u)$

(iii) On a $\exp(tu) = \sum_{k=0}^{+\infty} \frac{u^k}{k!} t^k$, série entière en $t$ de rayon de convergence infini puisqu'elle converge pour tout $t$. Sa somme est donc de classe $C^{\infty}$ et (en dérivant terme à terme cette série entière) on a

\begin{align*}
\frac{d^n}{dt^n} \exp(tu) &= \sum_{k=n}^{+\infty} \frac{u^k}{(k-n)!} t^{k-n} = u^n \cdot \sum_{k=n}^{+\infty} \frac{u^{k-n}}{(k-n)!} t^{k-n} \\
&= u^n \cdot \exp(tu)
\end{align*}

Mais $\exp(tu)$ et $u$ commutent évidemment, d'où $\frac{d^n}{dt^n} \exp(tu) = u^n \cdot \exp(tu) = \exp(tu) \cdot u^n$.
\end{proof}

Bien entendu, ce théorème a sa traduction matricielle et on a

\begin{thm}
\begin{enumerate}
\item $\forall A \in M_p(K), \forall P \in GL_p(K)$,

$\exp(P^{-1} A P) = P^{-1} \exp(A) P$

\item Si $A,B \in M_p(K)$ commutent, alors $\exp(A+B) = \exp(A) \exp(B) = \exp(B) \exp(A)$ ; en particulier, pour tout $A \in M_p(K)$, $\exp(A)$ est dans $GL_p(K)$ et $(\exp(A))^{-1} = \exp(-A)$

\item L'application $\mathbb{R} \mapsto M_p(K), t \mapsto \exp(tA)$ est de classe $C^{\infty}$ et on a

$\forall n \in \mathbb{N}, \frac{d^n}{dt^n} \exp(tA) = A^n \exp(tA) = \exp(tA) A^n$
\end{enumerate}
\end{thm}

La première propriété montre en particulier que si $A$ est diagonalisable, on a $A = P \mathrm{diag}(\lambda_1,\ldots,\lambda_p) P^{-1}$, et donc $\exp(A) = P \mathrm{diag}(e^{\lambda_1},\ldots,e^{\lambda_p}) P^{-1}$.

Si $A$ est nilpotente d'indice $r$, on a $\exp(A) = \sum_{n=0}^{r-1} \frac{A^n}{n!}$.

Si $A \in M_p(\mathbb{C})$ est quelconque, on a la décomposition de Jordan $A = D + N$ avec $D$ diagonalisable, $N$ nilpotente et $DN = ND$. On a donc d'après la propriété (ii) ci-dessus $\exp(A) = \exp(D) \exp(N)$ ce qui permet le calcul de $\exp(A)$.

Une autre manière de voir, est d'introduire les sous-espaces caractéristiques de $u \in L(E)$. Soit $\lambda_1,\ldots,\lambda_k$ les valeurs propres distinctes de $u$ et $E_i$ le sous-espace caractéristique de $u$ associé à $\lambda_i$. Soit $u_i$ la restriction de $u$ à $E_i$, $\pi_i$ la projection sur $E_i$ parallèlement à $\bigoplus_{j \neq i} E_j$. On a évidemment $\exp(tu)|_{E_i} = \exp(tu_i)$ et donc $\exp(tu) = \sum_{i=1}^k \exp(tu_i) \cdot \pi_i$. Mais $u_i = \lambda_i \mathrm{Id} + n_i$ avec $n_i$ nilpotent. On a donc $\exp(tu_i) = e^{t\lambda_i} \sum_{k=0}^{r_i-1} t^k n_i^k$. On en déduit que $\exp(tu) = \sum_{i=0}^k e^{t\lambda_i} \sum_{k=0}^{r_i-1} t^k v_{i,k}$, avec $v_{i,k} = n_i^k \cdot \pi_i$ ce qui donne la forme générique de $\exp(tu)$ sous forme de sommes de produits de fonctions exponentielles par des fonctions polynomiales.

\subsection{Application aux systèmes différentiels homogènes à coefficients constants}

Soit $A \in M_p(K)$ et le système différentiel à coefficients constants

$\frac{dX}{dt} = AX \Leftrightarrow \begin{cases} 
\frac{dx_1}{dt} = a_{11} x_1 + \ldots + a_{1p} x_p \\
\vdots \\
\frac{dx_p}{dt} = a_{p1} x_1 + \ldots + a_{pp} x_p 
\end{cases}$

\begin{thm}
\index{solution d'un système différentiel homogène}
Soit $X_0 \in M_{p,1}(K)$. L'unique solution du système homogène $\frac{dX}{dt} = AX$ vérifiant $X(0) = X_0$ est l'application $t \mapsto \exp(tA) X_0$.
\end{thm}

\begin{proof}
Cette application convient évidemment puisque $\frac{d}{dt}(\exp(tA)X_0) = A\exp(tA)X_0$. Soit $t \mapsto X(t)$ une autre solution et soit $Y(t) = \exp(-tA)X(t)$. On a $Y'(t) = -\exp(-tA)AX(t) + \exp(-tA)X'(t) = \exp(-tA)(X'(t) - AX(t)) = 0$. On en déduit que $Y$ est constante égale à $Y(0)$. Mais $Y(0) = X_0$. On a donc $Y(t) = X_0$ soit encore $X(t) = \exp(tA)X_0$.
\end{proof}

\begin{rem}
En particulier, si $K = \mathbb{C}$, la discussion précédente montre que les fonctions $x_1,\ldots,x_p$ sont des exponentielles polynômes.
\end{rem}

\index{exponentielle d'un endomorphisme}
\index{exponentielle d'une matrice}
\index{système différentiel homogène}
\index{exponentielles polynômes}