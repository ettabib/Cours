\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Polynomes d'endomorphismes},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.paragraphHead, .likeparagraphHead { margin-top:2em; font-weight: bold;}
.subparagraphHead, .likesubparagraphHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:justify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.subsubsectionToc, .likesubsubsectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Polynomes d'endomorphismes}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

[
[
[]
[

\subsubsection{3.2 Polynômes d'endomorphismes}

\paragraph{3.2.1 Généralités}

Soit E un K-espace vectoriel et u \in L(E). On pose u^0 =
\mathrmId_E et pour k ≥ 1, u^k = u
\cdot u^k-1. Si P \in K[X], P =\
\sum ~
_k=0^da_kX^k, on pose

P(u) = \sum _k=0^da_
ku^k

Proposition~3.2.1 L'application P\mapsto~P(u) est un
morphisme de K-algèbres de K[X] dans L(E). Son image K[u] est la
plus petite sous-algèbre de L(E) contenant u~; elle est commutative.

Démonstration Vérifications immédiates.

\paragraph{3.2.2 Idéal annulateur. Polynôme minimal}

Définition~3.2.1 On appelle idéal annulateur de u \in L(E) l'idéal
I_u = \P \in
K[X]∣P(u) = 0\. On dit
que u admet un polynôme minimal si
I_u\neq~\0\.
Dans ce cas I_u est engendré par un unique polynôme normalisé
\mu_u(X) appelé le polynôme minimal de u.

Exemple~3.2.1 Si E = K[X] et u :
P(X)\mapsto~XP(X), on a Q(u) :
P(X)\mapsto~Q(X)P(X) soit
Q(u)\neq~0 et donc u n'admet pas de polynôme
minimal. Ce phénomène ne peut pas se produire en dimension finie

Théorème~3.2.2 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors u admet un polynôme minimal.

Démonstration Premier argument~: comme dim~
K[X] = +\infty~ et dim~ L(E) < +\infty~,
l'application P(X)\mapsto~P(u) ne peut être
injective. Deuxième argument~: comme dim~ L(E)
= n^2 (où n = dim~ E), la famille de
cardinal n^2 + 1,
(u^k)_0\leqk\leqn^2 doit être liée~; il existe
donc
\lambda_0,\\ldots,\lambda_n^2~
non tous nuls tels que \lambda_0u^0 +
\\ldots~ +
\lambda_n^2u^n^2  = 0~; le polynôme
\lambda_01 +
\\ldots~ +
\lambda_n^2X^n^2  n'est pas nul et
il est dans I_u.

On suppose désormais dim~ E < +\infty~

Proposition~3.2.3 Soit F un sous-espace de E stable par u et v
l'endomorphisme de F induit par u. Alors \mu_v divise
\mu_u.

Démonstration On vérifie facilement que pour tout k dans \mathbb{N}~,
v^k est l'endomorphisme de F induit par u^k, donc
si P(X) \in K[X], P(v) l'endomorphisme de F induit par P(u). En
particulier \mu_u(v) est l'endomorphisme de F induit par
\mu_u(u) = 0 donc \mu_u(v) = 0 et \mu_v divise
\mu_u.

Théorème~3.2.4 Les racines de \mu_u sont exactement les valeurs
propres de u.

Démonstration Soit \lambda~ une valeur propre de u et x un vecteur propre
associé. On a \forall~k \in \mathbb{N}~, u^k~(x) =
\lambda~^kx et donc \forall~~P \in K[X], P(u)(x)
= P(\lambda~)x. En particulier 0 = \mu_u(u)(x) = \mu_u(\lambda~)x et
comme x\neq~0, on a \mu_u(\lambda~) = 0.
Inversement soit \lambda~ une racine de \mu_u et écrivons
\mu_u(X) = (X - \lambda~)Q(X). Supposons que \lambda~ n'est pas valeur propre
de u. On a 0 = \mu_u(u) = (u -
\lambda~\mathrmId_E) \cdot Q(u). Mais comme \lambda~ n'est pas
valeur propre de u, u - \lambda~\mathrmId_E est
injectif et donc Q(u) = 0. Ceci impose que \mu_u divise Q ce qui
est impossible pour une question de degrés.

\paragraph{3.2.3 Théorème de Cayley-Hamilton}

Théorème~3.2.5 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors \chi_u(u) = 0.

Remarque~3.2.1 Une version équivalente est~: soit M \in M_K(n).
Alors \chi_M(M) = 0.

Démonstration Démonstration 1. Soit x \in E,
x\neq~0 et soit d =\
max\k∣(x,u(x),\\ldots,u^k-1~(x))\text
libre \. On a alors u^d(x) = \lambda_0x
+ \\ldots~ +
\lambda_d-1u^d-1(x). Soit E_x
=\
\mathrmVect(x,u(x),\\ldots,u^d-1~(x)).
Alors E_x est stable par u. Soit v la restriction de u à
E_x. \mathcal{E}_x =
(x,u(x),\\ldots,u^d-1~(x))
est une base de E_x et

\mathrmMat (v,\mathcal{E}_x~)
= \left
(\matrix\,0&0&\\ldots&\\\ldots&\lambda_0~
\cr
1&0&\\ldots&\\\ldots&\lambda_1~
\cr
\\ldots&⋱&\mathrel⋱&\\\ldots&\\\ldots~
\cr
\\ldots&\\\ldots&⋱&\mathrel⋱&\\\ldots~
\cr
0&\\ldots&\\\ldots&1&\lambda_d-1~\right
)

Un calcul simple montre que \chi_v(X) = X^d -
\lambda_d-1X^d-1
-\\ldots~ -
\lambda_0. On a donc \chi_v(v)(x) = 0 et donc
\chi_v(u)(x) = 0. Mais \chi_v divise \chi_u et donc on
a aussi \chi_u(u)(x) = 0. Comme x est quelconque, la relation
\chi_u(u)(x) = 0 étant évidente si x = 0, on a \chi_u(u) =
0.

Démonstration 2. Montrons que si K est un sous-corps de \mathbb{C} et M \in
M_K(n), alors \chi_M(M) = 0. Il suffit bien entendu de le
montrer lorsque M \in M_\mathbb{C}(n). Si M est diagonalisable, on a M =
P^-1DP avec D =\
\mathrmdiag(\lambda_1,\\ldots,\lambda_n~).
On a alors \chi_M(M) = \chi_D(M) =
P^-1\chi_D(D)P =
P^-1\
\mathrmdiag(\chi_D(\lambda_1),\\ldots,\chi_D(\lambda_n~))P
= P^-10P = 0. Si M n'est pas diagonalisable, soit
M_n une suite de matrices diagonalisables qui converge vers M.
Comme l'application A\mapsto~\chi_A(A) est
polynomiale en les coefficients de A, elle est continue et on a

\chi_M(M) =\
lim\chi_M_n(M_n) =\
lim0 = 0

Corollaire~3.2.6 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors \mu_u divise \chi_u.

\paragraph{3.2.4 Polynôme annulateur et trigonalisation}

Théorème~3.2.7 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors u est trigonalisable si et seulement si il existe un
polynôme P \in K[X] \diagdown 0, scindé sur K tel que P(u) = 0.

Démonstration Si u est trigonalisable, le polynôme caractéristique
\chi_u de u est scindé et vérifie \chi_u(u) = 0.

Inversement, supposons qu'il existe un polynôme non nul, scindé P tel
que P(u) = 0. On peut supposer que P est normalisé si bien que l'on peut
écrire P = \∏ ~
_i=1^k(X - \lambda_i). On a 0
= \∏ ~
_i=1^k(u - \lambda_i\mathrmId), si
bien que l'un au moins des u - \lambda_i\mathrmId
est non injectif. Donc u possède au moins une valeur propre.

Montrons donc le résultat par récurrence sur n =\
dim E~; il n'y a rien à démontrer si n = 1. Soit \lambda~ une valeur propre
de u. Soit e_1 un vecteur propre associé à \lambda~, que l'on complète
en
(e_1,\\ldots,e_n~)
base de E. Soit F =\
\mathrmVect(e_2,\\ldots,e_n~),
p la projection sur F parallèlement à Ke_1 et v : F \rightarrow~ F défini
par v(x) = p(u(x)) si x \in F. Alors M =\
\mathrmMat (u,\mathcal{E}) = \left
(\matrix\,\lambda~&∗∗∗ \cr
\matrix\,0 \cr
\⋮~ \cr
0&A \right ) avec A =\
\mathrmMat
(v,(e_2,\\ldots,e_n~)).
Le produit de matrices par blocs et une récurrence élémentaire montrent
que \forall~~p \in \mathbb{N}~,

M^p = \left
(\matrix\,\lambda~&∗∗∗ \cr
\matrix\,0 \cr
\⋮~ \cr
0&A^p \right )

et par combinaisons linéaires que

P(M) = \left
(\matrix\,\lambda~&∗ ∗ ∗ \cr
\matrix\,0 \cr
\⋮~ \cr
0&P(A)\right )

On en déduit que P(A) = 0 et comme P(A) =\
\mathrmMat
(P(v),(e_2,\\ldots,e_n~)),
on a aussi P(v) = 0. Par hypothèse de récurrence, il existe une base
(\epsilon_2,\\ldots,\epsilon_n~)
de F telle que \mathrmMat~
(v,(\epsilon_2,\\ldots,\epsilon_n~))
soit triangulaire supérieure et alors
\mathrmMat~
(u,(e_1,\epsilon_2,\\ldots,\epsilon_n~))
= \left (\matrix\,\lambda~&∗
\cr \matrix\,0
\cr \⋮~
\cr
0&\mathrmMat~
(v,(\epsilon_2,\\ldots,\epsilon_n))~\right
) est triangulaire supérieure.

\paragraph{3.2.5 Décomposition des noyaux}

Théorème~3.2.8 Soit E un K-espace vectoriel et u \in L(E). Soit P \in
K[X] et P =
P_1\\ldotsP_k~
une décomposition de P en produit de polynômes deux à deux premiers
entre eux. Alors
\mathrmKer~P(u)
= \mathrmKerP_1~(u)
\oplus~⋯
\oplus~\mathrmKerP_k~(u).

Démonstration Par récurrence sur k ≥ 2. Pour k = 2, écrivons P =
P_1P_2 avec P_1 et P_2 premiers
entre eux. Soit U et V tels que UP_1 + V P_2 = 1
(Bezout). On a déjà
\mathrmKerP_i~(u)
\subset~\mathrmKer~P(u). De plus on
a

U(u)P_1(u) + V (u)P_2(u) =
\mathrmId_E

Soit x
\in\mathrmKerP_1~(u)
\bigcap\mathrmKerP_2~(u).
On a x = U(u)P_1(u)(x) + V (u)P_2(u)(x) = 0 + 0 = 0,
donc
\mathrmKerP_1~(u)
\bigcap\mathrmKerP_2~(u)
= \0\. Soit maintenant x
\in\mathrmKer~P(u). On a
encore x = x_1 + x_2 avec x_1 = V
(u)P_2(u)(x) et x_2 = U(u)P_1(u)(x). Mais
alors

\begin{align*} P_1(u)(x_1)& =&
P_1(u)V (u)P_2(u)(x) = V
(u)(P_1P_2)(u)(x)\%& \\
& =& V (u)P(u)(x) = 0 \%& \\
\end{align*}

donc x_1
\in\mathrmKerP_1~(u).
De même x_2
\in\mathrmKerP_2~(u)
et donc \mathrmKer~P(u)
= \mathrmKerP_1~(u)
\oplus~\mathrmKerP_2~(u).
Supposons donc le résultat vrai pour k - 1. Comme P_k et
P_1\\ldotsP_k-1~
sont premiers entre eux, le cas k = 2 nous donne
\mathrmKer~P(u)
=\
\mathrmKerP_1\\ldotsP_k-1~(u)
\oplus~\mathrmKerP_k~(u)
puis grâce à l'hypothèse de récurrence
\mathrmKer~P(u)
= \mathrmKerP_1~(u)
\oplus~⋯
\oplus~\mathrmKerP_k~(u).

Corollaire~3.2.9 Soit E un K-espace vectoriel et u \in L(E). Soit P \in
K[X] et P =
P_1\\ldotsP_k~
une décomposition de P en produit de polynômes deux à deux premiers
entre eux. On suppose que P(u) = 0. Alors E =\
\mathrmKerP_1(u)
\oplus~⋯
\oplus~\mathrmKerP_k~(u).

Proposition~3.2.10

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) Soit E un K-espace vectoriel et u \in L(E) tel que u^2 =
  u. Alors u est un projecteur
\item
  (ii) Soit E un K-espace vectoriel et u \in L(E) tel que u^2 =
  \mathrmId. Si
  carK\mathrel\neq~~2, alors u
  est la symétrie par rapport à un sous-espace V parallèlement à un
  supplémentaire.
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) On écrit X^2 - X = X(X - 1)~: les polynômes X et X - 1
  sont premiers entre eux d'où E =\
  \mathrmKer(u -\mathrmId)
  \oplus~\mathrmKer~u. Alors u est
  la projection sur
  \mathrmKer~(u
  -\mathrmId) parallèlement à
  \mathrmKer~u.
\item
  (ii) On écrit X^2 - 1 = (X - 1)(X + 1)~: les polynômes X +
  1 et X - 1 sont premiers entre eux (si
  carK\mathrel\neq~~2) d'où E
  = \mathrmKer~(u
  -\mathrmId)
  \oplus~\mathrmKer~(u +
  \mathrmId). Alors u est la symétrie par rapport à
  \mathrmKer~(u
  -\mathrmId) parallèlement à
  \mathrmKer~(u +
  \mathrmId).
\end{itemize}

Théorème~3.2.11 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors u est diagonalisable si et seulement si il existe P \in
K[X] scindé à racines simples tel que P(u) = 0.

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  ( \rigtharrow~). Soit \mathcal{E} une base de E telle que D =\
  \mathrmMat (u,\mathcal{E}) soit diagonale, D
  =\
  \mathrmdiag(\lambda_1,\\ldots,\lambda_n~).
  Supposons que
  \lambda_1,\\ldots,\lambda_k~
  sont distinctes et que pour i ≥ k + 1, \lambda_i
  \in\\lambda_1,\\ldots\lambda_k\~.
  Soit P = \∏ ~
  _i=1^k(X - \lambda_i). On a P(D)
  =\
  \mathrmdiag(P(\lambda_1),\\ldots,P(\lambda_n~))
  = 0 et donc P(u) = 0 avec P scindé à racines simples.
\item
  ( ⇐) Soit P(X) =\ \∏
   _i=1^k(X - \lambda_i). On a donc E
  = \mathrmKer~(u -
  \lambda_1\mathrmId)
  \oplus~⋯
  \oplus~\mathrmKer~(u -
  \lambda_k\mathrmId). En réunissant des bases de
  tous ces sous-espaces, on obtient une base de E formée de vecteurs
  propres de u. Donc u est diagonalisable.
\end{itemize}

Remarque~3.2.2 Ce théorème peut encore s'exprimer sous la forme~: u est
diagonalisable si et seulement si \mu_u est scindé à racines
simples.

\paragraph{3.2.6 Sous-espaces caractéristiques}

Remarque~3.2.3 Soit E un K-espace vectoriel de dimension finie, u \in L(E)
et P tel que P(u) = 0. Soit P =
P_1\\ldotsP_k~
une décomposition de \chi_u en produit de polynômes deux à deux
premiers entre eux. Soit E_i =\
\mathrmKerP_i(u). On a E = E_1
\oplus~⋯ \oplus~ E_k et chacun des sous-espaces
E_i est stable par u. Soit u_i la restriction de u à
E_i. Dans une base \mathcal{E} = \mathcal{E}_1
\cup\\ldots~
\cup\mathcal{E}_k adaptée à la décomposition en somme directe, on a M
= \mathrmMat~ (u,\mathcal{E})
=\
\mathrmdiag(M_1,\\ldots,M_k~)
avec M_i =\
\mathrmMat (u_i,\mathcal{E}_i). On en
déduit (calcul par blocs d'un déterminant) que \chi_u
= \∏ ~
\chi_u_i. De même, on a, si Q \in K[X], Q(M)
=\
\mathrmdiag(Q(M_1),\\ldots,Q(M_k~))
et donc Q(u) = 0 \Leftrightarrow
\forall~i, Q(u_i~) = 0. On en déduit que
\mu_u = ppcm\mu_u_i~.
Mais P_i(u_i) = 0 et donc \mu_u_i
divise P_i. On en déduit que les \mu_u_i sont
deux à deux premiers entre eux et donc \mu_u
= \∏ ~
\mu_u_i.

Supposons maintenant que le polynôme caractéristique de u est scindé,
\chi_u(X) = \\∏
 _i=1^k(X - \lambda_i)^m_i avec
\lambda_1,\\ldots,\lambda_k~
distincts. Appliquons les résultats précédents avec P = \chi_u et
P_i = (X - \lambda_i)^m_i. Ceci nous
conduit à la définition~:

Définition~3.2.2 Soit u \in L(E) et \lambda~ une valeur propre de u de
multiplicité m. Le sous-espace
\mathrmKer~(u -
\lambda~\mathrmId)^m est appelé sous-espace
caractéristique de u associé à \lambda~, il est stable par u.

Remarque~3.2.4 Appelons donc E_i le sous-espace caractéristique
de u associé à \lambda_i, et u_i la restriction de u à
E_i. On sait que \mu_u_i divise (X -
\lambda_i)^m_i, donc \mu_u_i = (X
- \lambda_i)^r_i. Dans ce cas, \mu_u(X)
= \∏ ~
_i=1^k(X - \lambda_i)^r_i. De plus,
\chi_u(X) = \∏ ~
\chi_u_i, chacun des \chi_u_i est scindé
et a les mêmes racines que \mu_u_i. On en déduit que
\chi_u_i(X) = (X -
\lambda_i)^dim E_i~. Mais
alors \chi_u(X) =\
∏  _i=1^k~(X -
\lambda_i)^m_i =\
∏ _i=1^k~(X -
\lambda_i)^dim E_i~. Ceci
démontre que dim E_i = m_i~.
D'où le théorème

Théorème~3.2.12 Soit E un K-espace vectoriel de dimension finie et u \in
L(E) dont le polynôme caractéristique est scindé sur K. Alors E est
somme directe des sous-espaces caractéristiques de u~; chaque
sous-espace caractéristique a pour dimension la multiplicité de la
valeur propre correspondante.

\paragraph{3.2.7 Application~: récurrences linéaires d'ordre 2}

Théorème~3.2.13 Soit E un K-espace vectoriel de dimension 2 et u \in L(E)
dont le polynôme caractéristique est scindé. Alors, il existe une base \mathcal{E}
de E telle que la matrice de u dans cette base soit de l'une des deux
formes suivantes~: \left
(\matrix\,\lambda_1&0
\cr 0 &\lambda_2\right ) ou
\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right ).

Démonstration Si u est diagonalisable, il existe une base \mathcal{E} de E telle
que la matrice de u dans cette base soit \left
(\matrix\,\lambda_1&0
\cr 0 &\lambda_2\right ). Supposons
donc u non diagonalisable. Le polynôme caractéristique de u a
nécessairement une racine double \lambda~ (sinon u serait diagonalisable) et le
sous-espace propre associé E_u(\lambda~) est nécessairement de
dimension 1 (pour la même raison). Soit donc e_2 \in E \diagdown
E_u(\lambda~) et posons e_1 = u(e_2) - \lambda~e_2
= (u - \lambda~\mathrmId)(e_2)~; le vecteur
e_1 est non nul car e_2 n'est pas vecteur propre de u.
Le théorème de Cayley Hamilton garantit que (u -
\lambda~\mathrmId)^2 = 0 et donc (u -
\lambda~\mathrmId)(e_1) = 0. Donc e_1 est
vecteur propre de u. Ceci garantit que (e_1,e_2) est
libre (puisque e_2 n'est pas vecteur propre de u), et donc est
une base de E. On a u(e_1) = \lambda~e_1 et u(e_2) =
e_1 + \lambda~e_2 et donc la matrice de u dans cette base est
\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right ).

Proposition~3.2.14 Soit n \in \mathbb{N}~, alors

 \left
(\matrix\,\lambda_1&0
\cr 0 &\lambda_2\right )^n
= \left
(\matrix\,\lambda_1^n&0
\cr 0 &\lambda_2^n\right )

et

 \left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right )^n = \left
(\matrix\,\lambda~^n&n\lambda~^n-1
\cr 0 &\lambda~^n \right )

Démonstration La première formule est évidente~; la deuxième peut se
montrer soit par récurrence, soit en appliquant la formule du binôme~;
on écrit que

\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right ) = \lambda~I_2 + \left
(\matrix\,0&1 \cr
0&0\right )

et on remarque que \left
(\matrix\,0&1 \cr
0&0\right )^2 = 0.

Considérons a,b \in \mathbb{C}, b\neq~0 et soit E l'ensemble
des suites (u_n)_n\in\mathbb{N}~ de nombres complexes vérifiant

\forall~n \in \mathbb{N}~, u_n+2 = au_n+1~ +
bu_n

Proposition~3.2.15 E est un \mathbb{C}-espace vectoriel et l'application E \rightarrow~
\mathbb{C}^2,
(u_n)_n\in\mathbb{N}~\mapsto~(u_0,u_1)
est un isomorphisme d'espaces vectoriels~; en particulier,
dim~ E = 2.

Démonstration La vérification du premier point est élémentaire.
L'application
(u_n)_n\in\mathbb{N}~\mapsto~(u_0,u_1)
est visiblement linéaire et elle est bijective car une suite de E est
entièrement déterminée par la donnée de ses deux premiers éléments.

Soit (u_n)_n\in \mathbb{N}~ \in E et posons U_n =
\left
(\matrix\,u_n
\cr u_n+1\right ). On a alors

\begin{align*} U_n+1& =&
\left
(\matrix\,u_n+1
\cr u_n+2\right ) =
\left
(\matrix\,u_n+1
\cr au_n+1 +
bu_n\right )\%&
\\ & =& \left
(\matrix\,0&1 \cr
b&a\right )\left
(\matrix\,u_n
\cr u_n+1\right ) =
AU_n \%& \\
\end{align*}

avec A = \left
(\matrix\,0&1 \cr
b&a\right ). On en déduit que U_n =
A^nU_0. Comme \mathbb{C} est algébriquement clos,
\chi_A est scindé. Si A est diagonalisable, il existe P inversible
telle que A = P^-1\left
(\matrix\,\lambda_1&0
\cr 0 &\lambda_2\right )P, d'où

U_n = P^-1\left
(\matrix\,\lambda_1^n&0
\cr 0 &\lambda_2^n\right
)PU_0

et en prenant la première coordonnée, u_n =
\alpha~\lambda_1^n + \beta~\lambda_2^n. Si par contre, A n'est
pas diagonalisable, il existe P inversible telle que A =
P^-1\left
(\matrix\,\lambda~&1\cr 0
&\lambda~\right )P, d'où

U_n = P^-1\left
(\matrix\,\lambda~^n&n\lambda~^n-1
\cr 0 &\lambda~^n \right
)PU_0

et en prenant la première coordonnée, u_n = \alpha~\lambda~^n +
\beta~n\lambda~^n.

On a donc E \subset~ F avec F =\
\mathrmVect((\lambda_1^n)_n\in\mathbb{N}~,(\lambda_2^n)_n\in\mathbb{N}~)
ou F =\
\mathrmVect((\lambda~^n)_n\in\mathbb{N}~,(n\lambda~^n)_n\in\mathbb{N}~)
suivant le cas. Comme dim~ E = 2 et
dim~ F \leq 2, on a nécessairement égalité.
Remarquons alors que, pour \lambda~\neq~0,

(\lambda~^n)_ n\in\mathbb{N}~ \in E\quad
\Leftrightarrow \lambda~^2 = a\lambda~ + b
\Leftrightarrow \chi_ A(\lambda~) = 0

Ceci nous conduit à la méthode suivante de résolution de la récurrence
linéaire \forall~n \in \mathbb{N}~, u_n+2~ =
au_n+1 + bu_n

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  rechercher les solutions particulières de la forme u_n =
  \lambda~^n ceci conduit à une équation du second degré en \lambda~, P(\lambda~)
  = 0
\item
  si cette équation a deux racines simples \lambda_1 et \lambda_2,
  les solutions sont les suites de la forme u_n =
  \alpha~\lambda_1^n + \beta~\lambda_2^n
\item
  si cette équation a une racine double \lambda~, les solutions sont les suites
  de la forme u_n = \alpha~\lambda~^n + \beta~n\lambda~^n.
\end{itemize}

[
[
[
[

\end{document}
