\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{graphicx}
% Redefine \includegraphics so that, unless explicit options are
% given, the image width will not exceed the width of the page.
% Images get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\ScaleIfNeeded{%
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother
\let\Oldincludegraphics\includegraphics
{%
 \catcode`\@=11\relax%
 \gdef\includegraphics{\@ifnextchar[{\Oldincludegraphics}{\Oldincludegraphics[width=\ScaleIfNeeded]}}%
}%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Matrices},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.paragraphHead, .likeparagraphHead { margin-top:2em; font-weight: bold;}
.subparagraphHead, .likesubparagraphHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:\jmathustify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.subsubsectionToc, .likesubsubsectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Matrices}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}
{[}{]}
{[}

\subsubsection{2.6 Matrices}

\paragraph{2.6.1 Généralités}

Définition~2.6.1 M\_K(m,n) =
\(a\_i,\jmath)\_ 1\leqi\leqm \atop
1\leq\jmath\leqn \ est un K-espace vectoriel de dimension mn.
Il admet pour base la famille (E\_k,l)\_ 1\leqk\leqm
\atop 1\leql\leqn  avec

 E\_k,l = (\delta\_i,\jmath^k,l)\_ 1\leqi\leqm
\atop 1\leq\jmath\leqn  =\left (
\includegraphics{cours0x.png} \,\right )

Définition~2.6.2 Soit E et F deux espaces vectoriels de dimensions
finies n et m respectivement et u \in L(E,F). Soit \mathcal{E} =
(e\_1,\\ldots,e\_n~)
une base de E et ℱ =
(f\_1,\\ldots,f\_m~)
une base de F de base duale ℱ^∗ =
(f\_1^∗,\\ldots,f\_m^∗~).
On définit la matrice de u dans les bases \mathcal{E} et ℱ comme étant la matrice
\mathrmMat~ (u,\mathcal{E},ℱ) =
(a\_i,\jmath)\_ 1\leqi\leqm \atop 1\leq\jmath\leqn 
construite de fa\ccon équivalente par (i)
\forall~\jmath \in {[}1,n{]}, u(e\_\jmath~)
= \\sum ~
\_i=1^ma\_i,\jmathf\_i (ii)
\forall~i \in {[}1,m{]}, \\forall~~\jmath \in
{[}1,n{]}, a\_i,\jmath = f\_i^∗(u(e\_\jmath))
=\langle
f\_i^∗∣u(e\_\jmath)\rangle

Proposition~2.6.1 L'application L(E,F) \rightarrow~ M\_K(m,n),
u\mapsto~\mathrmMat~
(u,\mathcal{E},ℱ) est un isomorphisme de K-espaces vectoriels .

Produit de matrices A = (a\_i,\jmath) \in M\_K(m,n) et B =
(b\_i,\jmath) \in M\_K(n,p). On définit AB = (c\_i,\jmath) \in
M\_K(m,p) par

\forall~(i,\jmath) \in {[}1,m{]} \times {[}1,p{]}, c\_i,\jmath~
= \sum \_k=1^na~\_
i,kb\_k,\jmath

Théorème~2.6.2 Soit u \in L(E,F), v \in L(F,G) où E, F et G sont trois
espaces vectoriels de dimensions finies admettant des bases \mathcal{E}, ℱ et G.
Alors on a

\mathrmMat~ (v \cdot u,\mathcal{E},G)
= \mathrmMat~
(v,ℱ,G)\mathrmMat~ (u,\mathcal{E},ℱ)

Démonstration En effet, si les dimensions des espaces sont
respectivement p, n et m, et si l'on note A =\
\mathrmMat (u,\mathcal{E},ℱ) et B =\
\mathrmMat (v,ℱ,G), on a

\begin{align*} v \cdot u(e\_\jmath)& =&
v(u(e\_\jmath) = v(\\sum
\_k=1^na\_ k,\jmathf\_k)\%&
\\ & =& \\sum
\_k=1^na\_ k,\jmathv(f\_k) \%&
\\ & =& \\sum
\_k=1^na\_ k,\jmath \\sum
\_i=1^mb\_ i,kg\_i \%&
\\ & =& \\sum
\_i=1^m\left (\\sum
\_k=1^nb\_ i,ka\_k,\jmath\right
)g\_i\%& \\
\end{align*}

ce qui montre que la i-ième coordonnée de v \cdot u(e\_\jmath) est bien
le terme d'indice i,\jmath de la matrice BA.

Remarque~2.6.1 Ceci lié à l'isomorphisme avec les applications linéaires
permet d'obtenir immédiatement les propriétés essentielles du produit
des matrices~: associativité et bilinéarité.

Définition~2.6.3 Soit x \in E et y \in F. Si x =\
\sum ~
\_i=1^nx\_ie\_i, on définit X =
\left
(\matrix\,x\_1
\cr \⋮~
\cr x\_n\right ) (vecteur
colonne des coordonnées de x dans la base \mathcal{E}). De même, on définit Y
vecteur colonne des coordonnées de y dans la base ℱ. On a alors

Théorème~2.6.3 \mathrmMat~
(u,\mathcal{E},ℱ) est l'unique matrice A \in M\_K(m,n) vérifiant

\forall~~(x,y) \in E \times F,\quad u(x) = y
\Leftrightarrow Y = AX

Démonstration En effet

\begin{align*} u(x) = y&
\Leftrightarrow & \\sum
\_\jmath=1^nx\_ \jmathu(e\_\jmath) =
\sum \_i=1^my~\_
if\_i \%& \\ &
\Leftrightarrow & \\sum
\_\jmath=1^nx\_ \jmath \\sum
\_i=1^ma\_ i,\jmathf\_i =
\sum \_i=1^my~\_
if\_i\%& \\ &
\Leftrightarrow & \forall~~i,
y\_i = \\sum
\_\jmath=1^na\_ i,\jmathx\_\jmath
\Leftrightarrow Y = AX \%&
\\ \end{align*}

Inversement, si une matrice A vérifie cette condition, en prenant x =
e\_\jmath, on voit que a\_i,\jmath est la i-ième coordonnée de
u(e\_\jmath), et donc A =\
\mathrmMat (u,\mathcal{E},ℱ).

\paragraph{2.6.2 Matrices carrées}

Lemme~2.6.4 Soit (E\_i,\jmath) la base canonique de M\_K(n).
On a E\_i,\jmathE\_k,l = \delta\_\jmath^kE\_i,l

Démonstration Calcul élémentaire

Proposition~2.6.5 M\_K(n) (= M\_K(n,n)) est une
K-algèbre de dimension n^2 dont le centre est constitué des
matrices scalaires. Le groupe de ses éléments inversibles est noté
GL\_K(n) (groupe linéaire d'indice n).

Démonstration Tout est élémentaire, sauf la recherche du centre. Soit A
\in M\_k(n) une matrice qui commute à toutes les autres matrices
carrées. On a A =\ \\sum
 \_i,\jmatha\_i,\jmathE\_i,\jmath. On écrit pour
k\neq~l, E\_k,lA = AE\_k,l soit
encore \\sum ~
\_\jmatha\_l,\jmathE\_k,\jmath =\
\sum  \_ia\_i,kE\_i,l~. En
prenant la coordonnée suivant E\_k,k, on a a\_l,k = 0.
En prenant la coordonnée suivant E\_k,l on a a\_l,l =
a\_k,k soit A = a\_1,1I\_n.

Remarque~2.6.2 Bien entendu si E est un K-espace vectoriel de dimension
n admettant une base \mathcal{E}, les résultats précédents impliquent que
l'application L(E) \rightarrow~ M\_K(n),
u\mapsto~\mathrmMat~
(u,\mathcal{E}) est un isomorphisme de K-algèbres.

Définition~2.6.4 Si A = (a\_i,\jmath) \in M\_K(n), on définit
la trace de A comme
\mathrm{tr}~A
= \\sum ~
\_i=1^na\_i,i.

Théorème~2.6.6 Soit A \in M\_K(m,n) et B \in M\_K(n,m).
Alors \mathrm{tr}~(AB)
= \mathrm{tr}~(BA). En
particulier, si A \in M\_K(n) et P \in GL\_K(n), alors
\mathrm{tr}(P^-1~AP)
= \mathrm{tr}~A.

Démonstration On a en effet
\mathrm{tr}~(AB)
= \\sum ~
\_i,\jmatha\_i,\jmathb\_\jmath,i qui est une expression
visiblement symétrique en a et b.

\paragraph{2.6.3 Transposée}

Définition~2.6.5 Si A = (a\_i,\jmath) \in M\_K(m,n) on pose
^tA = (b\_i,\jmath) \in M\_K(n,m) définie par
\forall~~(i,\jmath) \in {[}1,n{]} \times
{[}1,m{]},\quad b\_i,\jmath = a\_\jmath,i.

Proposition~2.6.7 L'application
M\mapsto~^tM est linéaire bi\jmathective de
M\_K(m,n) sur M\_K(n,m) et on a
^t(^tM) = M. Si M \in M\_K(m,n),N \in
M\_K(n,p), alors ^t(MN) =
^tN^tM. Si M \in M\_K(n) est inversible,
^tM aussi et (^tM)^-1 =
^t(M^-1).

Démonstration Calcul élémentaire pour montrer que ^t(\alpha~M + \beta~N)
= \alpha~^tM + \beta~^tN et que ^t(MN) =
^tN^tM. Si M est inversible, on a MM^-1 =
M^-1M = I\_n et prenant la transposée,
^t(M^-1)^tM =
^tM^t(M^-1) = ^tI\_n =
I\_n. On en déduit que ^tM est inversible et que
(^tM)^-1 = ^t(M^-1).

Théorème~2.6.8 Soit E et F deux K-espaces vectoriels de dimensions
finies admettant des bases \mathcal{E} et ℱ. Alors

\mathrmMat~
(^tu,ℱ^∗,\mathcal{E}^∗) =
^t \mathrmMat~
(u,\mathcal{E},ℱ)

Démonstration Soit A et B les deux matrices. On a
^tu(f\_\jmath^∗) =\
\sum ~
\_ib\_i,\jmathe\_i^∗ soit b\_i,\jmath =
^tu(f\_\jmath^∗)(e\_i) =
f\_\jmath^∗\cdot u(e\_i) =
f\_\jmath^∗(u(e\_i)) = a\_\jmath,i.

Définition~2.6.6 Soit M \in M\_K(n). On dit que M est symétrique
si ^tM = M et antisymétrique si ^tM = -M.

Proposition~2.6.9 L'ensemble S\_K(n) (resp. A\_K(n)) des
matrices symétriques (resp. antisymétriques) est un sous-espace
vectoriel de M\_K(n). On a dim~
S\_K(n) = n(n+1) \over 2 . Si
carK\mathrel\neq~~2, on a
M\_K(n) = S\_K(n) \oplus~ A\_K(n) et
dim A\_K~(n) = n(n-1)
\over 2 .

Démonstration Une base de S\_K(n) est clairement constituée des
E\_i,i 1 \leq i \leq n et des E\_i,\jmath + E\_\jmath,i, 1 \leq i
\textless{} \jmath \leq n, donc dim S\_K~(n)
= n(n+1) \over 2 . Si
carK\mathrel\neq~~2, on peut
écrire A = 1\over 2(A + ^tA) +
1\over 2(A -^tA), ce qui montre que
M\_K(n) = S\_K(n) + A\_K(n) et on a clairement
S\_K(n) \bigcap A\_K(n) =
\0\.

Remarque~2.6.3 Par contre, si car~K = 2, on a 1
= -1 et donc S\_K(n) = A\_K(n).

\paragraph{2.6.4 Rang d'une matrice}

Définition~2.6.7 Soit A \in M\_K(m,n). On appelle rang de A le
rang dans K^m de la famille
(c\_1,\\ldots,c\_n~)
de ses vecteurs colonnes.

Théorème~2.6.10 Si u \in L(E,F), \mathcal{E} une base de E, ℱ une base de F, A
= \mathrmMat~ (u,\mathcal{E},ℱ). Alors
\mathrmrg~A
= \mathrmrg~u.

Démonstration On a
\mathrmrg~u
=\
\mathrmrg(u(e\_1),\\ldots,u(e\_n~)).
Mais le rang de la famille des u(e\_\jmath) est aussi le rang de son
image par l'isomorphisme de F dans K^m qui à un vecteur
associe la famille de ses coordonnées dans la base ℱ, c'est-à-dire le
rang de la famille des vecteurs colonnes de la matrice A.

Corollaire~2.6.11
\mathrmrg~(AB)
\leq\
min(\mathrmrgA,\\mathrmrg~B).

Théorème~2.6.12 Soit A \in M\_K(n). On a équivalence de

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) A est inversible
\item
  (ii) \mathrmrg~A = n
\item
  (iii) \existsB \in M\_K~(n), AB =
  I\_n
\item
  (iv) \existsB \in M\_K(n), BA = I\_n~
\end{itemize}

Démonstration Se déduit immédiatement du théorème analogue sur les
endomorphismes d'un espace vectoriel de dimension finie.

\paragraph{2.6.5 La méthode du pivot}

La recherche pratique du rang d'une matrice peut se faire par la méthode
du pivot~; dans les algorithmes qui suivent, la flèche vers la gauche
décrira un remplacement~: x ← y signifiera remplacer x par y. Il s'agit
là de l'analogue d'une affectation dans un langage de programmation.

Définition~2.6.8 On définit les opérations élémentaires sur les vecteurs
colonnes
c\_1,\\ldots,c\_n~
d'une matrice A \in M\_K(m,n)~:

\begin{itemize}
\item
  (i) a\jmathouter à une colonne c\_\jmath une combinaison linéaire des
  autres vecteurs colonnes

  c\_i ← c\_i + \\sum
  \_\jmath\neq~i\lambda~\_\jmathc\_\jmath

  ou encore

  A ← A\left
  (\matrix\,1& &\lambda~\_1& &
  \cr
  &⋱&\⋮~
  & & \cr & &1 & & \cr &
  &\⋮~
  &⋱& \cr &
  &\lambda~\_n& &1\right )
\item
  (ii) multiplier la colonne c\_i par un scalaire non nul

  c\_i ← \lambda~c\_i

  ou encore

  A ← A\left
  (\matrix\,1& & & &
  \cr &⋱& & &
  \cr & &\lambda~& & \cr & &
  &⋱& \cr & & &
  &1\right )
\item
  (iii) effectuer une permutation \sigma sur les vecteurs colonnes

  (c\_1,\\ldots,c\_n~)
  ←
  (c\_\sigma(1),\\ldots,c\_\sigma(n)~)

  ou encore

  A ← AP\_\sigma

  où P\_\sigma = (\delta\_i^\sigma(\jmath)).
\end{itemize}

Proposition~2.6.13 Les opérations élémentaires ne changent pas le rang
d'une matrice.

Démonstration En effet, il est clair que les opérations élémentaires ne
changent pas le sous-espace de K^n,
\mathrmVect(c\_1,\\\ldots,c\_n~)~;
on peut remarquer aussi que les opérations élémentaires s'effectuent par
multiplications à droite par des matrices inversibles, donc ne changent
pas le rang.

Théorème~2.6.14 Soit M \in M\_K(n,p). Alors il existe une matrice
M' qui se déduit de M par une suite d'opérations élémentaires, de la
forme

M' = \left (\matrix\,0
&0 &\\ldots~&0
&0&\\ldots~&0
\cr \⋮~
&\⋮~ &
&\⋮~
&\⋮~&
&\⋮~
\cr 0
&\⋮~ &
&\⋮~
&\⋮~&
&\⋮~
\cr 1 &0
&\\ldots~&0
&0&\\ldots~&0
\cr a\_\sigma(1)+1,1& &
&\⋮~
&\⋮~&
&\⋮~
\cr \⋮~
&\⋮~ &
&\⋮~
&\⋮~&
&\⋮~
\cr \⋮~
&1 & & & & & \cr
\⋮~
&a\_\sigma(2)+1,2 \cr
\⋮~
&\⋮~ &
&\⋮~
\cr & & &1 \cr & &
&a\_\sigma(r)+1,r&\⋮~&
&\⋮~
\cr \⋮~
&\⋮~ &
&\⋮~ &0&
&0\right )

où \sigma est une application strictement croissante de {[}1,r{]} dans
{[}1,n{]}. Dans toute telle écriture, on a
\mathrmrg~M = r.

Démonstration Par récurrence sur le nombre de colonnes de M. C'est
évident s'il existe une seule colonne ou si M = 0. Sinon, soit \sigma(1)
l'indice de la première ligne non nulle et \jmath tel que
a\_\sigma(1),\jmath\neq~0. On effectue une
permutation des colonnes pour amener la colonne c\_\jmath en première
colonne, puis on effectue les opérations c\_1 ← 1
\over a\_\sigma(1),1c\_1 puis pour \jmath allant
de 2 à n, c\_\jmath ← c\_\jmath - a\_\sigma(1),\jmathc\_1.
On aboutit alors à une matrice

\left (\matrix\,0
&0&\\ldots&0&0&\\\ldots~&0
\cr \⋮~
&\⋮~&
&\⋮&\\⋮~&
&\⋮~
\cr 0
&\⋮~&
&\⋮&\\⋮~&
&\⋮~
\cr 1
&0&\\ldots&0&0&\\\ldots~&0
\cr a\_\sigma(1)+1,1& &
&\⋮&\\⋮~&
&\⋮~
\cr \⋮~
&\⋮~&
&\⋮&\\⋮~&
&\⋮~\right
)

Il suffit alors d'utiliser l'hypothèse de récurrence sur les n - 1
dernières colonnes de la matrice. Il est clair que
\mathrmrg~M' = r, mais on a
\mathrmrg~M
= \mathrmrg~M', d'où
\mathrmrg~M = r.

Remarque~2.6.4 On peut ensuite utiliser les 1 qui sont dans les r
premières colonnes pour éliminer au fur et à mesure en partant du bas
tous les a\_\sigma(i),\jmath. Si M est inversible, nécessairement \sigma =
\mathrmId et alors la matrice obtenue est
I\_n. Mais on a M' =
MP\_1\\ldotsP\_k~
où les P\_i sont les matrices des différentes opérations
élémentaires effectuées. On en déduit que M^-1 =
P\_1\\ldotsP\_k~.
On peut calculer ce produit en partant de B ← I\_n et en
effectuant sur la matrice B les mêmes opérations élémentaires que sur la
matrice A. On a donc à la fin B ←
P\_1\\ldotsP\_k~,
soit B ← M^-1.

\paragraph{2.6.6 Changement de bases}

Proposition~2.6.15 Soit \mathcal{E} =
(e\_1,\\ldots,e\_n~)
une base de E et A = (a\_i,\jmath) \in M\_K(n). Posons
x\_\jmath = \\sum ~
\_i=1^na\_i,\jmathe\_i. Alors A est inversible
si et seulement si
(x\_1,\\ldots,x\_n~)
est une base de E.

Démonstration Comme précédemment, le rang de la famille des x\_\jmath
est aussi le rang de son image par l'isomorphisme de E sur
K^n qui à un vecteur associe la famille de ses coordonnées
dans la base \mathcal{E}, c'est-à-dire ici de la famille des vecteurs colonnes de
A.

Définition~2.6.9 Soit \mathcal{E} et \mathcal{E}' deux bases de E. On définit
P\_\mathcal{E}^\mathcal{E}' comme étant la matrice inversible
(a\_i,\jmath) \in M\_k(n) définie par e\_\jmath'
= \\sum ~
\_i=1^na\_i,\jmathe\_i.

Remarque~2.6.5 On a donc P\_\mathcal{E}^\mathcal{E}'
= \mathrmMat~
(\mathrmId\_E,\mathcal{E}',\mathcal{E}) =\
\mathrmMat (u,\mathcal{E},\mathcal{E}) où u est défini par
u(e\_i) = e\_i'. De la première égalité on déduit
immédiatement~:

Théorème~2.6.16

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) P\_\mathcal{E}'^\mathcal{E} = (P\_\mathcal{E}^\mathcal{E}')^-1 et
  P\_\mathcal{E}^\mathcal{E}'` =
  P\_\mathcal{E}^\mathcal{E}'P\_\mathcal{E}'^\mathcal{E}''
\item
  (ii) si X et X' sont les vecteurs colonnes des coordonnées de x \in E
  dans les bases \mathcal{E} et \mathcal{E}' et P = P\_\mathcal{E}^\mathcal{E}', on a X = PX'.
\end{itemize}

Théorème~2.6.17 Soit u \in L(E,F), \mathcal{E} et \mathcal{E}' deux bases de E, ℱ et ℱ' deux
bases de F. On pose P = P\_\mathcal{E}^\mathcal{E}', Q =
P\_ℱ^ℱ', M =\
\mathrmMat (u,\mathcal{E},ℱ), M' =\
\mathrmMat (u,\mathcal{E}',ℱ'). Alors on a M' =
Q^-1MP.

Démonstration y = u(x) \Leftrightarrow Y = MX
\Leftrightarrow QY `= MPX' \mathrel\Leftrightarrow Y
`= Q^-1MPX'. L'unicité de la matrice d'une application
linéaire permet de conclure que M' = Q^-1MP.

Définition~2.6.10 On dit que M,M' \in M\_K(m,n) sont équivalentes
s'il existe Q \in GL\_K(m) et P \in GL\_K(n) telles que M' =
Q^-1MP.

Remarque~2.6.6 Ceci revient à dire que M et M' sont les matrices d'une
même application linéaire dans des bases ''différentes''~; sous cette
forme, il est clair qu'il s'agit d'une relation d'équivalence.

Lemme~2.6.18 Si M est de rang r, elle est équivalente à J\_r =
\left
(\matrix\,I\_r&0
\cr 0 &0\right )

Démonstration Soit M =\
\mathrmMat (u,\mathcal{E},ℱ). Soit V un supplémentaire de
\mathrmKer~u dans E,
(e\_1',\\ldots,e\_r~')
une base de V ,
(e\_r+1',\\ldots,e\_n~')
une base de \mathrmKer~u.
Comme u\textbar{}\_V  est un isomorphisme de V sur
\mathrmIm~u,
(f\_1',\\ldots,f\_r~')
=
(u(e\_1'),\\ldots,u(e\_r~'))
est une base de \mathrmIm~u
que l'on peut compléter en
(f\_1',\\ldots,f\_m~')
base de F. On a alors
\mathrmMat~ (u,\mathcal{E}',ℱ') =
J\_r, d'où le résultat.

Théorème~2.6.19 Deux matrices de M\_K(m,n) sont équivalentes si
et seulement si elles ont même rang.

Démonstration La condition est bien évidemment nécessaire, et le lemme
précédent montre qu'elle est suffisante.

Théorème~2.6.20 \mathrmrg~A
= \mathrmrg^t~A.
Autrement dit, le rang d'une matrice est aussi égal au rang de la
famille de ses vecteurs lignes.

Démonstration En effet si A est équivalente à J\_r,
^tA est équivalente à ^tJ\_r qui est aussi
de rang r.

Remarque~2.6.7 Dans le cas d'un endomorphisme, on a en général \mathcal{E} = ℱ et
\mathcal{E}' = ℱ' d'où le théorème

Théorème~2.6.21 Soit u \in L(E), \mathcal{E} et \mathcal{E}' deux bases de E. On pose P =
P\_\mathcal{E}^\mathcal{E}', M =\
\mathrmMat (u,\mathcal{E}), M' =\
\mathrmMat (u,\mathcal{E}'). Alors on a M' =
P^-1MP.

Définition~2.6.11 On dit que M,M' \in M\_K(n) sont semblables s'il
existe P \in GL\_K(n) telles que M' = P^-1MP.

Remarque~2.6.8 Cela revient à dire que M et M' sont les matrices d'un
même endomorphisme dans des bases ''différentes''~; sous cette forme, il
est clair qu'il s'agit d'une relation d'équivalence.

Remarque~2.6.9 On remarque que deux matrices semblables ont même trace
ce qui permet de définir

Définition~2.6.12 Soit u \in L(E). On pose
\mathrm{tr}~u
=\
\mathrm{tr}\mathrmMat~
(u,\mathcal{E}), indépendant du choix de la base \mathcal{E} de E.

\paragraph{2.6.7 Produit des matrices par blocs}

Soit M \in M\_K(m,n),M' \in M\_K(n,p), 1 \leq q \leq m, 1 \leq r \leq n,
1 \leq s \leq p. On écrit

M =\left ( \includegraphics{cours1x.png}
\,\right ),\quad M'
=\left (

\includegraphics{cours2x.png} \,\right )

avec A \in M\_K(q,r),B \in M\_K(q,n - r),C \in M\_K(m
- q,r),D \in M\_K(m - q,n - r),A' \in M\_K(r,s),B' \in
M\_K(r,p - s),C' \in M\_K(n - r,s),D' \in M\_K(n -
r,p - s) Alors

MM' = \left (\matrix\,AA'
+ BC'&AB' + BD' \cr CA' + DC'&CB' +
DD'\right )

Démonstration Par le calcul (attention aux décalages d'indices).

{[}
{[}
{[}
{[}

\end{document}
