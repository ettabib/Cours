\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Endomorphismes d'un espace euclidien},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.paragraphHead, .likeparagraphHead { margin-top:2em; font-weight: bold;}
.subparagraphHead, .likesubparagraphHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:\\jmathmathustify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.subsubsectionToc, .likesubsubsectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Endomorphismes d'un espace euclidien}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}
{[}{]}
{[}

\subsubsection{12.6 Endomorphismes d'un espace euclidien}

\paragraph{12.6.1 Droites et plans stables}

Nous utiliserons à deux reprises le lemme suivant

Lemme~12.6.1 Soit E un \mathbb{R}~-espace vectoriel ~de dimension finie et u \in
L(E). Alors u admet soit une droite stable, soit un plan stable.

Démonstration Soit P un polynôme normalisé annulateur de u et soit P =
P_1\\ldotsP_n~
la décomposition de P en polynômes normalisés irréductibles sur \mathbb{R}~. On a
0 = P(u) = P_1(u) \cdot⋯ \cdot
P_n(u). Donc l'un des P_i(u) est non in\\jmathmathectif. Soit x
\in\mathrmKerP_i~(u)
\diagdown\0\. Deux cas sont possibles~:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  P_1 est de degré 1, soit P_1(X) = X - \lambda~, alors (u -
  \lambda~\mathrmId)(x) = 0, x est vecteur propre de u et la
  droite \mathbb{R}~x est stable par u~;
\item
  P_1 est de degré 2, alors P_1(X) = X^2 -
  aX - b et on a donc u^2(x) = au(x) + bx~; le sous-espace
  \mathrmVect~(x,u(x)) est
  de dimension au plus 2 (en fait il est facile de vérifier qu'elle est
  égale à 2) et il est stable par u.
\end{itemize}

\paragraph{12.6.2 Réduction des endomorphismes symétriques}

Théorème~12.6.2 Soit E un espace euclidien et u un endomorphisme de E.
Alors on a équivalence de

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) u est un endomorphisme symétrique
\item
  (ii) il existe une base orthonormée formée de vecteurs propres de u
\item
  (iii) il existe une base orthonormée \mathcal{E} telle que
  \mathrmMat~ (u,\mathcal{E}) soit
  diagonale.
\end{itemize}

Démonstration (ii) et (iii) sont clairement équivalents. Si (iii) est
vérifiée, la matrice de u dans la base orthonormée \mathcal{E} est symétrique et
donc u est un endomorphisme symétrique. Il nous reste donc à montrer que
(i) \rigtharrow~(ii), ce que nous allons faire par récurrence sur n
= dim~ E. Montrons pour cela que u a un vecteur
propre. D'après le lemme ci dessus, u admet soit une droite, soit un
plan stable. Si u admet une droite stable, cette droite est engendrée
par un vecteur propre. Si u a un plan stable \Pi, soit u' l'endomorphisme
induit par u sur \Pi (c'est bien entendu un endomorphisme symétrique de
\Pi), \mathcal{E} = (e_1,e_2) une base orthonormée de \Pi et
\mathrmMat~ (u',\mathcal{E}) =
\left
(\matrix\,a&b\cr b
&c\right )~; alors \chi_u(X) = (X - a)(X - c) -
b^2 = X^2 - (a + b)X + ac - b^2 de
discriminant \Delta = (a + c)^2 - 4(ac - b^2) = (a -
c)^2 + 4b^2 ≥ 0~; donc u' a un vecteur propre dans \Pi
qui est également un vecteur propre de u dans E. Supposons donc que tout
endomorphisme symétrique d'un espace de dimension n - 1 admet une base
orthonormée de vecteurs propres. Soit e_1 un vecteur propre de
u (endomorphisme symétrique d'un espace euclidien de dimension n).
Quitte à remplacer e_1 par  e_1 \over
\e_1\ , on
peut supposer que
\e_1\ = 1.
Soit H = e_1^\bot. Comme Ke_1 est stable par u,
son orthogonal H est stable par u^∗ = u. La restriction u' de
u à H est un endomorphisme symétrique de H de dimension n - 1, donc
admet une base orthonormée
(e_2,\\ldots,e_n~)
formée de vecteurs propres de u' (donc de u)~; alors
(e_1,\\ldots,e_n~)
est une base orthonormée de E formée de vecteurs propres de u.

Corollaire~12.6.3 Soit E un espace euclidien et u un endomorphisme
symétrique de E~; alors E est somme directe orthogonale des sous-espaces
propres de u.

Démonstration Puisque u est diagonalisable, E est somme directe des
sous-espaces propres de u. Il suffit de montrer que ces sous-espaces
sont deux à deux orthogonaux. Mais, si x \in E_u(\lambda~) et y \in
E_u(\mu) avec \lambda~\neq~\mu, on a

\lambda~(x∣y) = (u(x)\mathrel∣y)
= (x∣u(y)) =
(x∣\muy) = \mu(x\mathrel∣y)

Comme \lambda~\neq~\mu, on a
(x∣y) = 0.

Remarque~12.6.1 Ceci permet une pratique simple de la réduction d'un
endomorphisme symétrique~; il suffit en effet de déterminer une base
orthonormée de chacun des sous-espaces propres de u et de réunir ces
bases~; on obtient une base orthonormée de E formée de vecteurs propres
de u.

Définition~12.6.1 Soit E un espace euclidien et u un endomorphisme
symétrique de E. On dit que u est un endomorphisme positif (resp. défini
positif) s'il vérifie les conditions équivalentes~:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) \forall~~x \in
  E,(u(x)∣x) ≥ 0 (resp.
  \forall~x\neq~0,(u(x)\mathrel∣~x)
  \textgreater{} 0)
\item
  (ii) L'application
  x\mapsto~(u(x)\mathrel∣x) est
  une forme quadratique positive sur E (resp. définie positive)
\item
  (iii) Les valeurs propres de u sont positives (resp. strictement
  positives).
\end{itemize}

Démonstration En remarquant que l'application Q_u :
x\mapsto~(u(x)\mathrel∣x) est une
forme quadratique de forme polaire
(x,y)\mapsto~(u(x)\mathrel∣y), on
a immédiatement l'équivalence de (i) et (ii). Soit \lambda~ une valeur propre
de u et x un vecteur propre associé~; on a alors
(u(x)∣x) = (\lambda~x\mathrel∣x)
= \lambda~\x\^2,
si bien que \lambda~ = (u(x)∣x)
\over
\x\^2 ~;
il en résulte que (i) \rigtharrow~(iii). Inversement, supposons (iii) vérifiée et
soit
(e_1,\\ldots,e_n~)
une base orthonormée formée de vecteurs propres de u, u(e_i) =
\lambda_ie_i. On a alors, si x =\
\sum  _ix_ie_i~,

(u(x)∣x) = (\\sum
_i\lambda_ix_ie_i∣\\sum
_ix_ie_i) = \\sum
_i\lambda_ix_i^2 ≥ 0

(resp. \textgreater{} 0 si x\neq~0) si bien que
(iii) \rigtharrow~(i).

Théorème~12.6.4 (réduction simultanée des formes quadratiques). Soit E
un \mathbb{R}~ espace vectoriel de dimension finie, \Phi une forme quadratique
définie positive, \Psi une forme quadratique sur E. Alors il existe une
base \mathcal{E} de E orthonormée pour \Phi et orthogonale pour \Psi.

Démonstration On sait qu'il existe un unique endomorphisme u de E tel
que \forall~~x,y \in E, \psi(x,y) = \phi(u(x),y). Comme \psi est
symétrique, u est un endomorphisme symétrique de l'espace euclidien
(E,\Phi) et il existe une base \mathcal{E} =
(e_1,\\ldots,e_n~)
orthonormée pour \Phi formée de vecteurs propres de u~: u(e_i) =
\lambda_ie_i. On a alors

\psi(e_i,e_\\jmathmath) = \phi(e_i,u(e_\\jmathmath)) =
\phi(e_i,\lambda_\\jmathmathe_\\jmathmath) =
\lambda_\\jmathmath\delta_i^\\jmathmath

ce qui montre que \mathcal{E} est une base orthogonale pour \psi.

\paragraph{12.6.3 Normes d'endomorphismes}

Théorème~12.6.5 Soit E un espace euclidien et u \in L(E). Alors

\u\
=\
sup_\x\\leq1,\y\\leq1(u(x)∣y)

Démonstration Supposons que
\x\ \leq
1,\y\ \leq 1. On a alors
d'après l'inégalité de Schwarz

(u(x)∣y)\leq\
u(x)\\y\
\leq\ u\

ce qui montre que \u\
≥\
sup_\x\\leq1,\y\\leq1(u(x)∣y).
Mais d'autre part, puisque la boule unité fermée est compacte, il existe
x_0 tel que
\x_0\ \leq 1
avec \u(x_0)\
=\
sup_\x\\leq1\u(x)\
=\ u\. Posons alors,
si u\neq~0, y_0 = u(x_0)
\over
\u(x_0)\ .
On a \y_0\ =
1 et

(u(x_0)∣y_0)
= (u(x_0)∣u(x_0))
\over
\u(x_0)\
=\ u(x_0)\
=\ u\

ce qui montre que \u\
\leq\
sup_\x\\leq1,\y\\leq1(u(x)∣y),
et donc l'égalité.

Corollaire~12.6.6 Soit E un espace euclidien et u \in L(E). Alors
\u\
=\ u^∗\.

Démonstration En effet

\u\
=\
sup_\x\\leq1,\y\\leq1(u(x)∣y)
=\
sup_\x\\leq1,\y\\leq1(x∣u^∗(y))
=\ u^∗\

Théorème~12.6.7 Soit E un espace euclidien et u \in L(E) symétrique
positif. Alors

\u\
=\
sup_\x\\leq1(u(x)∣x)
=\
max_\lambda~\in\mathrm{Sp}(u)~\lambda~

Démonstration Supposons que
\x\ \leq 1. On a alors
d'après l'inégalité de Schwarz

(u(x)∣x) \leq\
u(x)\\x\
\leq\ u\

ce qui montre que \u\
≥\
sup_\x\\leq1(u(x)∣x).
De plus, soit k =\
sup_\x\\leq1(u(x)∣x),
si bien que \forall~~x \in
E,(u(x)∣x) \leq
k\x\^2, et
supposons que \x\ \leq
1,\y\ \leq 1. On a alors,
puisque u est symétrique

\begin{align*}
(u(x)∣y)& =& 1
\over 4 (u(x +
y)∣x + y) - (u(x -
y)∣x - y)\%&
\\ & \leq& k \over 4
(\x + y\^2
+\ x -
y\^2) = k \over 2
(\x\^2
+\ y\^2) \leq
k \%& \\ \end{align*}

et donc

\u\
=\
sup_\x\\leq1,\y\\leq1(u(x)∣y)\leq
k

et par conséquent

\u\
=\
sup_\x\\leq1(u(x)∣x)

Soit
(e_1,\\ldots,e_n~)
une base orthonormée formée de vecteurs propres de u, u(e_i) =
\lambda_ie_i. On peut supposer que \lambda_1 \leq
\lambda_2
\leq\\ldots~ \leq
\lambda_n. On a alors, si x =\
\sum  _ix_ie_i~,

(u(x)∣x) = (\\sum
_i\lambda_ix_ie_i∣\\sum
_ix_ie_i) = \\sum
_i\lambda_ix_i^2 \leq \lambda_ n
\sum _ix_i^2 \leq \lambda~_
n

avec égalité si x = e_n. Ceci montre que

sup_\x\\leq1(u(x)\mathrel∣~x)
=\
max_\lambda~\in\mathrm{Sp}(u)~\lambda~

et achève la démonstration.

Corollaire~12.6.8 Soit E un espace euclidien et u \in L(E). Alors
u^∗u est un endomorphisme symétrique positif et
\u^∗u\
=\ u\^2.

Démonstration On a (u^∗u)^∗ =
u^∗u^∗∗ = u^∗u donc u^∗u est
symétrique. De plus (u^∗u(x)∣x) =
(u(x)∣u(x)) =\
u(x)\^2 ≥ 0, ce qui montre que
u^∗u est positif. On a alors

\u^∗u\^2
= sup_\
x\\leq1(u^∗u(x)∣x)
= sup_\
x\\leq1\u(x)\^2
=\ u\^2

Corollaire~12.6.9 \u\
est la racine carrée de la plus grande valeur propre de u^∗u.

Démonstration Résulte immédiatement des résultats précédents.

\paragraph{12.6.4 Endomorphismes orthogonaux d'un plan euclidien}

Remarque~12.6.2 Soit E un \mathbb{R}~-espace vectoriel ~de dimension finie. La
relation définie sur l'ensemble des bases de E par

\mathcal{E}\mathcal{R}\mathcal{E}'\Leftrightarrow
\mathrm{det}~
P_\mathcal{E}^\mathcal{E}' \textgreater{} 0

est une relation d'équivalence pour laquelle il y a deux classes
d'équivalence appelées orientations de l'espace. Le choix d'une de ces
classes (les bases directes) oriente l'espace E.

Soit E un espace euclidien de dimension 2.

Théorème~12.6.10 Soit u \in O(E) et \mathcal{E} une base orthonormée de E.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) Si u \in SO(E), alors
  \mathrmMat~ (u,\mathcal{E}) =
  \left
  (\matrix\,cos~
  \theta&-sin~ \theta\cr
  sin \theta &\cos~
  \theta\right ) pour un \theta \in \mathbb{R}~\diagup2\pi~\mathbb{Z} ne dépendant que de
  l'orientation de la base \mathcal{E} (un changement d'orientation changeant \theta en
  - \theta)~; le groupe SO(E) est commutatif, isomorphe au groupe (\mathbb{R}~\diagup2\pi~\mathbb{Z},+)
\item
  (ii) Si \mathrm{det}~ u =
  -1, alors \mathrmMat~
  (u,\mathcal{E}) = \left
  (\matrix\,cos~
  \theta&sin~ \theta \cr
  sin \theta&-\cos~
  \theta\right )~; u est une symétrie orthogonale par
  rapport à une droite.
\end{itemize}

Démonstration Posons \mathcal{E} = (e_1,e_2) et u(e_1)
= ae_1 + be_2. On a a^2 + b^2
=\
u(e_1)\^2
=\
e_1\^2 = 1, donc il existe
\theta \in \mathbb{R}~\diagup2\pi~\mathbb{Z} tel que a = cos~ \theta et b
= sin \theta. On a u(e_2~) \in
u(e_1)^\bot = \mathbb{R}~(-be_1 + ae_2). On en
déduit que u(e_2) = \lambda~(-sin~
\thetae_1 + cos \thetae_2~)~; comme
\u(e_2)\
=\ e_2\ = 1,
on doit avoir \lambda~^2 = 1, soit \lambda~ = ±1. Donc la matrice de u dans
la base \mathcal{E} est de l'une des deux formes

\left
(\matrix\,cos~
\theta&-sin~ \theta \cr
sin \theta&\cos~ \theta
\cr \right )\text ou
\left
(\matrix\,cos~
\theta&sin~ \theta \cr
sin \theta&-\cos~
\theta\right )

Il est clair que le premier cas correspond à
\mathrm{det}~ u = 1 et le
second cas à \mathrm{det}~ u =
-1. Dans le second cas, on vérifie immédiatement que u^2 =
\mathrmId_E, ce qui montre que u est une
symétrie (évidemment orthogonale). Comme
u\neq~\mathrmId et
u\neq~ -\mathrmId, c'est
nécessairement une symétrie par rapport à une droite.

Dans le premier cas, on a
\mathrm{tr}~u =
2cos~ \theta, ce qui montre que
cos~ \theta est indépendant du choix de la base \mathcal{E},
et que donc \theta \in \mathbb{R}~\diagup2\pi~\mathbb{Z} est déterminé au signe près. On vérifie
immédiatement que

\begin{align*} \left
(\matrix\,cos~
\theta&-sin~ \theta \cr
sin \theta&\cos~ \theta
\cr \right )\left
(\matrix\,cos~
\theta'&-sin~ \theta'\cr
sin \theta' &\cos~
\theta'\right )&& \%& \\ &
\quad & = \left
(\matrix\,cos~
(\theta + \theta')&-sin~ (\theta + \theta') \cr
sin (\theta + \theta')&\cos~ (\theta
+ \theta')\right )\%& \\
\end{align*}

ce qui montre que le groupe SO(2) = \R(\theta) =
\left
(\matrix\,cos~
\theta&-sin~ \theta\cr
sin \theta &\cos~
\theta\right )∣\theta \in
\mathbb{R}~\diagup2\pi~\mathbb{Z}\ est commutatif et isomorphe à (\mathbb{R}~\diagup2\pi~\mathbb{Z},+). Soit u
\in SO(E)~; si \mathcal{E} et \mathcal{E}' sont deux bases orthonormées de même sens
(c'est-à-dire que
\mathrm{det}~
P_\mathcal{E}^\mathcal{E}' \textgreater{} 0), alors P =
P_\mathcal{E}^\mathcal{E}'\in SO(2), on a

\mathrmMat~ (u,\mathcal{E}') =
P^-1 \mathrmMat~
(u,\mathcal{E})P =
P^-1P\mathrmMat~
(u,\mathcal{E}) = \mathrmMat~ (u,\mathcal{E})

puisque SO(2) est commutatif et que P et
\mathrmMat~ (u,\mathcal{E}) sont
toutes deux dans SO(2). Donc \theta ne dépend que de l'orientation de la base
\mathcal{E}. Si maintenant, \mathcal{E} et \mathcal{E}' sont deux bases orthonormées de sens contraire
(c'est-à-dire que
\mathrm{det}~
P_\mathcal{E}^\mathcal{E}' \textless{} 0), alors
\mathrmMat~ (u,\mathcal{E})P est une
matrice orthogonale de déterminant - 1. Comme on l'a vu, son carré est
nécessairement l'identité de même que le carré de P, ce qui montre que
\mathrmMat~ (u,\mathcal{E}') =
P^-1 \mathrmMat~
(u,\mathcal{E})P = P\mathrmMat~ (u,\mathcal{E})P
= \mathrmMat~
(u,\mathcal{E})^-1 = R(-\theta) (si
\mathrmMat~ (u,\mathcal{E}) = R(\theta))~:
un changement d'orientation de la base change donc \theta en - \theta.

Définition~12.6.2 Soit E un plan euclidien orienté, u \in SO(E). On
appelle mesure de la rotation u l'unique élément \theta de \mathbb{R}~\diagup2\pi~\mathbb{Z} tel que,
pour toute base orthonormée directe \mathcal{E} de E, on ait
\mathrmMat~ (u,\mathcal{E}) =
\left
(\matrix\,cos~
\theta&-sin~ \theta\cr
sin \theta &\cos~
\theta\right ).

\paragraph{12.6.5 Réduction des endomorphismes orthogonaux}

Théorème~12.6.11 Soit E un espace euclidien et u un endomorphisme
orthogonal de E. Alors il existe une base orthonormée \mathcal{E} de E telle que

\mathrmMat~ (u,\mathcal{E}) =
\left
(\matrix\,I_p&0
&\\ldots~
&\\ldots&\\\ldots~&0
\cr 0 &-I_q&0
&\\ldots&\\\ldots&\\⋮~
\cr \⋮~
&0
&A_1&0&\\ldots&\\⋮~
\cr \⋮~
&\\ldots~
&⋱
&⋱&\mathrel⋱&\⋮~
\cr \⋮~
&\\ldots~
&\\ldots~
&⋱&\mathrel⋱&0
\cr 0
&\\ldots~
&\\ldots~
&\\ldots&0&A_s~\right
)

avec A_i = \left
(\matrix\,cos~
\theta_i&-sin \theta_i~
\cr sin~
\theta_i&cos \theta_i~
\right ), \theta_i \in \mathbb{R}~ \diagdown 2\pi~\mathbb{Z}.

Démonstration Par récurrence sur n = dim~ E. Si
n = 1, alors u = ±\mathrmId_E et le résultat
est évident. Supposons le donc démontré pour tout espace euclidien de
dimension strictement inférieure à n et soit E de dimension n, u \in O(E).
Si u admet une valeur propre \lambda~, soit x un vecteur propre associé. On a
\lambda~\x\
=\ u(x)\
=\ x\, d'où \lambda~ = ±1. La
droite \mathbb{R}~x est stable par u, donc H = (\mathbb{R}~x)^\bot aussi. La
restriction v de u à H est un endomorphisme orthogonal de H et par
l'hypothèse de récurrence, il existe une base orthonormée
(e_2,\\ldots,e_n~)
de H telle que la matrice de v dans cette base soit de la forme voulue.
Alors ( x \over
\x\
,e_2,\\ldots,e_n~)
est une base orthonormée de E et à une permutation près de cette base
(si \lambda~ = -1), la matrice de u dans cette base est de la forme voulue. Si
u n'a pas de valeur propre (réelle), soit \Pi un plan stable par u, dont
l'existence est garantie par un lemme précédent. L'endomorphisme de \Pi
induit par u est un endomorphisme orthogonal de \Pi sans valeur propre,
donc une rotation d'angle \theta_1 \in \mathbb{R}~ \diagdown \pi~\mathbb{Z}. Soit
(e_1,e_2) une base orthonormée de \Pi. Le sous-espace de
dimension n - 2, H = \Pi^\bot est également stable par u et
l'endomorphisme v de H induit par u est un endomorphisme orthogonal de H
sans valeur propre. Par hypothèse de récurrence, il existe une base
orthonormée
(e_3,\\ldots,e_n~)
de H telle que la matrice de v dans cette base soit de la forme voulue
(avec p = q = 0). Alors la matrice de u dans la base orthonormée
(e_1,e_2,e_3,\\ldots,e_n~)
est de la forme voulue, ce qui achève la démonstration.

Exemple~12.6.1 Si dim~ E = 3, on a les formes
réduites possibles (en tenant compte de
\mathrm{det}~ u =
(-1)^q et de p + q + 2s = 3)

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \mathrm{det}~ u = 1

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \left
    (\matrix\,1&0&0 \cr
    0&1&0 \cr 0&0&1\right )
    (identité),
  \item
    \left (\matrix\,1&0
    &0 \cr 0&-1&0 \cr 0&0
    &-1\right ) (retournement d'axe \mathbb{R}~e_1),
  \item
    \left (\matrix\,1&0
    &0 \cr 0&cos~
    \theta&-sin~ \theta \cr
    0&sin \theta&\cos~ \theta
    \right ) (rotation d'axe \mathbb{R}~e_1)
  \end{itemize}
\item
  \mathrm{det}~ u = -1

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \left (\matrix\,-1&0
    &0 \cr 0 &-1&0 \cr 0 &0
    &-1\right )\quad (
    -\mathrmId_E),
  \item
    \left
    (\matrix\,1&0&0 \cr
    0&1&0 \cr 0&0&-1\right )
    (symétrie par rapport au plan
    \mathrmVect(e_1,e_2~)),
  \item
    \left (\matrix\,-1&0
    &0 \cr 0 &cos~
    \theta&-sin~ \theta \cr 0
    &sin \theta&\cos~ \theta
    \right ) (composée de la symétrie par rapport au
    plan
    \mathrmVect(e_2,e_3~)
    et d'une rotation d'axe \mathbb{R}~e_1)
  \end{itemize}
\end{itemize}

\paragraph{12.6.6 Produit vectoriel, produit mixte}

Théorème~12.6.12 Soit E un espace euclidien orienté de dimension n.
L'application n linéaire alternée
\mathrm{det} _\mathcal{E}~ est
indépendante du choix de la base orthonormée directe \mathcal{E}.

Démonstration Si \mathcal{E}' est une autre base orthonormée directe, la matrice
de passage P_\mathcal{E}^\mathcal{E}' est à la fois orthogonale et de
déterminant strictement positif, donc de déterminant 1. Or on a

\mathrm{det} _\mathcal{E}~
= \mathrm{det}~
_\mathcal{E}(\mathcal{E}')\mathrm{det}~
_\mathcal{E}' = \mathrm{det}~
P_\mathcal{E}^\mathcal{E}'\mathrm{det}~
_ \mathcal{E}' = \mathrm{det}~
_\mathcal{E}'

Définition~12.6.3 On notera
{[}x_1,\\ldots,x_n~{]}
= \mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n~)
et on l'appellera le produit mixte des n vecteurs
x_1,\\ldots,x_n~.

Remarque~12.6.3 Il est clair qu'un changement d'orientation de l'espace
change le produit mixte en son opposé.

Théorème~12.6.13 Soit E un espace euclidien orienté. Alors, pour toute
famille
(x_1,\\ldots,x_n~)
de E on a

\mathrm{det}~
Gram(x_1,\\\ldots,x_n~)
=
{[}x_1,\\ldots,x_n{]}^2~

Démonstration Soit \mathcal{E} une base orthonormée directe et soit A la matrice
des coordonnées de
(x_1,\\ldots,x_n~)
dans la base \mathcal{E}. On a alors
\mathrm{det}~ A =
{[}x_1,\\ldots,x_n~{]}.
D'autre part

(x_i∣x_\\jmathmath) =
\sum _k=1^na_
k,ia_k,\\jmathmath = (^tAA)_ i,\\jmathmath

si bien que
Gram(x_1,\\\ldots,x_n~)
= ^tAA. On a donc

\mathrm{det}~
Gram(x_1,\\\ldots,x_n~)
= \mathrm{det}~
^tAA =
(\mathrm{det} A)^2~
= {[}x_
1,\\ldots,x_n{]}^2~

Théorème~12.6.14 (et définition). Soit E un espace euclidien orienté de
dimension n. Soit
x_1,\\ldots,x_n-1~
\in E. Il existe un unique vecteur, appelé le produit vectoriel des n - 1
vecteurs
x_1,\\ldots,x_n-1~
et noté x_1
∧\\ldots~ ∧
x_n-1 tel que

\forall~~y \in E,
{[}x_1,\\ldots,x_n-1~,y{]}
= (x_1
∧\\ldots~ ∧
x_n-1∣y)

Démonstration L'application
y\mapsto~{[}x_1,\\ldots,x_n-1~,y{]}
est une forme linéaire sur E, donc représentée par le produit scalaire
avec un unique vecteur.

Proposition~12.6.15

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) x_1
  ∧\\ldots~ ∧
  x_n-1 = 0 \Leftrightarrow
  (x_1,\\ldots,x_n-1~)
  est une famille liée
\item
  (ii) \forall~i \in {[}1,n - 1{]}, x_1~
  ∧\\ldots~ ∧
  x_n-1 \bot x_i
\item
  (iii) si
  (x_1,\\ldots,x_n-1~)
  est une famille libre, alors
  (x_1,\\ldots,x_n-1,x_1~
  ∧\\ldots~ ∧
  x_n-1) est une base directe de E
\item
  (iv) \x_1
  ∧\\ldots~ ∧
  x_n-1\^2
  = \mathrm{det}~
  Gram(x_1,\\\ldots,x_n-1~).
\end{itemize}

Démonstration (i) On a en effet

\begin{align*}
(x_1,\\ldots,x_n-1~)\text
libre & \Leftrightarrow & \exists~y
\in E,
(x_1,\\ldots,x_n-1~,y)\text
base de E\%& \\ &
\Leftrightarrow & \exists~y \in E,
{[}x_1,\\ldots,x_n-1,y{]}\mathrel\neq~~0
\%& \\ & \Leftrightarrow &
\existsy \in E, (x_1~
∧\\ldots~ ∧
x_n-1∣y)\mathrel\neq~0
\%& \\ & \Leftrightarrow &
x_1
∧\\ldots~ ∧
x_n-1\neq~0 \%&
\\ \end{align*}

(ii) (x_1
∧\\ldots~ ∧
x_n-1∣x_i) =
{[}x_1,\\ldots,x_i,\\\ldots,x_n-1,x_i~{]}
= 0

(iii) On a

\begin{align*}
{[}x_1,\\ldots,x_n-1,x_1~
∧\\ldots~ ∧
x_n-1{]}& =& (x_1
∧\\ldots~ ∧
x_n-1∣x_1
∧\\ldots~ ∧
x_n-1)\%& \\ & =&
\x_1
∧\\ldots~ ∧
x_n-1\^2 \textgreater{} 0
\%& \\ \end{align*}

(iv) Comme on vient de le voir,

\begin{align*}
\x_1
∧\\ldots~ ∧
x_n-1\^4&& \%&
\\ & =&
{[}x_1,\\ldots,x_n-1,x_1~
∧\\ldots~ ∧
x_n-1{]}^2 \%& \\ &
=& \mathrm{det}~
Gram(x_1,\\\ldots,x_n-1,x_1~
∧\\ldots~ ∧
x_n-1) \%& \\ & =&
\left
\matrix\,Gram(x_1,\\\ldots,x_n-1~)&0
\cr 0 &\x_1
∧\\ldots~ ∧
x_n-1\^2\right
 \%& \\ & =&
\x_1
∧\\ldots~ ∧
x_n-1\^2\
\mathrm{det} Gram(x_
1,\\ldots,x_n-1~)\%&
\\ \end{align*}

puisque (x_1
∧\\ldots~ ∧
x_n-1∣x_i) = 0. Si
(x_1,\\ldots,x_n~)
est libre, on peut simplifier par \x_1
∧\\ldots~ ∧
x_n-1\^2 et on obtient
\x_1
∧\\ldots~ ∧
x_n-1\^2
= \mathrm{det}~
Gram(x_1,\\\ldots,x_n-1~),
formule qui est encore exacte si la famille est liée puisque les deux
termes valent 0.

Remarque~12.6.4 Si
(x_1,\\ldots,x_n-1~)
est une famille libre, (ii) définit la droite engendrée par x_1
∧\\ldots~ ∧
x_n-1, (iii) définit son orientation sur cette droite et (iv)
définit sa norme, ce qui fournit une construction géométrique du produit
vectoriel~: c'est le vecteur orthogonal à l'hyperplan
\mathrmVect(x_1,\\\ldots,x_n-1~),
tel que la base
(x_1,\\ldots,x_n-1,x_1~
∧\\ldots~ ∧
x_n-1) soit une base directe de E et dont la norme est
\sqrt\\mathrm{det}
  Gram (x_1 ~ ,
\\ldots~ ,
x_n-1  ).

Coordonnées du produit vectoriel

Soit \mathcal{E} une base orthonormée directe de E, x_\\jmathmath
= \\sum ~
_i=1^n\alpha_i,\\jmathmathe_i, y
= \\sum ~
_i=1^ny_ie_i. On a alors, en développant
le déterminant suivant la dernière colonne

\begin{align*}
{[}x_1,\\ldots,x_n-1~,y{]}&
=& \left
\matrix\,\alpha_1,1&\\ldots&\alpha_1,n-1&y_1~
\cr
\\ldots~
&\\ldots&\\\ldots~
&\\ldots~
\cr
\alpha_n,1&\\ldots&\alpha_n,n-1&y_n~\right
 = \sum _i=1^n\Delta_
iy_i\%& \\ & =&
(\sum _i=1^n\Delta_
ie_i∣y) \%&
\\ \end{align*}

avec

\Delta_i = (-1)^n+i\left
\matrix\,\alpha_1,1
&\\ldots&\alpha_1,n-1~
\cr
\\ldots~
&\\ldots&\\\ldots~
\cr
\alpha_i-1,1&\\ldots&\alpha_i-1,n-1~
\cr
\alpha_i+1,1&\\ldots&\alpha_i+1,n-1~
\cr
\\ldots~
&\\ldots&\\\ldots~
\cr \alpha_n,1
&\\ldots&\alpha_n,n-1~
\right 

On en déduit que

x_1
∧\\ldots~ ∧
x_n-1 = \\sum
_i=1^n\Delta_ ie_i

Produit vectoriel en dimension 3

On a alors \x_1 ∧
x_2\^2
= \mathrm{det}~
Gram(x_1,x_2~)
=\
x_1\^2\x_2\^2
- (x_1∣x_2)^2
=\
x_1\^2\x_2\^2(1
- cos ^2~\theta) où \theta désigne l'angle non
orienté des vecteurs x_1 et x_2. On a donc alors

\x_1 ∧
x_2\ =\
x_1\\,\x_2\\
sin \theta

On a également le résultat important suivant

Théorème~12.6.16 Soit E un espace euclidien de dimension 3,
x_1,x_2,x_3 \in E. Alors

(x_1 ∧ x_2) ∧ x_3 =
(x_1∣x_3)x_2 -
(x_2∣x_3)x_1

Démonstration Si (x_1,x_2) est liée, on a par exemple
x_2 = \lambda~x_1 et on vérifie facilement que les deux
membres valent 0. Si (x_1,x_2) est libre, alors
(x_1 ∧ x_2) ∧ x_3 \in (x_1 ∧
x_2)^\bot =\
\mathrmVect(x_1,x_2), si bien
qu'a priori (x_1 ∧ x_2) ∧ x_3 = \lambda~x_1
+ \mux_2. Un calcul sur les coordonnées dans une base orthonormée
directe adéquate (par exemple ( x_1 \over
\x_1\ ,
x_1∧x_2 \over
\x_1∧x_2\
,\\ldots~)) fournit
les valeurs de \lambda~ et \mu.

Remarque~12.6.5 On pourra utiliser le moyen mnémotechnique suivant~: le
produit scalaire affecté du signe + concerne les deux termes extrêmes de
l'expression (x_1 ∧ x_2) ∧ x_3.

Corollaire~12.6.17 Soit a\neq~0. L'équation x ∧ a
= b a une solution si et seulement si~a \bot b.

Démonstration Il est clair que la condition est nécessaire. Si elle est
vérifiée, cherchons x sous la forme x_0 = \lambda~a ∧ b. On a alors

x_0 ∧ a = \lambda~(a ∧ b) ∧ a = \lambda~(a∣a)b -
\lambda~(a∣b)a =
\lambda~\a\^2b

Donc x_0 = 1 \over
\a\^2 a ∧
b est une solution.

Remarque~12.6.6 On a alors

\begin{align*} x ∧ a = b&
\Leftrightarrow & x ∧ a = x_0 ∧ a
\Leftrightarrow (x - x_0) ∧ a = 0\%&
\\ & \Leftrightarrow & x -
x_0 = \lambda~a \%& \\
\end{align*}

\paragraph{12.6.7 Angles}

On désigne par E un espace euclidien (de dimension finie), par O(E)
(resp. O^+(E)) le groupe orthogonal (resp. le groupe des
rotations de E).

Notion générale d'angles d'ob\\jmathmathets

Soit X un ensemble de parties de E stable par O(E), c'est-à-dire que

\forall~~r \in O(E)\quad
\forall~~A \in X\quad r(A) \in X.

Exemple~: X peut être l'ensemble D(E) des droites de E, ou l'ensemble
\tildeD(\mathcal{E}) des demi-droites de E, ou l'ensemble des
plans de E, ou l'ensemble des hyperplans de E.

Définition~12.6.4 On appelle angle non orienté (resp angle orienté)
d'éléments de X le quotient de X \times X par la relation \mathcal{R} définie par

\begin{align*}
(D_1,D_2)\mathcal{R}(D_1',D_2')
\Leftrightarrow&& \%& \\
& & \exists~r \in O(E) (\textresp.
O^+(E))\quad r(D_ 1) =
D_1'\text et r(D_2) =
D_2'\%& \\
\end{align*}

On notera \overline(D_1,D_2)(resp.
\widehat(D_1,D_2)) la classe
d'équivalence du couple (D_1,D_2) et on l'appellera
l'angle non orienté (resp. l'angle orienté) des ob\\jmathmathets D_1 et
D_2.

Cela revient à définir les angles par les propriétés

\overline(D_1,D_2) =
\overline(D_1',D_2')
\Leftrightarrow \exists~r \in O(E),
\quad r(D_1) = D_1',r(D_2) =
D_2'

et

\widehat(D_1,D_2)
=\widehat (D_1',D_2')
\Leftrightarrow \exists~r \in
O^+(E), \quad r(D_ 1) =
D_1',r(D_2) = D_2'

On s'intéressera par la suite uniquement au cas où X est l'ensemble D(E)
des droites de E ou l'ensemble \tildeD(\mathcal{E}) des
demi-droites de E (angles de droites ou de demi droites).

Comparaison des angles orientés et non orientés

Théorème~12.6.18 Si dim~ E ≥ 3 les notions
d'angles orientés ou non orientés coïncident aussi bien pour les droites
que pour les demi-droites, c'est-à-dire que

\existsr \in O(E)\quad r(D_1~)
= D_1'\text et r(D_2) =
D_2'

si et seulement si

\existsr \in O^+~(E)\quad
r(D_ 1) = D_1'\text et
r(D_2) = D_2'.

Démonstration L'implication '' ⇐'' est claire, et si la propriété de
gauche est vérifiée, soit r appartient à O^+(E) et c'est
terminé, soit r appartient à O^-(E), mais alors il suffit de
composer r par une symétrie s par rapport à un hyperplan contenant
D_1' et D_2' pour trouver un r' = s \cdot r \in
O^+(E) tel que r'(D_1) = D_1' et
r'(D_2) = D_2', car s laisse invariantes D_1'
et D_2'.

Mesure des angles non orientés de droites ou de demi-droites

Pour (D_1,D_2) \inD(E)^2, on définit
\phi(D_1,D_2) de la manière suivante~: soit x_1
un vecteur directeur de D_1 et x_2 un vecteur
directeur de D_2, le réel 
(x_1∣x_2)
\over
\x_1\
\x_2\ \in
{[}0,1{]} est indépendant du choix des vecteurs directeurs de
D_1 et D_2 (changer x_1 en \lambda~x_1 et
x_2 en \mux_2 avec \lambda~\neq~0 et
\mu\neq~0 ne change pas sa valeur), on le définit
comme \phi(D_1,D_2).

Théorème~12.6.19 \forall~(D_1,D_2~)
\inD(E)^2\quad
\overline(D_1,D_2) =
\overline(D_1',D_2')\quad
\Leftrightarrow \quad
\phi(D_1,D_2) = \phi(D_1',D_2').

Démonstration ( \rigtharrow~). Il suffit de remarquer que si r \in O(E) alors


(r(x_1)∣r(x_2))
\over
\r(x_1)\
\r(x_2)\
=
(x_1∣x_2)
\over
\x_1\
\x_2\ .

( ⇐). Supposons que \phi(D_1,D_2) =
\phi(D_1',D_2')\neq~1. Soit
d'abord r \in O(E) qui envoie D_1 sur D_1' et le plan
Vect(D_1,D_2) sur le plan
Vect(D_1',D_2') (la construire en prenant des bonnes
bases orthonormées). Alors si on pose D_3 = r(D_2), on
a \phi(D_1',D_3) = \phi(D_1,D_2) =
\phi(D_1',D_2'). Dans le plan
Vect(D_1',D_2') (qui contient les trois droites
D_1', D_2' et D_3) ceci impose que soit
D_3 = D_2' (et dans ce cas on a trouvé r tel que
r(D_1) = D_1' et r(D_2) = D_2'),
soit D_3 et D_2' sont symétriques par rapport à
D_1, auquel cas en composant r par la symétrie orthogonale par
rapport à l'hyperplan D_1' \oplus~ V
ect(D_1',D_2')^\bot on trouve un r' tel que
r'(D_1) = D_1' et r'(D_2) = D_2'.

Si \phi(D_1,D_2) = \phi(D_1',D_2') = 1, on
a D_1 = D_2, D_1' = D_2' et il
suffit de choisir un r tel que r(D_1) = D_1'.

Définition~12.6.5 On appelle mesure de l'angle non orienté des droites
D_1 et D_2 l'unique réel \theta \in {[}0,\pi~\diagup2{]} tel que
cos \theta = \phi(D_1,D_2~).

Le théorème précédent montre que deux angles non orientés de droites
sont égaux si et seulement si leurs mesures sont égales.

Pour les demi-droites on suit un plan analogue en posant cette fois
\phi(D_1,D_2) =
(x_1∣x_2)
\over
\x_1\
\x_2\ \in
{[}-1,1{]} qui ne dépend pas du choix des vecteurs directeurs des
demi-droites (car cette fois \lambda~ et \mu sont nécessairement positifs). On a
le même théorème (avec une démonstration analogue) et on peut donc poser

Définition~12.6.6 On appelle mesure de l'angle non orienté des
demi-droites D_1 et D_2 l'unique réel \theta \in {[}0,\pi~{]}
tel que cos \theta = \phi(D_1,D_2~).

Le théorème montre que deux angles non orientés de demi-droites sont
égaux si et seulement si leurs mesures sont égales.

Angles orientés de demi-droites dans le plan euclidien

On notera \tildeA(\mathcal{E}) l'ensemble des angles orientés
de demi-droites du plan euclidien E.

Théorème~12.6.20 Soit D \in\tildeD(\mathcal{E}). Alors
l'application f : O^+(E) \rightarrow~\tildeA(\mathcal{E}),
r\mapsto~\widehat(D,r(D)) est une
bi\\jmathmathection qui ne dépend pas du choix de D.

Démonstration Pour l'in\\jmathmathectivité, si on a f(r) = f(r') c'est qu'il
existe r'' \in O^+(E) tel que r''(D) = D et r'`\cdot r(D) = r'(D).
Mais la première relation impose que r'' = \mathrmId
(une rotation du plan euclidien qui laisse invariante une demi-droite
est l'identité) et la deuxième que r^-1 \cdot r'(D) = D soit r' =
r pour la même raison. En ce qui concerne l'indépendance de D il suffit
de remarquer que si D' \in\tildeD(\mathcal{E}), il existe
r_0 \in O^+(E) tel que D' = r_0(D) et alors

\widehat(D',r(D')) =\widehat
(r_0(D),r \cdot r_0(D)) =\widehat
(r_0(D),r_0 \cdot r(D)) =\widehat
(D,r(D))

(on voit ici le rôle essentiel de la commutativité de O^+(E)
en dimension 2). La sur\\jmathmathectivité provient alors du fait que tout angle
\widehat(D_1,D_2) est de la forme
\widehat(D_1,r(D_1)) = f(r) en
utilisant une rotation r qui envoie D_1 sur D_2.

Cette bi\\jmathmathection naturelle permet de transporter la structure de groupe
commutatif de O^+(E) à \tildeA(\mathcal{E}) en
posant~:

si \theta_1 = f(r_1) et \theta_2 = f(r_2) ,
alors on pose \theta_1 + \theta_2 = f(r_1 \cdot
r_2).

On définit de plus l'angle nul comme étant
f(\mathrmId) =\widehat (D,D),
l'angle plat comme étant f(-\mathrmId)
=\widehat (D,-D).

Théorème~12.6.21 (relation de Chasles)
\widehat(D_1,D_2)
+\widehat (D_2,D_3)
=\widehat (D_1,D_3).

Démonstration Soit r_1 \in O^+(E) telle que
r_1(D_1) = D_2 et r_2 \in
O^+(E) telle que r_2(D_2) = D_3.
Alors

\widehat(D_1,D_2)
+\widehat (D_2,D_3) =
f(r_1) + f(r_2) = f(r_2 \cdot r_1)
=\widehat (D_1,r_2 \cdot
r_1(D_1)) =\widehat
(D_1,D_3)

On retrouve alors sans difficulté toutes les propriétés des angles de
demi-droites. Comme de plus, si E est orienté, on connait un
isomorphisme de groupes abéliens entre \mathbb{R}~\diagup2\pi~\mathbb{Z} et O^+(E), on en
déduit un isomorphisme entre \tildeA(\mathcal{E}) et \mathbb{R}~\diagup2\pi~\mathbb{Z} qui
permet de mesurer les angles modulo 2\pi~ et d'effectuer les calculs (en
particulier les divisions par 2) dans \mathbb{R}~\diagup2\pi~\mathbb{Z}. On en déduit par exemple
facilement qu'un couple de demi-droites a deux bissectrices opposés.

Angles orientés de droites dans le plan euclidien

On notera A(E) l'ensemble des angles orientés de droites du plan
euclidien E. La seule différence est que f n'est plus in\\jmathmathective, mais
que f(r_1) = f(r_2) \Leftrightarrow
r_2 = ±r_1. On en déduit que f induit une bi\\jmathmathection
\tildef de O^+(E)\diagup\
±\mathrmId\ sur A(E). On peut donc
encore transporter la structure de groupe de
O^+(E)\diagup\
±\mathrmId\ sur A(E) et on obtient
encore une relation de Chasles. Si E est orienté, on obtient un
isomorphisme de A(E) avec \mathbb{R}~\diagup\pi~\mathbb{Z} qui permet de mesurer les angles de
droites modulo \pi~.

Exemple~12.6.2 L'application \theta\mapsto~2\theta est un
isomorphisme de groupes de \mathbb{R}~\diagup\pi~\mathbb{Z} sur \mathbb{R}~\diagup2\pi~\mathbb{Z}. On en déduit un isomorphisme
encore noté \theta\mapsto~2\theta de A(E) sur
\tildeA(\mathcal{E}). Soit D_1 et D_2 deux
droites de E, s_1 et s_2 les symétries orthogonales
par rapport à ces droites. Montrer que s_2 \cdot s_1 est
la rotation d'angle (de demi-droites)
2\widehat(D_1,D_2).

{[}
{[}
{[}
{[}

\end{document}
