
\section{13.1 Compléments sur la conjugaison}

\subsection{13.1.1 Applications semi-linéaires}

Définition~13.1.1 Soit E et F deux \mathbb{C}-espaces vectoriels et u : E \rightarrow~ F. On
dit que u est semi-linéaire si elle vérifie

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) \forall~~x,y \in E, u(x + y) = u(x) + u(y)
\item
  (ii) \forall~\lambda~ \in \mathbb{C}, \\forall~~x \in
  E, u(\lambda~x) = \overline\lambda~u(x).
\end{itemize}

Remarque~13.1.1 Soit E un \mathbb{C}-espace vectoriel . On munit E d'une autre
structure d'espace vectoriel, notée \checkE en posant
\lambda~ ∗ x = \overline\lambda~x. Une application semi-linéaire de
E dans F n'est autre qu'une application linéaire de E dans
\checkF. Ceci permet d'appliquer aux applications
semi-linéaires la plupart des résultats sur les applications linéaires
en tenant compte des résultats suivants dont la démonstration est
élémentaire~:

\begin{itemize}
\item
  a) une famille (x_i)_i\inI d'éléments de E est libre
  (resp. génératrice, resp. base) dans \checkE si et
  seulement si~il en est de même dans E
\item
  b) \mathrmrg~
  _\checkE(x_i)_i\inI
  = \mathrmrg~
  _E(x_i)_i\inI, dim~
  \checkE = dim~ E
\item
  c) F est un sous-espace vectoriel de \checkE si et
  seulement si~c'est un sous-espace vectoriel de E
\item
  d) le théorème du rang s'applique aux applications semi-linéaires~; en
  particulier, si u : E \rightarrow~ F est semi-linéaire entre deux espaces de même
  dimension finie, alors u est injective si et seulement si~elle est
  surjective
\item
  e) si l'on définit A =\
  \mathrmMat (u,\mathcal{E},ℱ) par u(e_j)
  = \\sum ~
  _ia_i,jf_i (notations évidentes) alors

  y = u(x) \Leftrightarrow Y =
  A\overlineX
\item
  f) la composée de deux applications semi-linéaires n'est pas
  semi-linéaire, mais au contraire linéaire.
\end{itemize}

\subsection{13.1.2 Matrices conjuguées et transconjuguées}

Définition~13.1.2 Soit A = (a_i,j)_1\leqi\leqm,1\leqj\leqn \in
M_\mathbb{C}(m,n). On appelle matrice conjuguée de A la matrice
\overlineA =
(\overlinea_i,j)_1\leqi\leqm,1\leqj\leqn \in
M_\mathbb{C}(m,n).

Proposition~13.1.1 L'application
A\mapsto~\overlineA est un
automorphisme semi-linéaire de M_\mathbb{C}(m,n). On a
\mathrmrg\overlineA~
= \mathrmrg~A. Si A \in
M_\mathbb{C}(m,n) et B \in M_\mathbb{C}(n,p), alors
\overlineAB =
\overlineA\,\overlineB.
Dans le cadre des matrices carrées, on a
\mathrm{det}~
\overlineA =
\overline\mathrm{det}~
A,
\mathrm{tr}\overlineA~
=
\overline\mathrm{tr}A~,
\chi_\overlineA(X) =
\overline\chi_A(X), A est inversible si et
seulement si~\overlineA est inversible, et dans ce
cas (\overlineA)^-1 =
\overlineA^-1.

Démonstration Vérification élémentaire laissée au lecteur.

Définition~13.1.3 Soit A = (a_i,j)_1\leqi\leqm,1\leqj\leqn \in
M_\mathbb{C}(m,n). On appelle matrice transconjuguée (ou matrice
adjointe) de A la matrice A^∗ =
^t(\overlineA) =
\overline^tA =
(\overlinea_j,i)_1\leqi\leqm,1\leqj\leqn \in
M_\mathbb{C}(n,m).

A partir des propriétés de A\mapsto~^tA
et de A\mapsto~\overlineA on
déduit facilement les propriétés suivantes

Proposition~13.1.2 L'application
A\mapsto~A^∗ est un isomorphisme
semi-linéaire involutif de M_\mathbb{C}(m,n) sur M_\mathbb{C}(n,m). On a
\mathrmrgA^∗~
= \mathrmrg~A. Si A \in
M_\mathbb{C}(m,n) et B \in M_\mathbb{C}(n,p), alors (AB)^∗ =
B^∗A^∗. Dans le cadre des matrices carrées, on a
\mathrm{det} A^∗~ =
\overline\mathrm{det}~
A,
\mathrm{tr}A^∗~ =
\overline\mathrm{tr}A~,
\chi_A^∗(X) =
\overline\chi_A(X), A est inversible si et
seulement si~A^∗ est inversible, et dans ce cas
(A^∗)^-1 = (A^-1)^∗.

Remarque~13.1.2 On prendra garde à la relation (\lambda~A)^∗ =
\overline\lambda~A^∗ en n'oubliant pas la
conjugaison.

\subsection{13.1.3 Matrices hermitiennes, antihermitiennes}

Définition~13.1.4 Soit A \in M_\mathbb{C}(n). on dit que A est hermitienne
(resp. antihermitienne) si A^∗ = A (resp. A^∗ =
-A).

Remarque~13.1.3 A = (a_i,j) est hermitienne si et seulement
si~\forall~i,j, a_j,i~ =
\overlinea_i,j. En particulier les
coefficients diagonaux a_i,i doivent être réels

Théorème~13.1.3 Les ensembles des matrices hermitiennes et
antihermitiennes sont des \mathbb{R}~-sous-espaces vectoriels (mais pas des
\mathbb{C}-sous-espaces vectoriels) de M_\mathbb{C}(n). On a

A\text hermitienne  \Leftrightarrow
iA\text antihermitienne

Si ℋ_n désigne le \mathbb{R}~-sous-espace vectoriel des matrices
hermitiennes, on a M_\mathbb{C}(n) = ℋ_n \oplus~ iℋ_n.

Démonstration La vérification du premier point est élémentaire. Si on a
A = A_1 + iA_2 avec A_1 et A_2
hermitiennes, alors A^∗ = A_1 - iA_2 ce qui
donne A_1 = 1 \over 2 (A + A^∗)
et A_2 = 1 \over 2i (A - A^∗) et
démontre déjà l'unicité de la décomposition. De plus la formule

A = 1 \over 2 (A + A^∗) + i 1
\over 2i (A - A^∗)

avec  1 \over 2 (A + A^∗) et  1
\over 2i (A - A^∗) qui sont hermitiennes
(facile) montre l'existence de la décomposition.

Remarque~13.1.4 On voit donc que contrairement aux matrices symétriques
ou antisymétriques qui sont de nature différentes, il n'y a pas de
différence essentielle entre matrices hermitiennes ou antihermitiennes~:
on passe des unes aux autres par multiplication par i, ce qui permet de
limiter l'étude aux matrices hermitiennes. Pour une telle matrice, les
formules \mathrm{det}~
A^∗ =
\overline\mathrm{det}~
A,
\mathrm{tr}A^∗~ =
\overline\mathrm{tr}A~,
\chi_A^∗(X) =
\overline\chi_A(X) montrent que
\mathrm{det}~ A \in \mathbb{R}~,
\mathrm{tr}~A \in \mathbb{R}~ et que
\chi_A(X) \in \mathbb{R}~[X].

Voici le texte LaTeX corrigé avec l'utilisation des environnements demandés, l'indexation des mots-clés (notamment les définitions), et la suppression de la numérotation manuelle :

\section{Compléments sur la conjugaison}

\subsection{Applications semi-linéaires}

\begin{de}
\index{application semi-linéaire}
Soit $E$ et $F$ deux $\mathbb{C}$-espaces vectoriels et $u : E \rightarrow F$. On dit que $u$ est semi-linéaire si elle vérifie
\begin{enumerate}
\item $\forall x,y \in E, u(x + y) = u(x) + u(y)$
\item $\forall \lambda \in \mathbb{C}, \forall x \in E, u(\lambda x) = \overline{\lambda} u(x)$.
\end{enumerate}
\end{de}

\begin{rem}
Soit $E$ un $\mathbb{C}$-espace vectoriel. On munit $E$ d'une autre structure d'espace vectoriel, notée $\check{E}$ en posant $\lambda * x = \overline{\lambda} x$. Une application semi-linéaire de $E$ dans $F$ n'est autre qu'une application linéaire de $E$ dans $\check{F}$. Ceci permet d'appliquer aux applications semi-linéaires la plupart des résultats sur les applications linéaires en tenant compte des résultats suivants dont la démonstration est élémentaire :

\begin{enumerate}
\item Une famille $(x_i)_{i\in I}$ d'éléments de $E$ est libre (resp. génératrice, resp. base) dans $\check{E}$ si et seulement si il en est de même dans $E$
\item $\mathrm{rg}_{\check{E}}(x_i)_{i\in I} = \mathrm{rg}_E(x_i)_{i\in I}, \dim \check{E} = \dim E$
\item $F$ est un sous-espace vectoriel de $\check{E}$ si et seulement si c'est un sous-espace vectoriel de $E$
\item Le théorème du rang s'applique aux applications semi-linéaires ; en particulier, si $u : E \rightarrow F$ est semi-linéaire entre deux espaces de même dimension finie, alors $u$ est injective si et seulement si elle est surjective
\item Si l'on définit $A = \mathrm{Mat}(u,\mathcal{E},\mathcal{F})$ par $u(e_j) = \sum_i a_{i,j} f_i$ (notations évidentes) alors

$y = u(x) \Leftrightarrow Y = A \overline{X}$

\item La composée de deux applications semi-linéaires n'est pas semi-linéaire, mais au contraire linéaire.
\end{enumerate}
\end{rem}

\subsection{Matrices conjuguées et transconjuguées}

\begin{de}
\index{matrice conjuguée}
Soit $A = (a_{i,j})_{1\leq i\leq m,1\leq j\leq n} \in M_\mathbb{C}(m,n)$. On appelle matrice conjuguée de $A$ la matrice $\overline{A} = (\overline{a_{i,j}})_{1\leq i\leq m,1\leq j\leq n} \in M_\mathbb{C}(m,n)$.
\end{de}

\begin{prop}
L'application $A \mapsto \overline{A}$ est un automorphisme semi-linéaire de $M_\mathbb{C}(m,n)$. On a $\mathrm{rg} \overline{A} = \mathrm{rg} A$. Si $A \in M_\mathbb{C}(m,n)$ et $B \in M_\mathbb{C}(n,p)$, alors $\overline{AB} = \overline{A} \overline{B}$. Dans le cadre des matrices carrées, on a $\det \overline{A} = \overline{\det A}$, $\mathrm{tr} \overline{A} = \overline{\mathrm{tr} A}$, $\chi_{\overline{A}}(X) = \overline{\chi_A(X)}$, $A$ est inversible si et seulement si $\overline{A}$ est inversible, et dans ce cas $(\overline{A})^{-1} = \overline{A^{-1}}$.
\end{prop}

\begin{proof}
Vérification élémentaire laissée au lecteur.
\end{proof}

\begin{de}
\index{matrice transconjuguée}
\index{matrice adjointe}
Soit $A = (a_{i,j})_{1\leq i\leq m,1\leq j\leq n} \in M_\mathbb{C}(m,n)$. On appelle matrice transconjuguée (ou matrice adjointe) de $A$ la matrice $A^* = {}^t(\overline{A}) = \overline{{}^t A} = (\overline{a_{j,i}})_{1\leq i\leq m,1\leq j\leq n} \in M_\mathbb{C}(n,m)$.
\end{de}

A partir des propriétés de $A \mapsto {}^t A$ et de $A \mapsto \overline{A}$ on déduit facilement les propriétés suivantes

\begin{prop}
L'application $A \mapsto A^*$ est un isomorphisme semi-linéaire involutif de $M_\mathbb{C}(m,n)$ sur $M_\mathbb{C}(n,m)$. On a $\mathrm{rg} A^* = \mathrm{rg} A$. Si $A \in M_\mathbb{C}(m,n)$ et $B \in M_\mathbb{C}(n,p)$, alors $(AB)^* = B^* A^*$. Dans le cadre des matrices carrées, on a $\det A^* = \overline{\det A}$, $\mathrm{tr} A^* = \overline{\mathrm{tr} A}$, $\chi_{A^*}(X) = \overline{\chi_A(X)}$, $A$ est inversible si et seulement si $A^*$ est inversible, et dans ce cas $(A^*)^{-1} = (A^{-1})^*$.
\end{prop}

\begin{rem}
On prendra garde à la relation $(\lambda A)^* = \overline{\lambda} A^*$ en n'oubliant pas la conjugaison.
\end{rem}

\subsection{Matrices hermitiennes, antihermitiennes}

\begin{de}
\index{matrice hermitienne}
\index{matrice antihermitienne}
Soit $A \in M_\mathbb{C}(n)$. on dit que $A$ est hermitienne (resp. antihermitienne) si $A^* = A$ (resp. $A^* = -A$).
\end{de}

\begin{rem}
$A = (a_{i,j})$ est hermitienne si et seulement si $\forall i,j, a_{j,i} = \overline{a_{i,j}}$. En particulier les coefficients diagonaux $a_{i,i}$ doivent être réels
\end{rem}

\begin{thm}
Les ensembles des matrices hermitiennes et antihermitiennes sont des $\mathbb{R}$-sous-espaces vectoriels (mais pas des $\mathbb{C}$-sous-espaces vectoriels) de $M_\mathbb{C}(n)$. On a

$A \text{ hermitienne} \Leftrightarrow iA \text{ antihermitienne}$

Si $\mathcal{H}_n$ désigne le $\mathbb{R}$-sous-espace vectoriel des matrices hermitiennes, on a $M_\mathbb{C}(n) = \mathcal{H}_n \oplus i\mathcal{H}_n$.
\end{thm}

\begin{proof}
La vérification du premier point est élémentaire. Si on a $A = A_1 + iA_2$ avec $A_1$ et $A_2$ hermitiennes, alors $A^* = A_1 - iA_2$ ce qui donne $A_1 = \frac{1}{2}(A + A^*)$ et $A_2 = \frac{1}{2i}(A - A^*)$ et démontre déjà l'unicité de la décomposition. De plus la formule

$A = \frac{1}{2}(A + A^*) + i\frac{1}{2i}(A - A^*)$

avec $\frac{1}{2}(A + A^*)$ et $\frac{1}{2i}(A - A^*)$ qui sont hermitiennes (facile) montre l'existence de la décomposition.
\end{proof}

\begin{rem}
On voit donc que contrairement aux matrices symétriques ou antisymétriques qui sont de nature différentes, il n'y a pas de différence essentielle entre matrices hermitiennes ou antihermitiennes : on passe des unes aux autres par multiplication par $i$, ce qui permet de limiter l'étude aux matrices hermitiennes. Pour une telle matrice, les formules $\det A^* = \overline{\det A}$, $\mathrm{tr} A^* = \overline{\mathrm{tr} A}$, $\chi_{A^*}(X) = \overline{\chi_A(X)}$ montrent que $\det A \in \mathbb{R}$, $\mathrm{tr} A \in \mathbb{R}$ et que $\chi_A(X) \in \mathbb{R}[X]$.
\end{rem}