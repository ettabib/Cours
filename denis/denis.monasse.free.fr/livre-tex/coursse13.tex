\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{graphicx}
% Redefine \includegraphics so that, unless explicit options are
% given, the image width will not exceed the width of the page.
% Images get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\ScaleIfNeeded{%
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother
\let\Oldincludegraphics\includegraphics
{%
 \catcode`\@=11\relax%
 \gdef\includegraphics{\@ifnextchar[{\Oldincludegraphics}{\Oldincludegraphics[width=\ScaleIfNeeded]}}%
}%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Determinants},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.paragraphHead, .likeparagraphHead { margin-top:2em; font-weight: bold;}
.subparagraphHead, .likesubparagraphHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:\jmathustify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.subsubsectionToc, .likesubsubsectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Determinants}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}
{[}{]}
{[}

\subsubsection{2.7 Déterminants}

\paragraph{2.7.1 Formes p-linéaires}

Définition~2.7.1 On appelle forme p-linéaire sur le Kespace vectoriel E
toute application \phi : E^p \rightarrow~ K vérifiant
\forall~(a\_1,\\\ldots,a\_p~)
\in K^p, \forall~~i \in {[}1,p{]},
x\_i\mapsto~\phi(a\_1,\\ldots,a\_i-1,x\_i,a\_i+1,\\\ldots,a\_p~)
est linéaire de E dans K.

Définition~2.7.2 Soit f une forme p-linéaire sur E. On dit qu'elle est

\begin{itemize}
\item
  (i) alternée si
  \forall~(x\_1\\\ldots,x\_p~)
  \in E^p,

  \left (\exists~(i,\jmath) \in
  {[}1,p{]}^2,i\neq~\jmath\text
  et x\_ i = x\_\jmath\right ) \rigtharrow~
  f(x\_1,\\ldots,x\_p~)
  = 0
\item
  (ii) antisymétrique si
  \forall~(x\_1\\\ldots,x\_p~)
  \in E^p,

  \forall~\sigma \inS\_p~,
  f(x\_\sigma(1),\\ldots,x\_\sigma(p)~)
  =
  \epsilon(\sigma)f(x\_1,\\ldots,x\_p~)
\end{itemize}

Proposition~2.7.1 Toute forme alternée est antisymétrique. Si
carK\mathrel\neq~~2, toute forme
antisymétrique est alternée.

Démonstration On remarque que si f est alternée, on a

\begin{align*} 0& =&
f(\\ldots,x\_i~
+
x\_\jmath,\\ldots,x\_i~
+
x\_\jmath,\\ldots~)
\%& \\ & =&
f(\\ldots,x\_i,\\\ldots,x\_\jmath,\\\ldots~)
+
f(\\ldots,x\_\jmath,\\\ldots,x\_i,\\\ldots~)\%&
\\ \end{align*}

On a donc
f(x\_\sigma(1),\\ldots,x\_\sigma(p)~)
=
\epsilon(\sigma)f(x\_1,\\ldots,x\_p~)
lorsque \sigma est la transposition \tau\_i,\jmath. Comme les transpositions
engendrent S\_p, f est antisymétrique. Inversement si f est
antisymétrique et si x\_i = x\_\jmath, soit \sigma la
transposition \tau\_i,\jmath. On a alors

\begin{align*}
f(\\ldots,x\_i,\\\ldotsx\_\jmath,\\\ldots~)&
=&
-f(\\ldots,x\_\jmath,\\\ldotsx\_i,\\\ldots~)\%&
\\ & =&
-f(\\ldots,x\_i,\\\ldotsx\_\jmath,\\\ldots~)\%&
\\ \end{align*}

soit
f(\\ldots,x\_i,\\\ldotsx\_\jmath,\\\ldots~)
= 0 si carK\mathrel\neq~~2.

Définition~2.7.3 On note A\_p(E) l'espace vectoriel des formes
p-linéaires alternées sur E.

Théorème~2.7.2 Toute forme alternée est nulle sur une famille liée.

Démonstration Il suffit d'écrire un terme comme combinaison linéaire des
autres et de développer. Dans tous les termes obtenus figurent deux
termes identiques et donc chaque terme est nul~: si x\_k
= \\sum ~
\_\jmath\neq~k\alpha~\_\jmathx\_\jmath, on a

[} \varphi (x\_1,\ldots
,x\_k,\ldots
,x\_p)=\sum\_\jmath\ne
k\alpha \_\jmath\varphi
(x\_1,\ldots,\overbracex\_\jmath^\jmath,\ldots
,\overbracex\_\jmath^k,\ldots ,x\_p)=0
]}

où l'on a indiqué au dessus de x\_\jmath l'indice de sa position.

\paragraph{2.7.2 Déterminant d'une famille de vecteurs}

Définition~2.7.4 Soit E un K-espace vectoriel de dimension n, \mathcal{E} =
(e\_1,\\ldots,e\_n~)
une base de E de base duale \mathcal{E}^∗ =
(e\_1^∗,\\ldots,e\_n^∗~),
et
(x\_1,\\ldots,x\_n~)
une famille de vecteurs de E. On pose
\mathrm{det}~
\_\mathcal{E}(x\_1,\\ldots,x\_n~)
= \\sum ~
\_\sigma\inS\_n\epsilon(\sigma)\\∏
 \_\jmath=1^ne\_\jmath^∗(x\_\sigma(\jmath)).

Théorème~2.7.3 \mathrm{det}~
\_\mathcal{E}\in A\_n(E). L'espace vectoriel A\_n(E) est de
dimension 1~: pour toute f \in A\_n(E), on a f =
\lambda~\_f \mathrm{det}~
\_\mathcal{E} avec \lambda~\_f =
f(e\_1,\\ldots,e\_n~).
L'application \mathrm{det}~
\_\mathcal{E} est l'unique forme n-linéaire alternée vérifiant
f(e\_1,\\ldots,e\_n~)
= 1.

Démonstration \mathrm{det}~
\_\mathcal{E} est clairement n-linéaire. Supposons que x\_i =
x\_\jmath et écrivons S\_n = A \cup \tau\_i,\jmathA où A est
l'ensemble des permutations de signature +1. On a donc, en tenant compte
de \epsilon(\tau\_i,\jmath\sigma) = -\epsilon(\sigma)

\begin{align*}
\mathrm{det}~
\_\mathcal{E}(x\_1,\\ldots,x\_n~)&
=& \\sum
\_\sigma\inA\epsilon(\sigma)\∏
\_k=1^ne\_ k^∗(x\_ \sigma(k)) \%&
\\ & -& \\sum
\_\sigma\inA\epsilon(\sigma)\∏
\_k=1^ne\_ k^∗(x\_
\tau\_i,\jmath\sigma(k))\%& \\
\end{align*}

Mais on a pour tout k \in {[}1,n{]}, x\_\tau\_i,\jmath\sigma(k) =
x\_\sigma(k)~: c'est évident si
\sigma(k)∉\i,\jmath\
car alors \tau\_i,\jmath\sigma(k) = \sigma(k), et si par exemple \sigma(k) = i, on a
x\_\tau\_i,\jmath\sigma(k) = x\_\jmath = x\_i =
x\_\sigma(k). On en déduit que dans la différence précédente, les
deux sommes sont égales, et donc
\mathrm{det}~
\_\mathcal{E}(x\_1,\\ldots,x\_n~)
= 0. Ceci montre le caractère alterné de
\mathrm{det} \_\mathcal{E}~.

Soit maintenant f \in A\_n(E). On pose x\_\jmath
= \\sum ~
\_i=1^n\xi\_i,\jmathe\_i. On a alors

f(x\_1,\\ldots,x\_n~)
= \\sum
\_i\_1,\ldots,i\_n\in\mathbb{N}~\xi\_i\_1,1\\ldots\xi\_i\_n,nf(e\_i\_1,\\ldots,e\_i\_n~)

En fait dans cette somme on peut se limiter aux
i\_1,\\ldots,i\_n~
distincts, car sinon
f(e\_i\_1,\\ldots,e\_i\_n~)
= 0. Posant i\_1 =
\sigma(1),\\ldots,i\_n~
= \sigma(n) où \sigma est une permutation de {[}1,n{]}, on obtient (compte tenu de
f(e\_i\_1,\\ldots,e\_i\_n~)
=
f(e\_\sigma(1),\\ldots,e\_\sigma(n)~)
=
\epsilon(\sigma)f(e\_1,\\ldots,e\_n~))

\begin{align*}
f(x\_1,\\ldots,x\_n~)&
=&
f(e\_1,\\ldots,e\_n~)\\sum
\_\sigma\inS\_n\epsilon(\sigma)\xi\_\sigma(1),1\ldots\xi\_\sigma(n),n~\%&
\\ & =&
f(e\_1,\\ldots,e\_n)f\_0(x\_1,\\\ldots,x\_n~)
\%& \\ \end{align*}

avec une définition évidente de f\_0.

Ceci montre clairement que dim A\_n~(E)
\leq 1. Comme d'autre part,
\mathrm{det} \_\mathcal{E}~ est
non nul (car on vérifie immédiatement que
\mathrm{det}~
\_\mathcal{E}(e\_1,\\ldots,e\_n~)
= 1~: il y a un seul terme non nul dans la somme), c'est qu'il est bien
de dimension 1. Le reste en résulte immédiatement (ainsi que le fait que
f\_0 = \mathrm{det}~
\_\mathcal{E}).

Théorème~2.7.4 Soit \mathcal{E} =
(e\_1,\\ldots,e\_n~)
une base de E et
(x\_1,\\ldots,x\_n~)
une famille de E. Alors c'est une base de E si et seulement si
\mathrm{det}~
\_\mathcal{E}(x\_1,\\ldots,x\_n)\mathrel\neq~~0.

Démonstration En effet, si c'est une base X, on a
\mathrm{det} \_\mathcal{E}~
= \mathrm{det}~
\_\mathcal{E}(x\_1,\\ldots,x\_n)\\mathrm{det}~
\_X et comme
\mathrm{det}~
\_\mathcal{E}\neq~0, on a
\mathrm{det}~
\_\mathcal{E}(x\_1,\\ldots,x\_n)\mathrel\neq~~0~;
si ce n'est pas une base, c'est que la famille est liée (son cardinal
est n) et donc \mathrm{det}~
\_\mathcal{E}(x\_1,\\ldots,x\_n~)
= 0.

\paragraph{2.7.3 Déterminant d'un endomorphisme}

Théorème~2.7.5 Soit u \in L(E). Il existe un unique scalaire noté
\mathrm{det}~ u vérifiant
\forall~f \in A\_n~(E),
\forall~(x\_1,\\\ldots,x\_n~)
\in E^n~;

f(u(x\_1),\\ldots,u(x\_n~))
= \mathrm{det}~ u
f(x\_1,\\ldots,x\_n~)

Démonstration Soit \phi\_u : A\_n(E) \rightarrow~ A\_n(E)
définie par
\phi\_u(f)(x\_1,\\ldots,x\_n~)
=
f(u(x\_1),\\ldots,u(x\_n~)).
C'est un endomorphisme d'un espace vectoriel de dimension 1, donc une
homothétie~; on note
\mathrm{det}~ u son rapport.

Proposition~2.7.6

\begin{itemize}
\item
  (i) Soit \mathcal{E} =
  (e\_1,\\ldots,e\_n~)
  une base de E, alors

  \mathrm{det}~ u
  = \mathrm{det}~
  \_\mathcal{E}(u(e\_1),\\ldots,u(e\_n~))
\item
  (ii) \mathrm{det}~
  \mathrmId = 1,
  \mathrm{det}~ \lambda~u =
  \lambda~^n \mathrm{det}~
  u, \mathrm{det}~
  ^tu = \mathrm{det}~
  u
\item
  (iii) \mathrm{det}~ v \cdot u
  = \mathrm{det}~
  v\mathrm{det}~ u
\item
  (iv) u est un automorphisme de E si et seulement si
  \mathrm{det}~
  u\neq~0.
\end{itemize}

Démonstration Tout est presque immédiat~; (iii) découle de
\phi\_v\cdotu = \phi\_u \cdot \phi\_v et du fait que le rapport
du produit de deux homothéties est le produit des rapports.

\paragraph{2.7.4 Déterminant d'une matrice}

Définition~2.7.5 Soit A \in M\_k(n). On appelle déterminant de A
le déterminant de la famille de ses vecteurs colonnes dans la base
canonique. On a donc
\mathrm{det}~ A
= \\sum ~
\_\sigma\inS\_n\epsilon(\sigma)\\∏
 \_\jmath=1^na\_\jmath,\sigma(\jmath).

Proposition~2.7.7

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) Soit \mathcal{E} =
  (e\_1,\\ldots,e\_n~)
  une base de E, u \in L(E), alors
  \mathrm{det}~ u
  = \mathrm{det}~
  \mathrmMat~ (u,\mathcal{E})
\item
  (ii) \mathrm{det}~
  I\_n = 1,
  \mathrm{det}~ \lambda~A =
  \lambda~^n \mathrm{det}~
  A, \mathrm{det}~
  ^tA = \mathrm{det}~
  A
\item
  (iii) \mathrm{det}~ AB
  = \mathrm{det}~
  A\mathrm{det}~ B
\item
  (iv) A est inversible si et seulement si
  \mathrm{det}~
  A\neq~0.
\end{itemize}

Démonstration On a
\mathrm{det}~ u
= \mathrm{det}~
\_\mathcal{E}(u(e\_1),\\ldots,u(e\_n~))
ce qui n'est autre que
\mathrm{det}~
\mathrmMat~ (u,\mathcal{E}) puisque
les vecteurs colonnes de cette matrice sont constitués des coordonnées
des u(e\_\jmath) dans la base \mathcal{E}, ce qui montre (i). La formule
\mathrm{det} ^t~A
= \mathrm{det}~ A résulte de
la remarque que nous avons faite lors de la démonstration du fait que
A\_n(E) est de dimension 1~: f\_0
= \mathrm{det} \_\mathcal{E}~,
soit encore

\\sum
\_\sigma\inS\_n\epsilon(\sigma)\xi\_\sigma(1),1\ldots\xi\_\sigma(n),n~
= \\sum
\_\sigma\inS\_n\epsilon(\sigma)\xi\_1,\sigma(1)\ldots\xi\_n,\sigma(n)~

Tout le reste s'obtient en traduisant les résultats similaires sur les
endomorphismes.

Proposition~2.7.8 Le déterminant d'une matrice dépend linéairement de
chacun de ses vecteurs colonnes (resp lignes), le déterminant d'une
matrice ne change pas si on a\jmathoute à une colonne (resp. ligne) une
combinaison linéaire des autres colonnes (resp. lignes). Si on effectue
une permutation \sigma sur les colonnes (resp. lignes) d'une matrice, son
déterminant est multiplié par \epsilon(\sigma). Application Calcul par la méthode du
pivot.

Démonstration Evident de par la définition du déterminant d'une matrice
comme déterminant de la famille de ses vecteurs colonnes (ou de ses
vecteurs lignes par transposition)

Théorème~2.7.9 (calcul des déterminants par blocs).
\mathrm{det}~
\left ( \includegraphics{cours3x.png}
\,\right ) = \
\mathrm{det}
A.\mathrm{det}~ C.

Démonstration Notons M = (a\_i,\jmath)\_1\leqi,\jmath\leqn =
\left
(\matrix\,A&B\cr 0
&C\right ), si bien que l'on a a\_i,\jmath = 0 si i
≥ p + 1 et \jmath \leq p. On a alors
\mathrm{det}~ M
= \\sum ~
\_\sigma\inS\_n\epsilon(\sigma)\\∏
 \_k=1^na\_k,\sigma(k). Soit \sigma \inS\_n~; s'il
existe k\_0 \in {[}p + 1,n{]} tel que \sigma(k\_0) \in {[}1,p{]},
on a alors a\_k\_0,\sigma(k\_0) = 0 et donc
\∏ ~
\_k=1^na\_k,\sigma(k) = 0. Autrement dit, les seules
permutations qui peuvent donner une contribution non nulle au
déterminant sont les permutations \sigma \inS\_n telles que \sigma({[}p +
1,n{]}) \subset~ {[}p + 1,n{]}, c'est-à-dire vérifiant \sigma({[}p + 1,n{]}) = {[}p
+ 1,n{]} et donc aussi \sigma({[}1,p{]}) = {[}1,p{]}. Une telle permutation \sigma
est entièrement définie par ses restrictions \sigma\_1 à {[}1,p{]} et
\sigma\_2 à {[}p + 1,n{]}, l'application
\sigma\mapsto~(\sigma\_1,\sigma\_2) étant bi\jmathective
de l'ensemble de ces permutations sur S\_p \timesS\_n-p.
D'autre part, on voit immédiatement que toute décomposition de
\sigma\_1 et \sigma\_2 en produit de transpositions, fournit une
décomposition de \sigma en produit de transpositions, ce qui montre que \epsilon(\sigma)
= \epsilon(\sigma\_1)\epsilon(\sigma\_2). On obtient donc~:

\begin{align*}
\mathrm{det}~ M& =&
\sum \_ \sigma\_1\inS\_p~
\atop \sigma\_2\inS\_n-p
\epsilon(\sigma\_1)\epsilon(\sigma\_2)\∏
\_k=1^pa\_ k,\sigma\_1(k)
∏ \_k=p+1^na~\_
k,\sigma\_2(k) \%& \\ & =&
\\sum
\_\sigma\_1\inS\_p\epsilon(\sigma\_1)\∏
\_k=1^pa\_ k,\sigma\_1(k)
\\sum
\_\sigma\_2\inS\_n-p\epsilon(\sigma\_2)\∏
\_k=p+1^na\_ k,\sigma\_2(k)\%&
\\ & =&
\mathrm{det}~
A.\mathrm{det}~ C \%&
\\ \end{align*}

Corollaire~2.7.10 Le déterminant d'une matrice triangulaire par blocs
est égal au produit des déterminants des blocs diagonaux. Le déterminant
d'une matrice triangulaire est égal au produit de ses éléments
diagonaux.

Démonstration Récurrence évidente.

Définition~2.7.6 Soit A = (a\_i,\jmath) \in M\_K(n). On notera
A\_k,l = (-1)^k+l\
\mathrm{det}
(a\_i,\jmath)\_i\neq~k,\jmath\mathrel\neq~l
(cofacteur d'indice (k,l)). La matrice (A\_i,\jmath) \in
M\_K(n) est appelée la comatrice de la matrice A.

Théorème~2.7.11 (développement d'un déterminant). On a

\forall~~i \in {[}1,n{]},\quad
\mathrm{det}~ A =
\sum \_k=1^na~\_
i,kA\_i,k

(développement suivant la i-ième ligne)

\forall~~\jmath \in {[}1,n{]},\quad
\mathrm{det}~ A =
\sum \_k=1^na~\_
k,\jmathA\_k,\jmath

(développement suivant la \jmath-ième colonne)

Démonstration Par exemple sur les colonnes~; soit
(\epsilon\_1,\\ldots,\epsilon\_n~)
la base canonique de K^n,
(c\_1,\\ldotsc\_n~)
les vecteurs colonnes de la matrice A. On a c\_\jmath
= \\sum ~
\_k=1^na\_k,\jmath\epsilon\_k, d'où

\begin{align*}
\mathrm{det}~ A& =&
\mathrm{det}~
(c\_1,\\ldots,c\_n~)
\%& \\ & =& \\sum
\_k=1^na\_ k,\jmath \mathrm{det}
(c\_1,\ldots,c\_\jmath-1,\epsilon\_k,c\_\jmath+1,\\ldots,c\_n~)\%&
\\ & =& \\sum
\_k=1^na\_ k,\jmath\Delta\_k,\jmath \%&
\\ \end{align*}

Par combinaisons linéaires de colonnes (pour éliminer les termes de la
k-ième ligne) puis par échange de lignes et de colonnes, on obtient

\Delta\_k,l = (-1)^k+l\left
\textbar{}\matrix\,1&0\\ldots~0
\cr \matrix\,0
\cr \⋮~
\cr
0&(a\_i,\jmath)\_i\neq~k,\jmath\mathrel\neq~l\right
\textbar{} = (-1)^k+lA\_ k,l

Corollaire~2.7.12 A^t com~A =
^t com~A A =
(\mathrm{det} A)I\_n~.
Si A est inversible, A^-1 = 1 \over
\mathrm{det} A~
^t com~A.

Démonstration En effet (A^t\
comA)\_i,\jmath =\ \\sum
 \_k=1^na\_i,kA\_\jmath,k. Mais ceci n'est
autre que le développement suivant la \jmath-ième ligne du déterminant de la
matrice B obtenue à partir de A en rempla\ccant la
\jmath-ième ligne par la i-ième. Si i = \jmath, c'est donc
\mathrm{det}~ A. Si
i\neq~\jmath, la matrice B a deux lignes identiques,
donc son déterminant est nul.

\paragraph{2.7.5 Application des déterminants à la recherche du rang}

Lemme~2.7.13 Soit A = (a\_i,\jmath) \in M\_K(m,n) et soit B =
(a\_i,\jmath)\_i\inI,\jmath\inJ une sous-matrice de A (avec I \subset~
{[}1,m{]} et J \subset~ {[}1,n{]}. Alors
\mathrmrg~B
\leq\mathrmrg~A.

Démonstration Soit C = (a\_i,\jmath)\_i\in{[}1,m{]},\jmath\inJ et soit
c\_1,\\ldots,c\_n~
les vecteurs colonnes de A. Alors on a
\mathrmrg~C
= \mathrmrg(c\_\jmath~, \jmath
\in J)
\leq\mathrmrg(c\_1,\\\ldots,c\_n~)
= \mathrmrg~A. Mais soit
d'autre part
l\_1',\\ldots,l\_m~'
les vecteurs lignes de la matrice C. On a
\mathrmrg~B
= \mathrmrg(l\_i~',
i \in I)
\leq\mathrmrg(l\_1',\\\ldots,l\_m~')
= \mathrmrg~C, d'où
\mathrmrg~B
\leq\mathrmrg~A.

Soit alors r le rang de A. D'après le lemme précédent, toute
sous-matrice inversible B de A a une taille (un ordre) plus petit que r.
On a alors le théorème suivant

Théorème~2.7.14 Soit A = (a\_i,\jmath) \in M\_K(m,n) de rang r
et soit B = (a\_i,\jmath)\_i\inI,\jmath\inJ une sous-matrice de A
carrée inversible avec \textbar{}I\textbar{} = \textbar{}J\textbar{}
\textless{} r. Alors il existe i\_0 \in {[}1,m{]} \diagdown I,\jmath\_0
\in {[}1,n{]} \diagdown J tels que la matrice B' =
(a\_i,\jmath)\_i\inI\cup\i\_0\,\jmath\inJ\cup\\jmath\_0\
(matrice bordante de B dans A) soit encore inversible.

Démonstration Soit C = (a\_i,\jmath)\_i\in{[}1,m{]},\jmath\inJ et soit
c\_1,\\ldots,c\_n~
les vecteurs colonnes de A. On a
\mathrmrg~C
\leq\textbar{}J\textbar{} (car C a \textbar{}J\textbar{} vecteurs colonnes)
et \mathrmrg~C
≥\mathrmrg~B =
\textbar{}J\textbar{} (car B est une sous-matrice de C). Donc
\mathrmrg~C =
\textbar{}J\textbar{} \textless{} r. Ceci montre que la famille
(c\_\jmath)\_\jmath\inJ est libre. D'autre part dans V
=\
\mathrmVect(c\_1,\\ldots,c\_n~),
la famille
(c\_1,\\ldots,c\_n~)
est génératrice. Par le théorème de la base incomplète, il existe J' tel
que J \subset~ J' \subset~ {[}1,n{]} avec (c\_\jmath)\_\jmath\inJ' base de V .
Mais \textbar{}J'\textbar{} = dim~ V = r
\textgreater{} \textbar{}J\textbar{} donc on peut prendre un
\jmath\_0 \in J' \diagdown J et la famille
(c\_\jmath)\_\jmath\inJ\cup\\jmath\_0\
est encore libre. Soit D =
(a\_i,\jmath)\_i\in{[}1,m{]},\jmath\inJ\cup\\jmath\_0\.
Le rang de D est donc \textbar{}I\textbar{} + 1. Soit
l\_1',\\ldots,l\_m~'
les vecteurs colonnes de la matrice D. La matrice
(a\_i,\jmath)\_i\inI,\jmath\inJ\cup\\jmath\_0\
est de rang \textbar{}I\textbar{} (elle a \textbar{}I\textbar{} lignes
et contient la matrice B de rang \textbar{}I\textbar{}), donc la famille
(l\_i')\_i\inI est de rang \textbar{}I\textbar{} alors que
la famille (l\_i')\_i\in{[}1,m{]} est de rang
\textbar{}I\textbar{} + 1. Le même argument à base de théorème de la
base incomplète montre que l'on peut trouver i\_0 \in {[}1,m{]} \diagdown
I tel que la famille
(l\_i')\_i\inI\cup\i\_0\
soit encore libre. La matrice B' =
(a\_i,\jmath)\_i\inI\cup\i\_0\,\jmath\inJ\cup\\jmath\_0\
est donc inversible.

Remarque~2.7.1 Le théorème précédent montre donc que toute sous-matrice
inversible de taille strictement inférieure à r peut être complétée en
une autre sous-matrice inversible. On en déduit

Théorème~2.7.15 Soit A = (a\_i,\jmath) \in M\_K(m,n) de rang r.
Alors A contient des sous-matrices carrées inversibles de rang r
(sous-matrices principales). Une sous-matrice carrée inversible est une
sous-matrice principale si et seulement si toutes ses matrices bordantes
sont non inversibles.

Remarque~2.7.2 Ceci permet de rechercher théoriquement le rang d'une
matrice à l'aide de déterminants, en augmentant au fur et à mesure la
taille des sous-matrices inversibles.

\paragraph{2.7.6 Formes p-linéaires alternées}

Proposition~2.7.16 Soit E un K-espace vectoriel,
f\_1,\\ldots,f\_p~
\in E^∗. Alors f\_1
∧\\ldots~ ∧
f\_p : E^p \rightarrow~ K définie par
(x\_1,\\ldots,x\_p)\mapsto~\\mathrm{det}~
(f\_i(x\_\jmath))\_1\leqi\leqp,1\leq\jmath\leqp est une forme p-
linéaire alternée sur E.

Ceci va nous permettre d'exhiber une base de A\_p(E) en
utilisant les deux lemmes suivants. Pour cela soit E un K-espace
vectoriel de dimension n et \mathcal{E} =
(e\_1,\\ldots,e\_n~)
une base de E de base duale \mathcal{E}^∗ =
(e\_1^∗,\\ldots,e\_n^∗~).

Lemme~2.7.17 Soit f,g \in A\_p(E) telles que pour toute famille
(i\_1,\\ldots,i\_p~)
vérifiant 1 \leq i\_1 \textless{} i\_2 \textless{}
\\ldots~ \textless{}
i\_p \leq n, on ait
f(e\_i\_1,\\ldots,e\_i\_p~)
=
g(e\_i\_1,\\ldots,e\_i\_p~).
Alors f = g.

Démonstration La relation
f(e\_i\_1,\\ldots,e\_i\_p~)
=
g(e\_i\_1,\\ldots,e\_i\_p~)
reste encore vraie si
i\_1,\\ldots,i\_p~
sont distincts mais non ordonnés (il suffit de les réordonner par une
permutation \sigma, ce qui ne fait que multiplier les deux côtés par \epsilon(\sigma)).
Elle est triviale si
i\_1,\\ldots,i\_p~
ne sont pas distincts car alors les deux membres valent 0. Mais alors,
on a en posant x\_\jmath =\
\sum ~
\_i=1^n\xi\_i,\jmathe\_i

f(x\_1,\\ldots,x\_p~)
= \\sum
\_i\_1,\ldots,i\_p\in\mathbb{N}~\xi\_i\_1,1\\ldots\xi\_i\_p,pf(e\_i\_1,\\ldots,e\_i\_p~)

et la même chose pour g. Donc f = g.

Lemme~2.7.18 Soit 1 \leq i\_1 \textless{} i\_2 \textless{}
\\ldots~ \textless{}
i\_p \leq n et 1 \leq \jmath\_1 \textless{} \jmath\_2
\textless{} \\ldots~
\textless{} \jmath\_p \leq n. Alors

e\_i\_1^∗∧\\ldots~
∧ e\_ i\_p^∗(e\_
\jmath\_1,\\ldots,e\_\jmath\_p~)
=
\delta\_i\_1,\\ldots,i\_p^\jmath\_1,\\\ldots,\jmath\_p~


(symbole de Kronecker)

Démonstration Il est clair que
e\_i\_1^∗∧\\ldots~
∧
e\_i\_1^∗(e\_i\_1,\\ldots,e\_i\_p~)
= 1 (la matrice ''f\_i(x\_\jmath)'' est l'identité).
Supposons donc que i\_1 =
\jmath\_1,\\ldots,i\_k-1~
= \jmath\_k-1,i\_k\neq~\jmath\_k.
Si i\_k \textless{} \jmath\_k, on a pour tout l \in
{[}1,p{]},i\_k\neq~\jmath\_l soit
e\_\jmath\_l^∗(e\_i\_k) = 0. La
matrice ''f\_i(x\_\jmath)'' a donc sa k-ième ligne nulle et
son déterminant est donc nul. Si \jmath\_k \textless{} i\_k,
de manière similaire, la k-ième colonne de la matrice est nulle. Dans
les deux cas, on trouve donc 0 comme résultat.

Théorème~2.7.19 La famille des
(e\_i\_1^∗∧\\ldots~
∧
e\_i\_p^∗)\_1\leqi\_1\textless{}i\_2\textless{}\\ldots\textless{}i\_p\leqn~
est une base de A\_p(E) (qui est donc de dimension
C\_n^p).

Démonstration Montrons que la famille est génératrice. Soit f \in
A\_p(E) et

g = \\sum ~
\_1\leq\jmath\_1\textless{}\jmath\_2\textless{}\\ldots\textless{}\jmath\_p\leqnf(e\_\jmath\_1,\\\ldots,e\_\jmath\_p)e\_\jmath\_1^∗∧\\\ldots~
∧ e\_\jmath\_p^∗. Grâce au lemme 2, si 1 \leq
i\_1 \textless{} i\_2 \textless{}
\\ldots~ \textless{}
i\_p \leq n, on a
g(e\_i\_1,\\ldots,e\_i\_p~)
=
f(e\_i\_1,\\ldots,e\_i\_p~).
D'après le lemme 1, on a f = g. Il reste à montrer que la famille est
libre. Supposons que \\\sum

\_1\leq\jmath\_1\textless{}\jmath\_2\textless{}\\ldots\textless{}\jmath\_p\leqn\lambda~\_\jmath\_1,\\\ldots,\jmath\_pe\_\jmath\_1^∗∧\\\ldots~
∧ e\_\jmath\_p^∗ = 0. Grâce au lemme 2, si 1 \leq
i\_1 \textless{} i\_2 \textless{}
\\ldots~ \textless{}
i\_p \leq n on a

\begin{align*} 0& =&
0(e\_i\_1,\\ldots,e\_i\_p~)
\%& \\ & =& \\sum
\_1\leq\jmath\_1\textless{}\jmath\_2\textless{}\ldots\textless{}\jmath\_p\leqn\lambda~\_\jmath\_1,\\ldots,\jmath\_pe\_\jmath\_1^∗∧\\ldots~
∧ e\_ \jmath\_p^∗(e\_
i\_1,\ldots,e\_i\_p~)
=
\lambda~\_i\_1,\ldots,i\_p~\%&
\\ \end{align*}

ce qui montre que la famille est libre.

{[}
{[}
{[}
{[}

\end{document}
