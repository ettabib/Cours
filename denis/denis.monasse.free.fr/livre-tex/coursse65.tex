Voici le fichier LaTeX corrigé avec l'utilisation des environnements demandés, l'indexation des mots-clés (notamment les définitions), et la suppression de la numérotation manuelle :

\section{Développements en séries entières}

\subsection{Problème local, problème global}

\begin{de}
\index{série entière!développable globalement}
Soit $U$ un voisinage de 0 dans $K$ et $f : U \rightarrow E$ (espace vectoriel normé complet). Soit $r > 0$ tel que $D(0,r) = \{z \in K | |z| < r\} \subset U$. On dit que $f$ est développable en série entière sur le disque $D(0,r)$ s'il existe une série entière $\sum a_n z^n$ de rayon de convergence $R \geq r$ tel que $\forall z \in D(0,r), f(z) = \sum a_n z^n$.
\end{de}

\begin{de}
\index{série entière!développable localement}
Soit $U$ un voisinage de 0 dans $K$ et $f : U \rightarrow E$ (espace vectoriel normé complet). On dit que $f$ est développable en série entière au voisinage de 0 s'il existe $r > 0$ tel que $D(0,r) = \{z \in K | |z| < r\} \subset U$ et $f$ est développable en série entière dans $D(0,r)$.
\end{de}

\begin{rem}
Dans le premier cas, on impose donc le disque dans lequel $f$ doit être développable en série entière, alors que dans le second cas on laisse toute latitude à $r$ de prendre des valeurs aussi petites que nécessaires.
\end{rem}

\begin{rem}
Il est facile de construire des fonctions non développables en séries entières au voisinage de 0. Par exemple toute fonction admettant 0 comme zéro non isolé ne peut être développable en série entière au voisinage de 0. Par exemple $f(x) = e^{-1/x^2} \sin(1/x)$ si $x \neq 0$, $f(0) = 0$ ; cette fonction, bien que $C^\infty$ sur $\mathbb{R}$, ne peut être développable en série entière au voisinage de 0.
\end{rem}

\subsection{Méthodes de développement}

On peut utiliser tout d'abord les résultats sur les opérations algébriques ou analytiques concernant les séries entières. On obtiendra de manière évidente les résultats suivants

\begin{thm}
\begin{enumerate}
\item Si $f$ et $g : U \rightarrow E$ sont développables en séries entières sur $D(0,r) \subset U$ (resp. au voisinage de 0), alors $\alpha f + \beta g$ est développable en série entière sur $D(0,r)$ (resp. au voisinage de 0).
\item Si $f$ et $g : U \rightarrow K$ sont développables en séries entières sur $D(0,r) \subset U$ (resp. au voisinage de 0), alors $fg$ est développable en série entière sur $D(0,r)$ (resp. au voisinage de 0).
\item Si $f : U \rightarrow K$ est développable en série entière au voisinage de 0 et si $f(0) \neq 0$, alors $\frac{1}{f}$ est développable en série entière au voisinage de 0.
\item Si $U \subset \mathbb{R}$, $f : U \rightarrow E$ est dérivable et si $f'$ est développable en série entière sur $D(0,r) \subset U$ (resp. au voisinage de 0), alors $f$ est développable en série entière sur $D(0,r)$ (resp. au voisinage de 0).
\item Si $U \subset \mathbb{R}$, $f : U \rightarrow E$ est développable en série entière sur $D(0,r) \subset U$ (resp. au voisinage de 0), alors $f$ est $C^\infty$ sur $]-r,r[$ (resp. au voisinage de 0) et $f^{(p)}$ est développable en série entière sur $D(0,r)$ (resp. au voisinage de 0).
\end{enumerate}
\end{thm}

Dans un cadre général, on pourra utiliser le résultat suivant

\begin{thm}[Mac Laurin]
Soit $U$ un voisinage de 0, $f : U \rightarrow E$ de classe $C^\infty$ et $r > 0$ tel que $]-r,r[ \subset U$. Alors $f$ est développable en série entière dans $]-r,r[$ si et seulement si

$\forall t \in ]-r,r[, \lim_{n \rightarrow +\infty} \left(f(t) - \sum_{k=0}^n \frac{f^{(k)}(0)}{k!} t^k \right) = 0$
\end{thm}

\begin{proof}
On sait en effet que $f$ est développable en série entière dans $]-r,r[$ si et seulement si

$\forall t \in ]-r,r[, f(t) = \sum_{k=0}^{+\infty} \frac{f^{(k)}(0)}{k!} t^k$

ce qui est la même chose que l'assertion ci-dessus.
\end{proof}

Méthode : pour démontrer que $f$ est développable en série entière dans $]-r,r[$, on peut donc chercher à estimer (par exemple par utilisation de la formule de Taylor-Lagrange ou de la formule de Taylor avec reste intégral) l'expression $R_n(t) = f(t) - \sum_{k=0}^n \frac{f^{(k)}(0)}{k!} t^k$ et à montrer qu'elle admet la limite 0 quand $n$ tend vers $+\infty$.

\begin{ex}
Soit $f : ]-r,r[ \rightarrow E$ une fonction de classe $C^\infty$ telle que $\forall n \in \mathbb{N}, \forall t \in ]-r,r[, |f^{(n)}(t)| \leq M$. On peut alors écrire la formule de Taylor avec reste intégral, et on obtient, pour $t \in ]-r,r[$, $R_n(t) = \int_0^t \frac{(t-u)^n}{n!} f^{(n+1)}(u) du$ soit encore $|R_n(t)| \leq M \int_0^t \frac{(t-u)^n}{n!} du = M \frac{t^{n+1}}{(n+1)!}$ qui tend vers 0 quand $n$ tend vers $+\infty$ (la série converge d'après la règle de d'Alembert). On en déduit que la fonction $f$ est développable en série entière dans $]-r,r[$. C'est ainsi que quelle que soit la méthode utilisée pour définir une fonction exponentielle, qui vérifiera $(e^x)' = e^x$, on aura (pour un $r$ quelconque) $\forall n \in \mathbb{N}, \forall t \in ]-r,r[, |f^{(n)}(t)| \leq e^r$, donc $\forall t \in ]-r,r[, \exp(t) = \sum_{n=0}^{+\infty} \frac{t^n}{n!}$ et donc $\forall t \in \mathbb{R}, \exp(t) = \sum_{n=0}^{+\infty} \frac{t^n}{n!}$. Une étude similaire pourrait être faite pour les fonctions trigonométriques (cosinus et sinus). En fait, dans le subsection suivant, nous allons définir directement ces fonctions comme sommes de séries entières.
\end{ex}

Enfin, une dernière technique utilise le théorème de Cauchy-Lipschitz sur l'unicité des solutions d'équations différentielles à conditions initiales données. Soit $f : U \rightarrow K$ une fonction de classe $C^\infty$ vérifiant une équation différentielle $y^{(n)} = G(t,y,y',\ldots,y^{(n-1)})$. Supposons également que nous connaissions une série entière $\sum a_n t^n$ de rayon de convergence $R > 0$ telle que sa somme $S$ vérifie la même équation différentielle avec de plus $S(0) = f(0), \ldots, S^{(n-1)}(0) = f^{(n-1)}(0)$. Si l'équation différentielle relève de l'un des deux théorèmes de Cauchy Lipschitz, et si en particulier elle est linéaire à coefficients continus, on aura $\forall t \in ]-R,R[ \cap U, f(t) = S(t) = \sum_{k=0}^{+\infty} a_k t^k$. La démarche est alors la suivante

\begin{enumerate}
\item trouver une équation différentielle vérifiée par $f$
\item trouver une série entière $\sum a_n t^n$ vérifiant formellement l'équation différentielle en question avec $a_0 = f(0), \ldots, a_{n-1} = (n-1)! f^{(n-1)}(0)$
\item montrer que cette série entière a un rayon de convergence $R$ non nul
\item utiliser un théorème de Cauchy Lipschitz pour garantir que $\forall t \in ]-R,R[ \cap U, f(t) = S(t) = \sum_{k=0}^{+\infty} a_k t^k$
\end{enumerate}

\begin{ex}
Soit $m \in \mathbb{R}$ et $f : \mathbb{R} \rightarrow \mathbb{R}$ définie par $f(x) = (x + \sqrt{1+x^2})^m$. On a

\begin{align*}
f'(x) &= m\left(\frac{1+x}{\sqrt{1+x^2}}\right)(x + \sqrt{1+x^2})^{m-1} \\
&= \frac{m f(x)}{\sqrt{1+x^2}}
\end{align*}

d'où

\begin{align*}
f''(x) &= -\frac{2mx}{(1+x^2)^{3/2}} f(x) + \frac{m}{\sqrt{1+x^2}} f'(x) \\
&= -\frac{x}{1+x^2} f'(x) + \frac{m^2}{1+x^2} f(x)
\end{align*}

en remplaçant simultanément $\frac{m f(x)}{\sqrt{1+x^2}}$ par $f'(x)$ et $f'(x)$ par $\frac{m f(x)}{\sqrt{1+x^2}}$. Donc $f$ vérifie l'équation différentielle

$(1+x^2) y'' + xy' - m^2 y = 0$

avec les conditions initiales $f(0) = 1$, $f'(0) = m$.

Cherchons inversement une fonction $S$ développable en série entière sur $]-R,R[$ vérifiant cette équation différentielle. On a alors $S(t) = \sum_{n=0}^{+\infty} a_n t^n$, $tS'(t) = \sum_{n=0}^{+\infty} n a_n t^n$, $t^2 S''(t) = \sum_{n=0}^{+\infty} n(n-1) a_n t^n$ et $S''(t) = \sum_{n=0}^{+\infty} (n+1)(n+2) a_{n+2} t^n$ (vérifications faciles laissées au lecteur, en remarquant que $n a_n = 0$ pour $n = 0$ et $n(n-1) a_n = 0$ pour $n = 0$ et $n = 1$). On a donc

\begin{align*}
(1+t^2) S''(t) + tS'(t) - m^2 S(t) &= \sum_{n=0}^{+\infty} \left((n+1)(n+2) a_{n+2} + n(n-1) a_n + n a_n - m^2 a_n \right) t^n \\
&= \sum_{n=0}^{+\infty} \left((n+1)(n+2) a_{n+2} + (n^2-m^2) a_n \right) t^n
\end{align*}

L'unicité du développement en série entière de la fonction nulle montre que $S$ est solution de l'équation différentielle si et seulement si

$\forall n \in \mathbb{N}, (n+1)(n+2) a_{n+2} + (n^2-m^2) a_n = 0$

On veut de plus que $a_0 = 1$ et $a_1 = m$. Ces trois relations définissent parfaitement les deux suites $(a_{2n})_{n \in \mathbb{N}}$ et $(a_{2n+1})_{n \in \mathbb{N}}$ (l'une des deux, celle correspondant à la parité de $m$, étant nulle à partir du rang $m+2$ si $m \in \mathbb{Z}$). Des deux séries entières $\sum a_{2n} t^{2n}$ et $\sum a_{2n+1} t^{2n+1}$, l'une des deux est de rayon de convergence 1, l'autre de rayon de convergence soit 1 soit $+\infty$. Donc la série entière $\sum a_n t^n$ est de rayon de convergence $1 = \inf(1,+\infty)$.

Sur $]-1,1[$, on peut donc définir $S(t) = \sum_{n=0}^{+\infty} a_n t^n$. Cette fonction vérifie la même équation différentielle que $f$ avec les mêmes conditions initiales. Le théorème de Cauchy-Lipschitz pour les équations différentielles linéaires à coefficients continus garantit que $f$ et $S$ coïncident sur l'intersection de leurs intervalles de définition, c'est-à-dire sur $]-1,1[$. Donc $f$ est développable en série entière sur $]-1,1[$.
\end{ex}

\subsection{Fonction exponentielle. Fonctions trigonométriques}

La règle de d'Alembert montre que la série entière $\sum_{n \geq 0} \frac{z^n}{n!}$ a un rayon de convergence infini. Ceci justifie l'introduction de la définition suivante

\begin{de}
\index{exponentielle complexe}
Pour $z \in \mathbb{C}$, on pose $\exp(z) = \sum_{n=0}^{+\infty} \frac{z^n}{n!}$.
\end{de}

\begin{prop}
\begin{enumerate}
\item $z \mapsto \exp(z)$ est un morphisme de groupes de $(\mathbb{C},+)$ dans $(\mathbb{C}^*,\cdot)$, autrement dit $\exp(0) = 1$, $\exp(z_1 + z_2) = \exp z_1 \exp z_2$, $\exp(z) \neq 0$ et $(\exp(z))^{-1} = \exp(-z)$.
\item $\forall z \in \mathbb{C}, \exp(\overline{z}) = \overline{\exp z}$
\item Pour tout $z \in \mathbb{C}$, l'application $t \mapsto \exp(tz)$ est $C^\infty$ de $\mathbb{R}$ dans $\mathbb{C}$ et $\frac{d^n}{dt^n}(\exp(tz)) = z^n \exp(tz)$.
\end{enumerate}
\end{prop}

\begin{proof}
(i) Soit $z_1, z_2 \in \mathbb{C}$. On pose $a_n = \frac{z_1^n}{n!}$ et $b_n = \frac{z_2^n}{n!}$. Ces séries sont absolument convergentes. On peut donc faire le produit de Cauchy de ces deux séries et on a alors $c_n = \sum_{k=0}^n \frac{1}{k!(n-k)!} z_1^k z_2^{n-k} = \frac{1}{n!}(z_1 + z_2)^n$ d'après la formule du binôme. On a donc

$\sum_{n=0}^{+\infty} \frac{(z_1 + z_2)^n}{n!} = \left(\sum_{n=0}^{+\infty} \frac{z_1^n}{n!}\right) \left(\sum_{n=0}^{+\infty} \frac{z_2^n}{n!}\right)$

(ii) Il suffit de faire tendre $N$ vers $+\infty$ dans la formule évidente $\sum_{n=0}^N \frac{\overline{z}^n}{n!} = \overline{\sum_{n=0}^N \frac{z^n}{n!}}$

(iii) On a $\exp(tz) = \sum_{n=0}^{+\infty} \frac{z^n}{n!} t^n$ qui est une série entière en $t$ de rayon de convergence infini. On en déduit que sa somme est de classe $C^\infty$ sur $\mathbb{R}$ et que

\begin{align*}
\frac{d^n}{dt^n}(\exp(tz)) &= \sum_{k=n}^{+\infty} \frac{z^k}{k!} k(k-1) \ldots (k-n+1) t^{k-n} \\
&= \sum_{k=n}^{+\infty} \frac{z^k}{(k-n)!} t^{k-n} = z^n \exp(tz)
\end{align*}

après le changement de $k-n$ en $k$.
\end{proof}

\begin{de}
\index{nombre e}
On pose $e = \exp 1$; on a bien entendu, $\forall n \in \mathbb{Z}, \exp(n) = e^n$. On en déduit facilement que $\forall r \in \mathbb{Q}, \exp(r) = e^r$, ce qui justifie ensuite la notation $\exp(z) = e^z$ pour $z \in \mathbb{C}$.
\end{de}

\begin{de}
\index{cosinus complexe}
\index{sinus complexe}
\index{cosinus hyperbolique}
\index{sinus hyperbolique}
Pour $z \in \mathbb{C}$, on pose $\cos z = \frac{e^{iz} + e^{-iz}}{2}$, $\sin z = \frac{e^{iz} - e^{-iz}}{2i}$, $\ch z = \frac{e^z + e^{-z}}{2}$, $\sh z = \frac{e^z - e^{-z}}{2}$.
\end{de}

\begin{rem}
Il est clair que les fonctions $\cos$ et $\ch$ sont paires et que les fonctions $\sin$ et $\sh$ sont impaires.
\end{rem}

\begin{prop}
\begin{enumerate}
\item $\forall z \in \mathbb{C}, \ch(iz) = \cos z, \sh(iz) = i \sin z, \cos^2 z + \sin^2 z = 1, \ch^2 z - \sh^2 z = 1$
\item $\forall a, b \in \mathbb{C}$

\begin{align*}
\cos(a+b) &= \cos a \cos b - \sin a \sin b \\
\sin(a+b) &= \sin a \cos b + \cos a \sin b \\
\ch(a+b) &= \ch a \ch b + \sh a \sh b \\
\sh(a+b) &= \sh a \ch b + \ch a \sh b
\end{align*}
\end{enumerate}
\end{prop}

\begin{proof}
Par le calcul à partir de la définition.
\end{proof}

\begin{rem}
On déduit de ces formules de manière évidente toutes les formules usuelles de la trigonométrie circulaire ou hyperbolique dont nous ne citerons que celles qu'il est absolument indispensable de connaître par cœur :

$\forall a, b \in \mathbb{C}$

\begin{align*}
\cos(a+b) &= \cos a \cos b - \sin a \sin b \\
\cos(a-b) &= \cos a \cos b + \sin a \sin b \\
\sin(a+b) &= \sin a \cos b + \cos a \sin b \\
\sin(a-b) &= \sin a \cos b - \cos a \sin b \\
\cos(2a) &= \cos^2 a - \sin^2 a = 2\cos^2 a - 1 = 1 - 2\sin^2 a \\
\sin(2a) &= 2 \sin a \cos a \\
\cos^2 a &= \frac{1 + \cos 2a}{2}, \quad \sin^2 a = \frac{1 - \cos 2a}{2} \\
\cos p + \cos q &= 2 \cos\frac{p-q}{2} \cos\frac{p+q}{2} \\
\cos p - \cos q &= -2 \sin\frac{p-q}{2} \sin\frac{p+q}{2} \\
\sin p + \sin q &= 2 \cos\frac{p-q}{2} \sin\frac{p+q}{2} \\
\sin p - \sin q &= 2 \sin\frac{p-q}{2} \cos\frac{p+q}{2}
\end{align*}

$\forall a, b \in \mathbb{C}$

\begin{align*}
\ch(a+b) &= \ch a \ch b + \sh a \sh b \\
\ch(a-b) &= \ch a \ch b - \sh a \sh b \\
\sh(a+b) &= \sh a \ch b + \ch a \sh b \\
\sh(a-b) &= \sh a \ch b - \ch a \sh b \\
\ch(2a) &= \ch^2 a + \sh^2 a = 2\ch^2 a - 1 = 1 + 2\sh^2 a \\
\sh(2a) &= 2 \sh a \ch a
\end{align*}
\end{rem}

\subsection{Etude sur $\mathbb{R}$ des fonctions exponentielle et logarithme}

\begin{thm}
\index{logarithme}
L'application $t \mapsto e^t$ est un $C^\infty$ difféomorphisme de $\mathbb{R}$ sur $]0,+\infty[$. Le difféomorphisme réciproque est appelé le logarithme (naturel ou népérien). Pour $t > 0$ et $\alpha \in \mathbb{R}$, on pose $t^\alpha = e^{\alpha \log t}$ (la notation est cohérente pour $\alpha \in \mathbb{Q}$). On a alors (entre autres propriétés)

\begin{enumerate}
\item $\forall t_1, t_2 \in ]0,+\infty[, \log(t_1 t_2) = \log t_1 + \log t_2$
\item $\frac{d}{dt}(\log t) = \frac{1}{t}$
\item On a également

\begin{align*}
\forall \alpha > 0, & \lim_{t \rightarrow +\infty} t^{-\alpha} e^t = +\infty, \quad \lim_{t \rightarrow +\infty} t^{-\alpha} \log t = 0 \\
& \lim_{t \rightarrow +\infty} t^\alpha e^{-t} = 0, \quad \lim_{t \rightarrow 0} t^\alpha \log t = 0
\end{align*}
\end{enumerate}
\end{thm}

\begin{proof}
On a clairement $\exp t > 0$ pour $t \geq 0$; pour $t \leq 0$, on a $\exp t = (\exp(-t))^{-1} > 0$; on en déduit que $\frac{d}{dt}(\exp t) = \exp t > 0$ pour tout $t \in \mathbb{R}$ donc $t \mapsto \exp t$ est un $C^\infty$ difféomorphisme croissant de $\mathbb{R}$ sur son image. Mais, pour $t \geq 0$, on a $\exp t = \sum_{n=0}^{+\infty} \frac{t^n}{n!} \geq 1 + t$ donc $\lim_{t \rightarrow +\infty} \exp t = +\infty$. Comme pour $t < 0$, $\exp t = (\exp(-t))^{-1} > 0$, on a $\lim_{t \rightarrow -\infty} \exp t = 0$. Donc $\exp(\mathbb{R}) = ]0,+\infty[$. La cohérence de la définition de $t^\alpha = e^{\alpha \log t}$ est laissée aux soins du lecteur ainsi que les propriétés évidentes de l'application $t \mapsto t^\alpha$.

(i) Comme $\exp$ est un isomorphisme de $(\mathbb{R},+)$ sur $(\mathbb{R}^*_+,\cdot)$, $\log$ est également un isomorphisme de groupes de $(\mathbb{R}^*_+,\cdot)$ sur $(\mathbb{R},+)$

(ii) Le théorème sur la dérivation des fonctions réciproques montre que $\frac{d}{dt}(\log t) = \frac{1}{\exp'(\log t)} = \frac{1}{\exp(\log t)} = \frac{1}{t}$.

(iii) pour $t > 0$, on a $\exp t = \sum_{n=0}^{+\infty} \frac{t^n}{n!} \geq \frac{t^N}{N!}$. Soit $\alpha > 0$, on a alors pour $N > \alpha$, $t^{-\alpha} \exp t > \frac{t^{N-\alpha}}{N!}$, ce qui montre que $\lim_{t \rightarrow +\infty} t^{-\alpha} e^t = +\infty$. En passant à l'inverse, on trouve $\lim_{t \rightarrow \infty} t^\alpha e^{-t} = 0$. Comme $\lim_{x \rightarrow +\infty} \log x = +\infty$, le théorème de composition des limites donne $\lim_{x \rightarrow +\infty} (\log x)^{-1/\alpha} x = 0$, soit encore $\lim_{x \rightarrow +\infty} x^{-\alpha} \log x = 0$; en changeant $x$ en $1/x$, on obtient alors $\lim_{x \rightarrow 0} x^\alpha \log x = 0$.
\end{proof}

\subsection{Etude sur $\mathbb{R}$ des fonctions cosinus et sinus}

On a pour $t \in \mathbb{R}$, $\overline{e^{it}} = \exp(\overline{it}) = e^{-it}$. On a donc $\cos t = \Re(e^{it})$ et $\sin t = \Im(e^{it})$. On en déduit que $\cos t = \sum_{n=0}^{+\infty} (-1)^n \frac{t^{2n}}{(2n)!}$ et $\sin t = \sum_{n=0}^{+\infty} (-1)^n \frac{t^{2n+1}}{(2n+1)!}$. Ces formules (ou celles de définition) montrent immédiatement que $\cos' = -\sin$ et $\sin' = \cos$. On a, pour $t \in ]0,2]$, $\frac{\sin t}{t} = \sum_{n=0}^{+\infty} (-1)^n \frac{t^{2n}}{(2n+1)!}$ qui est la somme d'une série alternée à partir de $n = 1$ car

$\frac{t^{2n+2}/(2n+3)!}{t^{2n}/(2n+1)!} = \frac{t^2}{(2n+1)(2n+3)} < 1$

pour $t \in ]0,2]$ et $n \geq 1$. On en déduit (théorème sur l'encadrement des sommes d'une série alternée) que $\forall t \in ]0,2], \frac{\sin t}{t} \geq 1 - \frac{t^2}{6} > 0$, donc $\sin t > 0$. Comme $\cos' = -\sin$, la fonction cosinus est strictement décroissante sur $[0,2]$. On a $\cos 0 = 1 > 0$ et $\cos 2 = \sum_{n=0}^{+\infty} (-1)^n \frac{2^{2n}}{(2n)!}$ qui est la somme d'une série alternée à partir de $n = 1$ car

$\frac{2^{2n+2}/(2n+2)!}{2^{2n}/(2n)!} = \frac{4}{(2n+1)(2n+2)} < 1$

pour $n \geq 1$. On en déduit (théorème sur l'encadrement des sommes d'une série alternée) que

$\cos 2 \leq 1 - \frac{4}{2!} + \frac{16}{4!} < 0$

Le théorème des valeurs intermédiaires assure alors que la fonction cosinus s'annule une et une seule fois sur $]0,2[$ en un point $\alpha$. On posera $\pi = 2\alpha$. On a donc $\cos \frac{\pi}{2} = 0$ et $\cos t > 0$ pour $t \in [0, \frac{\pi}{2}[$. Comme $\sin' = \cos$, $\sin$ est strictement croissante sur $[0, \frac{\pi}{2}]$, comme $\sin 0 = 0$ et $\cos^2 \frac{\pi}{2} + \sin^2 \frac{\pi}{2} = 1$, on a donc $\sin \frac{\pi}{2} = 1$. Les formules d'addition donnent alors $\cos(x + \frac{\pi}{2}) = -\sin x$, $\sin(x + \frac{\pi}{2}) = \cos x$ et on a donc les variations suivantes

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$t$ & $0$ & $\frac{\pi}{2}$ & $\pi$ & $\frac{3\pi}{2}$ & $2\pi$ \\
\hline
$\cos t$ & $1$ & $\searrow$ & $0$ & $\searrow$ & $-1$ & $\nearrow$ & $0$ & $\nearrow$ & $1$ \\
\hline
$\sin t$ & $0$ & $\nearrow$ & $1$ & $\searrow$ & $0$ & $\searrow$ & $-1$ & $\nearrow$ & $0$ \\
\hline
\end{tabular}
\end{center}

On a $\cos(x + 2\pi) = \cos x$ et $\sin(x + 2\pi) = \sin x$ (de nouveau par les formules d'addition). L'ensemble des périodes des fonctions $\cos$ et $\sin$ (qui est évidemment le même puisque chacune est au signe près la dérivée de l'autre) est un sous-groupe fermé de $\mathbb{R}$ donc de la forme $a\mathbb{Z}$ avec donc $a = \frac{2\pi}{k}$, où $k \in \mathbb{Z}$; le tableau de variation montre clairement que $k$ ne peut pas être strictement supérieur à 1; donc les deux fonctions sont périodiques de plus petite période $2\pi$.

\subsection{Nombres complexes de module 1}

Avec l'aimable autorisation de Hervé Pépin.

\begin{thm}
\index{nombres complexes de module 1}
L'application $\phi : t \mapsto e^{it}$ est un morphisme surjectif de groupes de $(\mathbb{R},+)$ sur $(U,\cdot)$ (où $U$ désigne l'ensemble des nombres complexes de module 1) dont le noyau est $2\pi\mathbb{Z}$. Par factorisation canonique, il définit un isomorphisme $\overline{\phi}$ du groupe quotient $(\mathbb{R}/2\pi\mathbb{Z})$ sur le groupe $(U,\cdot)$.
\end{thm}

\begin{proof}
Il est clair que $\phi(t_1 + t_2) = \phi(t_1)\phi(t_2)$, donc $\phi$ est un morphisme de groupes. Soit $z = a + ib$ un nombre complexe de module 1. On a $a \in [-1,1]$, donc il existe $t \in \mathbb{R}$ tel que $\cos t = a$. Mais alors $\sin t = \pm \sqrt{1 - a^2} = \pm b$ et donc $b$ vaut soit $\sin t$ soit $\sin(-t)$. Dans les deux cas on a trouvé un $x \in \mathbb{R}$ tel que $z = e^{ix}$ ce qui montre que $\phi$ est surjectif. On a alors $x \in \Ker \phi \Leftrightarrow \phi(x) = 1 \Leftrightarrow \cos x = 1 \Leftrightarrow x \in 2\pi\mathbb{Z}$ ce qui assure que $\Ker \phi = 2\pi\mathbb{Z}$.
\end{proof}

\begin{de}
\index{argument d'un nombre complexe}
Soit $z \in \mathbb{C}, z \neq 0$. On pose alors $\Arg z = \overline{\phi}^{-1}(\frac{z}{|z|}) \in \mathbb{R}/2\pi\mathbb{Z}$ (argument du nombre complexe $z$).
\end{de}

\begin{rem}
Si $\Arg z = \theta + 2\pi\mathbb{Z}$, on a par définition $z = |z|e^{i\theta}$. On a bien entendu, $\Arg(z_1 z_2) = \Arg z_1 + \Arg z_2$. On peut prendre un représentant $\arg z$ dans $\mathbb{R}$ de $\Arg z$ (par exemple classiquement dans $]-\pi,\pi]$), mais alors en général $\arg(z_1 z_2) \neq \arg z_1 + \arg z_2$.
\end{rem}

\begin{de}
\index{fonction relevable}
Soit $X$ un espace métrique et $f : X \rightarrow U$ une application continue. On dit que $f$ est relevable s'il existe $\phi : X \rightarrow \mathbb{R}$ continue telle que $f = e^{i\phi}$.
\end{de}

\begin{prop}
Soit $X$ un espace métrique et $f : X \rightarrow U$ une application continue non surjective. Alors $f$ est relevable.
\end{prop}

\begin{proof}
Soit $\alpha \in \mathbb{R}$ tel que $e^{i\alpha} \notin f(X)$. L'application $\omega : ]\alpha,\alpha+2\pi[ \rightarrow U \setminus \{e^{i\alpha}\}, t \mapsto e^{it}$ est un homéomorphisme. Donc $\phi = \omega^{-1} \circ f$ convient.
\end{proof}

\begin{thm}
Soit $X$ un espace métrique et $f,g : X \rightarrow U$ telles que $\forall x \in X, |f(x) - g(x)| < 2$. Alors $\frac{f}{g}$ est relevable.
\end{thm}

\begin{proof}
On a $\forall x \in X, \frac{f(x)}{g(x)} \neq -1$ et donc $\frac{f}{g}$ n'est pas surjective.
\end{proof}

Nous allons maintenant introduire un concept important en topologie, l'homotopie.

\begin{de}
\index{homotopie}
Soit $X$ et $E$ deux espaces métriques, $f,g : X \rightarrow E$. On dit que $f$ et $g$ sont homotopes s'il existe $F : X \times [0,1] \rightarrow E$ continue telle que $\forall x \in X, f(x) = F(x,0)$ et $g(x) = F(x,1)$.
\end{de}

\begin{thm}
Soit $X$ un espace métrique compact et $f,g : X \rightarrow U$ homotopes. Alors $\frac{f}{g}$ est relevable et donc $(f \text{ relevable}) \Leftrightarrow (g \text{ relevable})$.
\end{thm}

\begin{proof}
Soit $F$ comme ci-dessus. Alors $F$ est continue sur $X \times [0,1]$ compact, donc uniformément continue. Soit donc $\eta > 0$ tel que $|t - t'| < \eta \Rightarrow \forall x \in X, |F(x,t) - F(x,t')| < 2$ et soit $t_0 = 0, \ldots, t_n = 1$ une subdivision de $[0,1]$ de pas plus petit que $\eta$. Alors, si $f_i(x) = F(x,t_i)$, on a $\forall x \in X, |f_{i+1}(x) - f_i(x)| < 2$ donc $\frac{f_i}{f_{i+1}}$ est relevable. Mais alors $\frac{f}{g} = \frac{f_0}{f_n} = \prod_{i=0}^{n-1} \frac{f_i}{f_{i+1}}$ est relevable.
\end{proof}

\begin{thm}
Soit $f : [a,b] \rightarrow U$ une application continue. Alors $f$ est relevable.
\end{thm}

\begin{proof}
$f$ est évidemment homotope à l'application constante $x \mapsto f(a)$ qui est relevable (car non surjective) par $F(x,t) = f(a + t(x - a))$.
\end{proof}

\begin{thm}
Soit $I$ un intervalle de $\mathbb{R}$ et $f : I \rightarrow U$ continue. Alors $f$ est relevable.
\end{thm}

\subsection{Fonctions classiques}

\subsubsection{Fonctions d'une variable complexe}

\begin{lem}
\index{série géométrique}
La fonction $z \mapsto \frac{1}{1-z}$ est développable en série entière dans le disque $D(0,1)$ ($R = 1$).
\end{lem}

\begin{proof}
On a $\frac{1}{1-z} = 1 + z + \cdots + z^n + \frac{z^{n+1}}{1-z}$ ce dernier terme tendant vers 0 quand $n$ tend vers $+\infty$ si $|z| < 1$. On en déduit que pour $|z| < 1$, $\frac{1}{1-z} = \sum_{n=0}^{+\infty} z^n$ avec un rayon de convergence évidemment égal à 1 par la règle de d'Alembert.
\end{proof}

\begin{prop}
Pour $|z| < 1$, on a $\frac{1}{(1-z)^{k+1}} = \sum_{n=0}^{+\infty} C_{n+k}^k z^n$ avec un rayon de convergence égal à 1.
\end{prop}

\begin{proof}
Soit $z \neq 0$; le lemme précédent montre que pour $t \in ]-\frac{1}{|z|}, \frac{1}{|z|}[$, on a $\frac{1}{1-tz} = \sum_{n=0}^{+\infty} z^n t^n$. Dérivons $k$ fois par rapport à $t$ cette série entière en $t$. On obtient donc

$\frac{z^k k!}{(1-tz)^{k+1}} = \sum_{n=k}^{+\infty} z^n \frac{n!}{(n-k)!} t^{n-k} = \sum_{n=0}^{+\infty} z^{n+k} \frac{(n+k)!}{n!} t^n$

après le changement de $n$ en $n+k$. Si $|z| < 1$, en divisant les deux membres par $z^k k!$ et en faisant $t = 1 \in ]-\frac{1}{|z|}, \frac{1}{|z|}[$, on obtient la formule désirée. Il est clair que le rayon de convergence de la série obtenue est 1 (règle de d'Alembert).
\end{proof}

\begin{thm}
Soit $R(z) = \frac{P(z)}{Q(z)}$ une fraction rationnelle à coefficients complexes n'admettant pas 0 pour pôle et soit $\rho = \min\{|\alpha| | \alpha \text{ pôle de } R\}$. Alors $R$ est développable en série entière dans le disque $D(0,\rho)$.
\end{thm}

\begin{proof}
Par décomposition en éléments simples sur le corps des nombres complexes, il suffit de montrer que si $\alpha$ est un pôle de $R(z)$, un élément simple de la forme $\frac{1}{(z-\alpha)^k}$ est développable en série entière dans le disque $D(0,\rho)$. Mais

$\frac{1}{(z-\alpha)^k} = \frac{(-1)^k}{\alpha^k} \frac{1}{(1-\frac{z}{\alpha})^k} = \frac{(-1)^k}{\alpha^k} \sum_{n=0}^{+\infty} C_{n+k-1}^{k-1} \frac{z^n}{\alpha^n}$

avec un rayon de convergence égal à $|\alpha| \geq \rho$.
\end{proof}

\begin{rem}
On montre facilement que le rayon de convergence est exactement égal à $\rho$, puisque la somme de la série entière ne peut admettre un prolongement continu en un pôle de la fraction rationnelle.
\end{rem}

\subsubsection{Fonctions d'une variable réelle}

\begin{lem}
\index{développement binomial}
Soit $\alpha \in \mathbb{R}$. La fonction $t \mapsto (1+t)^\alpha$ est développable en série entière dans $]-1,1[$ si $\alpha \notin \mathbb{N}$, dans $\mathbb{R}$ si $\alpha \in \mathbb{N}$.
\end{lem}

\begin{proof}
Le résultat est évident si $\alpha \in \mathbb{N}$ puisqu'alors la fonction est polynomiale. Nous supposerons donc $\alpha \notin \mathbb{N}$. Considérons la série entière $1 + \sum_{n \geq 1} \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{n!} t^n$. On a $\frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{n!} \neq 0$ et

$\lim \frac{\frac{\alpha(\alpha-1)\ldots(\alpha-n)}{(n+1)!}}{\frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{n!}} = \lim \frac{\alpha-n}{n+1} = -1$

Donc son rayon de convergence est 1. Posons donc, pour $t \in ]-1,1[$, $S(t) = 1 + \sum_{n=1}^{+\infty} \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{n!} t^n$. On a

\begin{align*}
S'(t) &= \sum_{n=1}^{+\infty} \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{(n-1)!} t^{n-1} \\
&= \sum_{n=0}^{+\infty} \frac{\alpha(\alpha-1)\ldots(\alpha-n)}{n!} t^n
\end{align*}

après un changement d'indice. On en déduit que

\begin{align*}
(1+t)S'(t) &= S'(t) + tS'(t) \\
&= \alpha + \sum_{n=1}^{+\infty} \left(\frac{\alpha(\alpha-1)\ldots(\alpha-n)}{n!} + \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{(n-1)!}\right)t^n
\end{align*}

en utilisant d'abord la seconde expression de $S'(t)$ puis la première. Mais

\begin{align*}
\frac{\alpha(\alpha-1)\ldots(\alpha-n)}{n!} + \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{(n-1)!} &= \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{(n-1)!} \left(\frac{\alpha-n}{n} + 1\right) \\
&= \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{(n-1)!} \frac{\alpha}{n}
\end{align*}

On en déduit que

$(1+t)S'(t) = \alpha + \alpha \sum_{n=1}^{+\infty} \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{n!} t^n = \alpha S(t)$

Alors

$\frac{d}{dt} \left(\frac{S(t)}{(1+t)^\alpha}\right) = \frac{(1+t)S'(t) - \alpha S(t)}{(1+t)^{\alpha+1}} = 0$

donc la fonction est constante sur $]-1,1[$. Comme elle vaut 1 au point 0, elle est constamment égale à 1 et donc $\forall t \in ]-1,1[, (1+t)^\alpha = S(t) = 1 + \sum_{n=1}^{+\infty} \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{n!} t^n$.
\end{proof}

On obtient ainsi facilement des développements en série entière de $(1+t)^{-1}$, $(1+t^2)^{-1}$, $(1-t^2)^{-1}$, $(1-t^2)^{-1/2}$, $(1+t^2)^{-1/2}$, puis par intégration des développements de $\log(1+t)$, $\arctan t$, $\arg\th t$, $\arcsin t$ et $\arg\sh t$. En récapitulant, on obtient la table suivante de développements en série entière

\begin{align*}
e^t &= \sum_{n=0}^{+\infty} \frac{t^n}{n!}, \quad R = +\infty \\
\cos t &= \sum_{n=0}^{+\infty} (-1)^n \frac{t^{2n}}{(2n)!}, \quad R = +\infty \\
\sin t &= \sum_{n=0}^{+\infty} (-1)^n \frac{t^{2n+1}}{(2n+1)!}, \quad R = +\infty \\
\ch t &= \sum_{n=0}^{+\infty} \frac{t^{2n}}{(2n)!}, \quad R = +\infty \\
\sh t &= \sum_{n=0}^{+\infty} \frac{t^{2n+1}}{(2n+1)!}, \quad R = +\infty \\
(1+t)^\alpha &= 1 + \sum_{n=1}^{+\infty} \frac{\alpha(\alpha-1)\ldots(\alpha-n+1)}{n!} t^n, \quad R = 1 \text{ ou } +\infty \\
\frac{1}{1+t} &= \sum_{n=0}^{+\infty} (-1)^n t^n, \quad R = 1 \\
\frac{1}{1-t} &= \sum_{n=0}^{+\infty} t^n, \quad R = 1 \\
\log(1+t) &= \sum_{n=1}^{+\infty} (-1)^{n-1} \frac{t^n}{n}, \quad R = 1 \\
\log(1-t) &= -\sum_{n=1}^{+\infty} \frac{t^n}{n}, \quad R = 1 \\
\arctan t &= \sum_{n=0}^{+\infty} (-1)^n \frac{t^{2n+1}}{2n+1}, \quad R = 1 \\
\arg\th t &= \sum_{n=0}^{+\infty} \frac{t^{2n+1}}{2n+1}, \quad R = 1 \\
\arcsin t &= t + \sum_{n=1}^{+\infty} \frac{1\cdot3\ldots(2n-1)}{2\cdot4\ldots(2n)} \frac{t^{2n+1}}{2n+1}, \quad R = 1 \\
\arg\sh t &= t + \sum_{n=1}^{+\infty} (-1)^n \frac{1\cdot3\ldots(2n-1)}{2\cdot4\ldots(2n)} \frac{t^{2n+1}}{2n+1}, \quad R = 1
\end{align*}

\subsection{Méthodes de sommation}

Le problème est ici, étant donné une série entière $\sum a_n t^n$, de reconnaître une fonction exprimable avec des fonctions classiques dont c'est le développement en série entière. Bien entendu, on utilise toutes les méthodes inverses des méthodes ci-dessus (combinaisons linéaires, produits, changement de variable, dérivation, intégration, équations différentielles). Nous décrirons ici seulement deux cas classiques qui se traitent avec des méthodes spécifiques.

\subsubsection{Séries entières $\sum P(n) t^n$, où $P$ est un polynôme}

Le rayon de convergence est évidemment égal à 1. On a vu que

$\frac{1}{(1-t)^{k+1}} = \sum_{n=0}^{+\infty} C_{n+k}^k t^n = \sum_{n=0}^{+\infty} \frac{(n+k)(n+k-1)\ldots(n+1)}{k!} t^n$

Posons donc $P_0(X) = 1$ et

$P_k(X) = \frac{(X+k)(X+k-1)\ldots(X+1)}{k!}$

pour $k \geq 1$ et soit $d = \deg P$. La famille $(P_0, P_1, \ldots, P_d)$ est une base de $\mathbb{R}_d[X]$ (espace vectoriel des polynômes de degré inférieur ou égal à $d$) car elle est échelonnée en degré. On peut donc écrire $P(X) = \lambda_0 P_0(X) + \lambda_1 P_1(X) + \ldots + \lambda_d P_d(X)$ et alors, pour $t \in ]-1,1[$,

$\sum_{n=0}^{+\infty} P(n) t^n = \frac{\lambda_0}{1-t} + \frac{\lambda_1}{(1-t)^2} + \ldots + \frac{\lambda_d}{(1-t)^{d+1}}$

\begin{rem}
Cette méthode s'étend à des séries entières du type $\sum \frac{P(n)}{n+k} t^n$ où $k$ est un entier. Il suffit en effet de multiplier par $t^k$ et de dériver pour tomber sur une série du type précédent. On peut ensuite, par décomposition en éléments simples sommer les séries du type $\sum \frac{P(n)}{(n+k_1)\ldots(n+k_m)} t^n$ où $k_1,\ldots,k_m$ sont des entiers distincts.
\end{rem}

\subsubsection{Séries entières $\sum \frac{P(n)}{n!} t^n$, où $P$ est un polynôme}

Le rayon de convergence est évidemment égal à $+\infty$. On remarque ici que $\sum_{n=0}^{+\infty} n(n-1)\ldots(n-k+1) \frac{t^n}{n!} = \sum_{n=k}^{+\infty} \frac{t^n}{(n-k)!} = t^k e^t$. Posons donc $P_0(X) = 1$ et $P_k(X) = X(X-1)\ldots(X-k+1)$ pour $k \geq 1$ et soit $d = \deg P$. La famille $(P_0, P_1, \ldots, P_d)$ est une base de $\mathbb{R}_d[X]$ (espace vectoriel des polynômes de degré inférieur ou égal à $d$) car elle est échelonnée en degré. On peut donc écrire $P(X) = \lambda_0 P_0(X) + \lambda_1 P_1(X) + \ldots + \lambda_d P_d(X)$ et alors, pour $t \in \mathbb{R}$, on a

$\sum_{n=0}^{+\infty} P(n) \frac{t^n}{n!} = (\lambda_0 + \lambda_1 t + \ldots + \lambda_d t^d) e^t$