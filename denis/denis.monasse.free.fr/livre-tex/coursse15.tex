\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{graphicx}
% Redefine \includegraphics so that, unless explicit options are
% given, the image width will not exceed the width of the page.
% Images get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\ScaleIfNeeded{%
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother
\let\Oldincludegraphics\includegraphics
{%
 \catcode`\@=11\relax%
 \gdef\includegraphics{\@ifnextchar[{\Oldincludegraphics}{\Oldincludegraphics[width=\ScaleIfNeeded]}}%
}%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Valeurs propres. Vecteurs propres},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.paragraphHead, .likeparagraphHead { margin-top:2em; font-weight: bold;}
.subparagraphHead, .likesubparagraphHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:\jmathustify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.subsubsectionToc, .likesubsubsectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Valeurs propres. Vecteurs propres}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}{]}
{[}

\subsubsection{3.1 Valeurs propres. Vecteurs propres}

\paragraph{3.1.1 Sous-espaces stables}

Définition~3.1.1 Soit E un K-espace vectoriel , u \in L(E). On dit qu'un
sous-espace F de E est stable si u(F) \subset~ F.

Remarque~3.1.1 Dans ce cas on peut considérer l'application (évidemment
linéaire) v : F \rightarrow~ F, x\mapsto~u(x). C'est un
endomorphisme de F appelé l'endomorphisme induit par u.

Proposition~3.1.1 Soit E un K-espace vectoriel de dimension finie, F un
sous-espace de E,
(e\_1,\\ldots,e\_p~)
une base de F complétée en une base \mathcal{E} =
(e\_1,\\ldots,e\_n~)
de E. Soit u \in L(E). Alors F est stable par u si et seulement si la
matrice de u dans la base \mathcal{E} est de la forme \left (
\includegraphics{cours4x.png} \,\right ).

Démonstration En effet F est stable par u si et seulement si
\forall~\jmath \in {[}1,p{]}, u(e\_\jmath~)
\in\mathrmVect(e\_1,\\\ldots,e\_p~),
ce que traduit exactement la forme de la matrice.

Remarque~3.1.2 Dans ce cas la matrice A n'est autre que la matrice dans
la base
(e\_1,\\ldots,e\_p~)
de l'endomorphisme v de F induit par u.

Proposition~3.1.2 Soit E un K-espace vectoriel de dimension finie,
E\_1,\\ldots,E\_p~
une famille de sous-espaces vectoriels de E tels que E = E\_1
\oplus~⋯ \oplus~ E\_p, soit \mathcal{E} une base de E
adaptée à cette décomposition en somme directe. Alors chacun des
E\_i est stable par u si et seulement si la matrice de u dans la
base \mathcal{E} est de la forme

\left
(\matrix\,A\_1& &0
\cr &⋱& \cr 0
& &A\_p\right )

Démonstration La même~; la forme de la matrice traduit exactement que

\forall~i \in {[}1,p{]}, u(\mathcal{E}\_i~)
\subset~\mathrmVect(\mathcal{E}\_i~)
= E\_i

où l'on désigne par \mathcal{E}\_i la base de E\_i extraite de \mathcal{E}.

Définition~3.1.2 Soit E un K-espace vectoriel de dimension finie n~; on
appelle drapeau de E une suite \0\ =
E\_0 \subset~ E\_1 \subset~⋯ \subset~ E\_n
= E de sous-espaces de E tels que dim~
E\_i = i.

Proposition~3.1.3 Soit E un K-espace vectoriel de dimension finie n,
\0\ = E\_0 \subset~ E\_1
\subset~⋯ \subset~ E\_n = E un drapeau de E et \mathcal{E} =
(e\_1,\\ldots,e\_n~)
une base de E adaptée à ce drapeau (c'est-à-dire que pour tout i \in
{[}1,n{]},
(e\_1,\\ldots,e\_i~)
est une base de E\_i). Soit u \in L(E). Alors on a équivalence
de~:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \forall~i \in {[}1,n{]}, u(E\_i~) \subset~
  E\_i
\item
  la matrice de u dans la base \mathcal{E} est triangulaire supérieure.
\end{itemize}

Démonstration En effet, on a évidemment au vu des inclusions
E\_i-1 \subset~ E\_i

\begin{align*} \forall~~i \in
{[}1,n{]}, u(E\_i) \subset~ E\_i&& \%&
\\ & \Leftrightarrow &
\forall~i \in {[}1,p{]}, u(e\_i) \in E\_i~
=\
\mathrmVect(e\_1,\\ldots,e\_i~)\%&
\\ \end{align*}

ce que traduit exactement la forme de la matrice.

\paragraph{3.1.2 Valeurs propres, vecteurs propres}

Définition~3.1.3 Soit E un K-espace vectoriel et u \in L(E). On dit que \lambda~
\in K est valeur propre de u s'il existe x \in E,
x\neq~0 tel que u(x) = \lambda~x. On dit alors que x est
un vecteur propre de u associé à la valeur propre \lambda~. L'ensemble des
valeurs propres de u est appelé le spectre de u et noté
\mathrm{Sp}~(u).

Remarque~3.1.3 On a u(x) = \lambda~x \Leftrightarrow (u -
\lambda~\mathrmId\_E)(x) = 0. On en déduit que \lambda~ est
valeur propre de u si et seulement si u -
\lambda~\mathrmId\_E est non in\jmathectif. Ceci amène
aussi à la définition suivante

Définition~3.1.4 Soit \lambda~
\in\mathrm{Sp}~(u). On appelle
sous-espace propre associé à \lambda~ le sous espace vectoriel E\_u(\lambda~)
= \mathrmKer~(u -
\lambda~\mathrmId\_E) (composé des vecteurs propres
associés à \lambda~ et du vecteur nul).

Remarque~3.1.4 On remarque bien entendu qu'un vecteur propre est associé
à une seule valeur propre (soit E\_u(\lambda~) \bigcap E\_u(\mu) =
\0\). En fait ce résultat peut être
précisé à l'aide du théorème essentiel suivant

Théorème~3.1.4 Soit E un K-espace vectoriel et u \in L(E). Soit
\lambda~\_1,\\ldots,\lambda~\_k~
des valeurs propres distinctes de u. Alors les sous-espaces
E\_u(\lambda~\_i) sont en somme directe.

Démonstration On va démontrer par récurrence sur n que x\_1 +
\\ldots~ +
x\_n = 0 \rigtharrow~\forall~i, x\_i~ = 0 si
x\_i \in E\_u(\lambda~\_i). C'est vrai pour n = 1. On
suppose le résultat vrai pour n - 1 et soit x\_1 +
\\ldots~ +
x\_n = 0. Appliquant u on obtient

\begin{align*} 0& =& u(x\_1) +
\\ldots~ +
u(x\_n) = \lambda~\_1x\_1 +
\\ldots~ +
\lambda~\_nx\_n\%& \\ & =&
\lambda~\_1x\_1 +
\\ldots~ +
\lambda~\_nx\_n - \lambda~\_n(x\_1 +
\\ldots~ +
x\_n) \%& \\ & =& (\lambda~\_1
- \lambda~\_n)x\_1 +
\\ldots~ +
(\lambda~\_n-1 - \lambda~\_n)x\_n-1 \%&
\\ \end{align*}

L'hypothèse de récurrence implique que \forall~~i \in
{[}1,n - 1{]}, (\lambda~\_i - \lambda~\_n)x\_i = 0 soit
x\_i = 0 (car
\lambda~\_i\neq~\lambda~\_n). La relation de
départ donne en plus x\_n = 0.

On en déduit immédiatement

Corollaire~3.1.5 Soit (x\_i)\_i\inI une famille de
vecteurs propres de u associés à des valeurs propres \lambda~\_i deux à
deux distinctes. Alors la famille est libre.

Exemple~3.1.1 La famille d'applications C^\infty~, f\_\lambda~ : \mathbb{R}~
\rightarrow~ \mathbb{C}, t\mapsto~e^\lambda~t est composée de
vecteurs propres de l'opérateur de dérivation (dans l'espace vectoriel
des fonctions C^\infty~ de \mathbb{R}~ dans \mathbb{C})~: Df\_\lambda~ =
\lambda~f\_\lambda~. On en déduit qu'elle est libre.

Enfin le résultat suivant est souvent fort utile

Proposition~3.1.6 Soit u et v deux endomorphismes de E tels que u \cdot v =
v \cdot u. Alors tout sous-espace propre de u est stable par v.

Démonstration Si u(x) = \lambda~x, alors u(v(x)) = v(u(x)) = \lambda~v(x), donc v(x) \in
E\_u(\lambda~).

\paragraph{3.1.3 Polynôme caractéristique}

Remarque~3.1.5 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). On a vu que \lambda~ est valeur propre de u si et seulement si
\lambda~\mathrmId\_E - u est non in\jmathectif, ce qui en
dimension finie signifie que
\mathrm{det}~
(\lambda~\mathrmId\_E - u) = 0. On va donc
introduire un polynôme \chi\_u(X) tel que
\forall~\lambda~ \in K,\chi\_u~(\lambda~)
= \mathrm{det}~
(\lambda~\mathrmId\_E - u).

Définition~3.1.5 Soit M \in M\_K(n). On appelle polynôme
caractéristique de la matrice M le déterminant \chi\_M(X) de la
matrice XI\_n - M \in M\_K{[}X{]}(n).

Proposition~3.1.7

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) si M et M' sont deux matrices semblables, alors \chi\_M' =
  \chi\_M
\item
  (ii) \chi\_^tM = \chi\_M
\item
  (iii) \chi\_M(X) = X^n
  -\mathrm{tr}(M)X^n-1~
  + \\ldots~ +
  (-1)^n\
  \mathrm{det} M
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) Si M' = P^-1MP, alors XI\_n - M' =
  XI\_n - P^-1MP = P^-1(XI\_n -
  M)P et donc \mathrm{det}~
  (XI\_n - M) =\
  \mathrm{det} (XI\_n - M').
\item
  (ii) découle de la même fa\ccon de
  ^t(XI\_n - M) = XI\_n -^tM
\item
  (iii) Le coefficient du terme constant est \chi\_M(0)
  = \mathrm{det}~ (-M) =
  (-1)^n\
  \mathrm{det} M. Pour les coefficients de plus haut
  degré, on écrit \chi\_M(X) =\
  \sum ~
  \_\sigma\inS\_n\epsilon(\sigma)\\∏
   \_i=1^n(\delta\_i^\sigma(i)X -
  a\_i,\sigma(i)). Or le degré de
  \∏ ~
  \_i=1^n(\delta\_i^\sigma(i)X - a\_i,\sigma(i))
  est le nombre de points fixes de \sigma, c'est-à-dire soit n pour \sigma =
  \mathrmId, soit inférieur ou égal à n - 2. Donc
  \chi\_M(X) =\ \∏
   \_i=1^n(X - a\_i,i) + R(X) avec
  deg~ R \leq n - 2. Le résultat en découle
  immédiatement.
\end{itemize}

Remarque~3.1.6 La partie (i) nous montre que si u \in L(E) et si \mathcal{E} est une
base de E, le polynôme caractéristique de la matrice
\mathrmMat~ (u,\mathcal{E}) est
indépendant du choix de \mathcal{E}.

Définition~3.1.6 Soit u \in L(E) où dim~ E
\textless{} +\infty~. On appelle polynôme caractéristique de u le polynôme
caractéristique de sa matrice dans n'importe quelle base de E.

Proposition~3.1.8

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) \chi\_u(X) = X^n
  -\mathrm{tr}(u)X^n-1~
  + \\ldots~ +
  (-1)^n\
  \mathrm{det} u
\item
  (ii) \chi\_^tu = \chi\_u
\item
  (iii) \lambda~ \in\mathrm{Sp}~(u)
  \Leftrightarrow \chi\_u(\lambda~) = 0
\end{itemize}

Définition~3.1.7 Soit \lambda~ une valeur propre de u. On appelle multiplicité
de \lambda~ le nombre m\_u(\lambda~) égal à la multiplicité de \lambda~ comme racine
de \chi\_u.

Lemme~3.1.9 Soit u \in L(E) et F un sous-espace vectoriel de E stable par
u. Soit u' : F \rightarrow~ F défini par u'(x) = u(x) pour x \in F. Alors
\chi\_u'(X) divise \chi\_u(X).

Démonstration Soit ℱ =
(e\_1,\\ldots,e\_p~)
une base de F que l'on complète en \mathcal{E} =
(e\_1,\\ldots,e\_n~)
base de E. Alors M =\
\mathrmMat (u,\mathcal{E}) = \left
(\matrix\,A&B\cr 0
&C\right ) où A =\
\mathrmMat (u',ℱ). On a alors par un calcul de
déterminants par blocs \chi\_M(X) = \chi\_A(X)\chi\_C(X)
ce qui montre que \chi\_u'(X) = \chi\_A(X) divise
\chi\_u(X) = \chi\_M(X).

Théorème~3.1.10 Soit u \in L(E), \lambda~
\in\mathrm{Sp}~(u),
m\_u(\lambda~) la multiplicité de la valeur propre \lambda~ et E\_u(\lambda~)
le sous-espace propre associé à \lambda~. Alors 1 \leq\
dim E\_u(\lambda~) \leq m\_u(\lambda~).

Démonstration E\_u(\lambda~) est stable par u et la restriction u' de u
à E\_u(\lambda~) est l'homothétie de rapport \lambda~ dont le polynôme
caractéristique est \chi\_u'(X) = (X -
\lambda~)^dim E\_u(\lambda~)~. Le lemme
précédent implique donc que dim~
E\_u(\lambda~) \leq m\_u(\lambda~). De plus
E\_u(\lambda~)\neq~\0\,
donc 1 \leq dim E\_u~(\lambda~).

Remarque~3.1.7 On a donc m\_u(\lambda~) = 1 \rigtharrow~\
dim E\_u(\lambda~) = 1.

\paragraph{3.1.4 Endomorphismes diagonalisables}

Définition~3.1.8 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). On dit que u est diagonalisable s'il vérifie les conditions
équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) il existe une base \mathcal{E} de E telle que
  \mathrmMat~ (u,\mathcal{E}) soit
  diagonale
\item
  (ii) il existe une base \mathcal{E} de E formée de vecteurs propres de u
\item
  (iii) E est somme (directe) des sous-espaces propres de u
\end{itemize}

Démonstration (i) et (ii) sont évidemment équivalents. Réunissant des
bases des sous-espaces propres de u, on a bien évidemment (iii) \rigtharrow~(ii).
Supposons maintenant que (i) est vrai. Quitte à permuter la base, on
peut supposer que
\mathrmMat~ (u,\mathcal{E})
=\
\mathrmdiag(\lambda~\_1,\\ldots,\lambda~\_1,\lambda~\_2,\\\ldots,\lambda~\_2,\\\ldots,\\\ldots,\lambda~\_k,\\\ldots,\lambda~\_k~)
avec
\lambda~\_1,\\ldots,\lambda~\_k~
deux à deux distincts, \lambda~\_i figurant m\_i fois. On a
alors dim E\_u(\lambda~\_i~) ≥
m\_i (on a m\_i vecteurs de base dans cet espace), soit

dim~ \\oplus~
\_\lambda~\in\mathrm{Sp}(u)E\_u(\lambda~) =
\sum \_i=1^k dim E~\_
u(\lambda~\_i) ≥\\sum
\_i=1^km\_ i = dim E

et donc E = \\oplus~ ~
\_\lambda~\in\mathrm{Sp}(u)E\_u~(\lambda~).
Donc (i) \rigtharrow~(iii).

Théorème~3.1.11 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors les conditions suivantes sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) u est diagonalisable
\item
  (ii) \chi\_u(X) est scindé sur K et pour toute valeur propre \lambda~ de
  u la dimension du sous-espace propre associé est égale à la
  multiplicité de la valeur propre.
\end{itemize}

Démonstration Supposons u diagonalisable et soit \mathcal{E} une base de E telle
que \mathrmMat~ (u,\mathcal{E}) = D
=\
\mathrmdiag(\lambda~\_1,\\ldots,\lambda~\_1,\lambda~\_2,\\\ldots,\lambda~\_2,\\\ldots,\\\ldots,\lambda~\_k,\\\ldots,\lambda~\_k~)
avec
\lambda~\_1,\\ldots,\lambda~\_k~
deux à deux distincts, \lambda~\_i figurant m\_i fois. Alors
\chi\_u(X) = \chi\_D(X) =\
∏  \_i=1^k~(X -
\lambda~\_i)^m\_i ce qui montre dé\jmathà que \chi\_u
est scindé et que les valeurs propres de u sont exactement
\lambda~\_1,\\ldots,\lambda~\_k~.
De plus dim E\_u(\lambda~\_i~) ≥
m\_i = m\_u(\lambda~\_i) puisque
E\_u(\lambda~\_i) contient une famille libre de cardinal
m\_i. On a donc dim~
E\_u(\lambda~\_i) = m\_u(\lambda~\_i), soit (i) \rigtharrow~(ii).
Inversement supposons (ii) vérifié. On a alors

\begin{align*} dim~
\oplus~ \_i=1^kE~\_
u(\lambda~\_i)& =& \\sum
\_i=1^km\_ u(\lambda~\_i) = deg
\chi\_u(X)\%& \\ & =&
dim~ E \%& \\
\end{align*}

puisque le polynôme est scindé. Soit E =\
\oplus~ ~
\_i=1^kE\_u(\lambda~\_i).

Corollaire~3.1.12 Soit E un K-espace vectoriel de dimension finie et u \in
L(E) tel que \chi\_u soit scindé à racines simples. Alors u est
diagonalisable.

Remarque~3.1.8 Pratique de la diagonalisation

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) calculer le polynôme caractéristique et en chercher les racines
  avec leurs multiplicités
\item
  (ii) pour chaque racine déterminer le sous-espace propre
  correspondant, défini par l'équation (u -
  \lambda~\mathrmId)(x) = 0~; comparer dimension du
  sous-espace propre et multiplicité de la valeur propre
\item
  (iii) déterminer une base de chaque sous-espace propre et les réunir
  en une base de E.
\end{itemize}

\paragraph{3.1.5 Matrices diagonalisables}

Définition~3.1.9 Soit M \in M\_K(n). On définit de manière
évidente les valeurs propres et vecteurs propres de M~: MX = \lambda~X avec
X\neq~0.

Définition~3.1.10 Soit M \in M\_K(n). On dit que M est
diagonalisable si elle vérifie les conditions équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est la matrice d'un endomorphisme diagonalisable dans une
  certaine base
\item
  (ii) M est semblable à une matrice diagonale
\item
  (iii) il existe une base de K^n ∼ M\_K(n,1) formée
  de vecteurs propres de M
\item
  (iii) K^n ∼ M\_K(n,1) est somme directe des
  sous-espaces propres de M
\end{itemize}

Démonstration Tout ceci est élémentaire.

On a immédiatement

Théorème~3.1.13 Soit M \in M\_K(n). Alors les conditions suivantes
sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est diagonalisable
\item
  (ii) \chi\_M(X) est scindé sur K et pour toute valeur propre \lambda~ de
  M la dimension du sous-espace propre associé est égale à la
  multiplicité de la valeur propre.
\end{itemize}

Corollaire~3.1.14 Soit M \in M\_K(n) telle que \chi\_M soit
scindé à racines simples. Alors M est diagonalisable.

Remarque~3.1.9 Pratique de la diagonalisation

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) calculer le polynôme caractéristique et en chercher les racines
\item
  (ii) pour chaque racine déterminer le sous-espace propre
  correspondant, défini par l'équation (M - \lambda~I\_n)X = 0~; ceci
  conduit à un système homogène de rang r\_M(\lambda~)~; on a
  dim E\_M(\lambda~) = n - r\_M~(\lambda~)~;
  comparer dimension du sous-espace propre et multiplicité de la valeur
  propre
\item
  (iii) déterminer une base de chaque sous-espace propre~; soit P la
  matrice qui admet ces vecteurs propres comme vecteurs colonnes~; alors
  P^-1MP est diagonale.
\end{itemize}

\paragraph{3.1.6 Endomorphismes et matrices trigonalisables}

Définition~3.1.11 Soit E un K-espace vectoriel de dimension finie. On
dit que u est trigonalisable s'il vérifie les conditions équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) il existe une base \mathcal{E} de E telle que
  \mathrmMat~ (u,\mathcal{E}) soit
  triangulaire (supérieure)
\item
  (ii) il existe une base \mathcal{E} de E telle que \forall~~i,
  u(e\_i)
  \in\mathrmVect(e\_1,\\\ldots,e\_i~)
\item
  (iii) il existe une suite \0\ =
  F\_0 \subset~ F\_1 \subset~⋯ \subset~
  F\_n = E de sous-espaces de E tels que
  dim F\_i = i et u(F\_i~) \subset~
  F\_i.
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) et (ii) sont trivialement équivalents
\item
  (i) \rigtharrow~(iii)~: prendre F\_i =\
  \mathrmVect(e\_1,\\ldots,e\_i~)
\item
  (iii) \rigtharrow~(i)~: construire par applications successives du théorème de la
  base incomplète une base
  (e\_1,\\ldots,e\_n~)
  telle que F\_i =\
  \mathrmVect(e\_1,\\ldots,e\_i~).
\end{itemize}

Théorème~3.1.15 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors les conditions suivantes sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) u est trigonalisable
\item
  (ii) \chi\_u(X) est scindé sur K (ce qui est automatiquement
  vérifié si K est algébriquement clos)
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) \rigtharrow~(ii)~: si M =\
  \mathrmMat (u,\mathcal{E}) = \left
  (\matrix\,a\_1,1&\\ldots~
  &\\ldots&\\\ldots~
  \cr 0
  &a\_2,2&\\ldots&\\\ldots~
  \cr &
  &⋱&\\ldots~
  \cr 0
  &\\ldots~
  &0&a\_n,n\right ), on a \chi\_u(X) =
  \chi\_M(X) =\ \∏
   \_i=1^n(X - a\_i,i). Donc \chi\_u est
  scindé.
\item
  (ii) \rigtharrow~ (i). Par récurrence sur n~; il n'y a rien à démontrer si n = 1.
  Supposons \chi\_u scindé, et soit \lambda~ une racine de \chi\_u.
  Soit e\_1 un vecteur propre associé à \lambda~, que l'on complète en
  (e\_1,\\ldots,e\_n~)
  base de E. Soit F =\
  \mathrmVect(e\_2,\\ldots,e\_n~),
  p la pro\jmathection sur F parallèlement à Ke\_1 et v : F \rightarrow~ F
  défini par v(x) = p(u(x)) si x \in F. Alors M =\
  \mathrmMat (u,\mathcal{E}) = \left
  (\matrix\,\lambda~&∗∗∗ \cr
  \matrix\,0 \cr
  \⋮~
  \cr 0&A \right ) avec A
  = \mathrmMat~
  (v,(e\_2,\\ldots,e\_n~)).
  On en déduit que \chi\_u(X) = (X - \lambda~)\chi\_v(X). Donc
  \chi\_v est aussi scindé. Par hypothèse de récurrence, il existe
  une base
  (\epsilon\_2,\\ldots,\epsilon\_n~)
  de F telle que \mathrmMat~
  (v,(\epsilon\_2,\\ldots,\epsilon\_n~))
  soit triangulaire supérieure et alors
  \mathrmMat~
  (u,(e\_1,\epsilon\_2,\\ldots,\epsilon\_n~))
  = \left (\matrix\,\lambda~&∗
  \cr \matrix\,0
  \cr
  \⋮~
  \cr
  0&\mathrmMat~
  (v,(\epsilon\_2,\\ldots,\epsilon\_n))~\right
  ) est triangulaire supérieure.
\end{itemize}

Remarque~3.1.10 Comme pour la diagonalisation, ces notions passent
immédiatement aux matrices

Définition~3.1.12 Soit M \in M\_K(n). On dit que M est
trigonalisable si elle vérifie les conditions équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est la matrice d'un endomorphisme trigonalisable dans une
  certaine base
\item
  (ii) M est semblable à une matrice triangulaire (supérieure).
\end{itemize}

Théorème~3.1.16 Soit M \in M\_K(n). Alors les conditions suivantes
sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est trigonalisable
\item
  (ii) \chi\_M(X) est scindé sur K (ce qui est automatiquement
  vérifié si K est algébriquement clos)
\end{itemize}

Corollaire~3.1.17 L'ensemble des matrices diagonalisables est dense dans
M\_\mathbb{C}(n).

Démonstration Soit M \in M\_\mathbb{C}(n) et P inversible telle que

P^-1MP = T = \left
(\matrix\,a\_1,1&\\ldots~
&\\ldots&\\\ldots~
\cr 0
&a\_2,2&\\ldots&\\\ldots~
\cr &
&⋱&\\ldots~
\cr 0
&\\ldots~
&0&a\_n,n\right )

et posons pour p \in \mathbb{N}~,

T\_p = \left
(\matrix\,a\_1,1 + 1
\over p
&\\ldots~
&\\ldots&\\\ldots~
\cr 0 &a\_2,2 + 1 \over
p^2
&\\ldots&\\\ldots~
\cr &
&⋱&\\ldots~
\cr 0
&\\ldots~
&0&a\_n,n + 1 \over p^n
\right )

Il n'y a qu'un nombre fini de p pour lesquels on peut avoir
a\_i,i + 1 \over p^i =
a\_\jmath,\jmath + 1 \over p^\jmath (il s'agit en
effet d'une équation polynomiale en  1 \over p ). On
en déduit que pour tous les p sauf en nombre fini, T\_p a un
polynôme caractéristique scindé à racines simples, donc est
diagonalisable. Il en est donc de même de M\_p =
PT\_pP^-1. Or
lim\_p\rightarrow~+\infty~M\_p~ =
PTP^-1 = M. Donc M est limite d'une suite de matrices
diagonalisables.

{[}
{[}

\end{document}
