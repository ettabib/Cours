\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Complements : developpements asymptotiques, analyse numerique},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.paragraphHead, .likeparagraphHead { margin-top:2em; font-weight: bold;}
.subparagraphHead, .likesubparagraphHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:\jmathustify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.subsubsectionToc, .likesubsubsectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Complements : developpements asymptotiques, analyse numerique}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

{[}
{[}
{[}{]}
{[}

\subsubsection{7.9 Compléments~: développements asymptotiques, analyse
numérique}

\paragraph{7.9.1 Calcul approché de la somme d'une série}

L'idée naturelle est d'approcher la somme S de la série convergente
\\sum  x\_n~ par
une somme partielle S\_N =\
\sum  \_n=0^Nx\_n~.
L'erreur de méthode est évidemment égale à R\_N
= \\sum ~
\_n=N+1^+\infty~x\_n. Bien entendu, à cette erreur de
méthode vient s'a\jmathouter une erreur de calcul de la somme S\_N
que l'on peut estimer ma\jmathorée par N\epsilon où \epsilon est la précision de
l'instrument de calcul. Entre la valeur cherchée S et la valeur calculée
\overlineS\_N il y a donc une erreur du type
\textbar{}S
-\overlineS\_N\textbar{}\leq\textbar{}R\_N\textbar{}
+ N\epsilon = \delta(N) que l'on cherchera donc à minimiser (la fonction \delta tend
manifestement vers + \infty~ quand N croît indéfiniment).

Etudions pour cela deux cas. Dans le premier cas, la série est à
convergence géométrique~: \textbar{}x\_n\textbar{}\leq
A\rho^n avec \rho \textless{} 1. Alors R\_N \leq
B\rho^N et \delta(N) \leq \delta\_1(N) = B\rho^N + N\epsilon. On a
\delta\_1'(t) = B(log \rho)\rho^t~ + \epsilon
qui s'annule pour t = t\_0 = 1 \over \rho
 log~ \left \textbar{} \epsilon
\over B log \rho~
\right \textbar{}. On a intérêt à choisir N aussi proche
que possible de t\_0 où la fonction \delta\_1 atteint son
minimum.

Exemple~7.9.1 ~: \epsilon = 10^-8,B = 1,\rho = 9 \over
10 . On trouve un N de l'ordre de 150 pour une erreur de l'ordre de
10^-5. C'est parfaitement raisonnable.

Dans le second cas, la série est à convergence polynomiale~:
\textbar{}x\_n\textbar{}\leq A \over
n^\alpha~ avec \alpha~ \textgreater{} 1. Alors R\_N \leq B
\over n^\alpha~-1 et \delta(N) \leq \delta\_1(N) = B
\over N^\alpha~-1 + N\epsilon. On a \delta\_1'(t) =
B(1 - \alpha~)t^-\alpha~ + \epsilon qui s'annule pour t = t\_0 =
\left ( B(\alpha~-1) \over \epsilon
\right )^ 1 \over \alpha~ . On a
intérêt à choisir N aussi proche que possible de t\_0 où la
fonction \delta\_1 atteint son minimum.

Exemple~7.9.2 ~: \epsilon = 10^-8,B = 1,\alpha~ = 11
\over 10 . On trouve un N de l'ordre de 10^7
pour une erreur de l'ordre de 0,25. On voit que la méthode fournit un
résultat très médiocre en un temps très long~; elle demande donc à être
améliorée par une accélération de convergence.

\paragraph{7.9.2 Accélération de la convergence}

Supposons que x\_n admet un développement asymptotique de la
forme

x\_n = a\_o \over n^K +
a\_1 \over n^K+1 +
\\ldots~ +
a\_N \over n^K+N + \epsilon\_n

avec \textbar{}\epsilon\_n\textbar{}\leq A \over
n^K+N+1 . Posons u\_n = b\_o
\over n^K-1 +
\\ldots~ +
b\_N \over n^K+N-1 (où
b\_o,\\ldots,b\_N~
sont des coefficients à déterminer) puis y\_n = u\_n -
u\_n+1 , et cherchons à déterminer les b\_i de telle
sorte que \textbar{}x\_n - y\_n\textbar{}\leq B
\over n^K+N+1 (pour une certaine constante
B), c'est-à-dire, x\_n - y\_n = O( 1
\over n^K+N+1 ). On a u\_n
= \\sum ~
\_i=0^N b\_i \over
n^K+i-1 , d'où

\begin{align*} y\_n& =&
\sum \_i=0^Nb~\_
i\left ( 1 \over n^K+i-1
- 1 \over (n + 1)^K+i-1
\right ) \%& \\ & =&
\sum \_i=0^Nb\_ i~ 1
\over n^K+i-1 \left (1 - (1
+ 1 \over n )^1-K-i\right
)\%& \\ \end{align*}

On sait que la fonction f\_\alpha~(x) = (1 + x)^\alpha~ admet au
voisinage de 0 un développement limité f\_\alpha~(x) = 1
+ \\sum ~
\_k=1^pc\_k^(\alpha~)x^k +
O(x^p+1) avec c\_k^(\alpha~) =
\alpha~(\alpha~-1)\\ldots~(\alpha~-k+1)
\over k! . On en déduit que

1 - (1 + 1 \over n )^1-K-i =
-\sum \_k=1^N+1-ic~\_
k^(1-K-i) 1 \over n^k + O( 1
\over n^N+2-i )

soit

\begin{align*} 1 \over
n^K+i-1 \left (1 - (1 + 1
\over n )^1-K-i\right )& =&
-\sum \_k=1^N+1-ic~\_
k^(1-K-i) 1 \over n^k+K+i-1 +
O( 1 \over n^N+K+1 )\%&
\\ & =& -\\sum
\_k=i^Nc\_ k+1-i^(1-K-i) 1
\over n^k+K + O( 1 \over
n^N+K+1 ) \%& \\
\end{align*}

après changement d'indices. On en déduit

\begin{align*} y\_n& =&
-\sum \_i=0^Nb\_ i~
\sum \_k=i^Nc~\_
k+1-i^(1-K-i) 1 \over n^k+K +
O( 1 \over n^N+K+1 )\%&
\\ & =& -\\sum
\_k=0^N 1 \over n^k+K 
\sum \_i=0^kb~\_
ic\_k+1-i^(1-K-i) + O( 1 \over
n^N+K+1 )\%& \\
\end{align*}

Donc

x\_n - y\_n = O( 1 \over
n^K+N+1 ) \Leftrightarrow
\forall~k \in {[}0,n{]}, a\_k~ +
\sum \_i=0^kb~\_
ic\_1-k-i^(1-K-i) = 0

Il s'agit d'un système triangulaire en les inconnues b\_i qui
admet une unique solution. En faisant le changement d'indice \jmath = k + 1 -
i, on obtient le système

\forall~k \in {[}0,n{]}, a\_k~ +
\sum \_\jmath=1^k+1b~\_
k+1-\jmathc\_\jmath^(-K-k+\jmath) = 0

On calcule donc les b\_k à l'aide de la formule de récurrence
c\_1^(-K-k+1)b\_k = -a\_k
-\\sum ~
\_\jmath=2^k+1b\_k+1-\jmathc\_\jmath^(-K-k+\jmath) où
les c\_\jmath^(t+\jmath) sont définis par récurrence par
c\_1^(t+1) = t + 1 et c\_\jmath+1^(t+\jmath+1) =
t+\jmath+1 \over \jmath+1 c\_\jmath^(t+\jmath). Supposons
les b\_i déterminés. Il existe une constante B telle que
\textbar{}x\_n - y\_n\textbar{}\leq B
\over n^K+N+1 . L'erreur faite en approchant
la somme de la série \\\sum
 (x\_n - y\_n) par sa somme partielle d'indice n est
donc ma\jmathorée par  B \over K+N  1
\over n^K+N . Mais la somme partielle
d'indice n de la série est

\sum \_k=1^n(x\_ k~ -
y\_k) = \\sum
\_k=1^nx\_ k -\\sum
\_k=1^n(u\_ k - u\_k+1) = S\_n +
u\_1 - u\_n+1

et la somme de la série est

\sum \_n=1^+\infty~(x\_ n~ -
y\_n) = \\sum
\_n=1^+\infty~x\_ n -\\sum
\_n=1^+\infty~(u\_ n - u\_n+1) = S -
u\_1

(puisque limu\_n~ = 0). On a donc
\textbar{}S - S\_n + u\_n+1\textbar{}\leq B
\over K+N  1 \over n^K+N
et S\_n - u\_n+1 est donc une bien meilleure valeur
approchée de S que S\_n.

Bien entendu ces méthodes peuvent se généraliser à d'autres types de
développements asymptotiques~: l'idée générale étant de trouver une
suite u\_n telle que la série x\_n - (u\_n -
u\_n+1) ait une décroissance vers 0 aussi rapide que possible.
Alors S\_n - u\_n+1 est donc une bien meilleure valeur
approchée de S que S\_n. Cette méthode fournira également des
développements asymptotiques de restes de séries car si x\_n -
(u\_n - u\_n+1) = o(v\_n), on aura
R\_n(x) + u\_n+1 = o(R\_n(v)) et donc le
développement R\_n(x) = -u\_n+1 + o(R\_n(v)).

En ce qui concerne les développements asymptotiques de sommes partielles
de séries divergentes, on se ramènera à la situation précédente en
rempla\ccant la série x\_n par une série du
type y\_n = x\_n - (v\_n - v\_n-1) de
telle sorte que la série
\\sum  y\_n~
converge. On aura alors S\_n(x) = v\_n - v\_0 +
S\_n(y) = v\_n + A + R\_n(y) où A = S(y) -
v\_0 est une constante (sa valeur ne pourra pas être obtenue
directement par cette méthode). Il suffira ensuite d'appliquer la
méthode précédente pour obtenir un développement asymptotique de
R\_n(y) à la précision souhaitée, et donc aussi un développement
asymptotique de R\_n(x).

Nous allons traiter deux exemples importants des techniques ci dessus.

Exemple~7.9.3 On recherche un développement asymptotique de
\\sum ~
\_k=1^n 1 \over k . Posons x\_n
= 1 \over n et y\_n =\
log (n) - log~ (n - 1) =
-log (1 - 1 \over n~ ). On a
z\_n = x\_n - y\_n = 1 \over
n - log (1 - 1 \over n~ )
= - 1 \over 2n^2 + O( 1
\over n^3 ). On en déduit que la série
\\sum  z\_n~
converge. On a alors

\begin{align*} \\sum
\_k=1^nx\_ k& =& 1 +
\sum \_k=2^nz\_ k~ +
\sum \_k=2^ny\_ k~ = 1 +
\sum \_k=2^nz\_ k~ +
\sum \_k=2^n~(log k - log (k -
1))\%& \\ & =&
log~ n + (1 + \\sum
\_k=2^+\infty~z\_ k) - R\_n(z)
\%&\\ \end{align*}

Mais les théorèmes de comparaison des séries à termes de signes
constants assurent que puisque z\_n ∼- 1 \over
2n^2 , on a R\_n(z) ∼- 1 \over 2
 \\sum ~
\_k=n+1^+\infty~ 1 \over k^2 ∼- 1
\over 2n . Posons alors \gamma = 1
+ \\sum ~
\_k=2^+\infty~z\_k (la constante d'Euler)~; on obtient

\sum \_k=1^n~ 1
\over k = log n + \gamma + 1 \over 2n +
o( 1 \over n )

(en fait il est clair que les techniques ci dessus permettent d'obtenir
un développement à un ordre arbitraire).

Exemple~7.9.4 Nous allons maintenant montrer la formule de Stirling, n!
∼\sqrt2\pi~n n^n \over
e^n . Pour cela posons a\_n = n!e^n
\over n^n+1\diagup2 et b\_n
= log a\_n~ -\
log a\_n-1 (pour n ≥ 2). On a

\begin{align*} b\_n& =&
log  a\_n~ \over
a\_n-1 = log  n!e^n~(n
- 1)^n-1\diagup2 \over (n -
1)!e^n-1n^n+1\diagup2 \%&
\\ & =& log~
\left (e (n - 1)^n-1\diagup2 \over
n^n-1\diagup2 \right ) = 1 + (n - 1
\over 2 )log~ (1 - 1
\over n )\%& \\
\end{align*}

d'où b\_n = 1 + (n - 1 \over 2 )(- 1
\over n - 1 \over 2n^n
- 1 \over 3n^3 + O( 1
\over n^4 )) = - 1 \over
12n^2 + O( 1 \over n^3 ) On
en déduit que la série \\\sum
 b\_n converge. Soit S sa somme. On a alors
\\sum ~
\_k=2^nb\_k = S - R\_n(b), mais comme
b\_n ∼- 1 \over 12n^2 , on a
R\_n(b) ∼- 1 \over 12
 \\sum ~
\_k=n+1^+\infty~ 1 \over k^2 ∼- 1
\over 12n . On a d'autre part
\\sum ~
\_k=2^nb\_k = log~
a\_n - log a\_1~, d'où
finalement log a\_n~
= \\sum ~
\_k=2^nb\_k + log~
a\_1 = S + log a\_1~ + 1
\over 12n + o( 1 \over n ) et donc
a\_n = e^S+log~
a\_1 exp~ ( 1 \over
12n + o( 1 \over n )) = \ell(1 + 1
\over 12n + o( 1 \over n )) en
posant \ell = e^S+log a\_1~
\textgreater{} 0, soit encore

n! = \ell n^n+1\diagup2 \over n!
\left (1 + 1 \over 12n + o( 1
\over n )\right )

La méthode précédente ne permet pas d'obtenir la valeur de \ell~; on
obtient celle ci classiquement à l'aide des intégrales de Wallis~:
I\_n =\int ~
\_0^\pi~\diagup2 sin ^n~x dx.
Pour n ≥ 2, on écrit à l'aide d'une intégration par parties, en
intégrant sin~ x et en dérivant
sin ^n-1~x

\begin{align*} I\_n& =&
\int ~
\_0^\pi~\diagup2 sin~
^n-1xsin~ x dx \%&
\\ & =& \left
{[}-cos x\sin~
^n-1x\right {]}\_ 0^\pi~\diagup2 + (n -
1)\int ~
\_0^\pi~\diagup2 sin~
^n-2xcos ^2~x dx \%&
\\ & =& (n -
1)\int ~
\_0^\pi~\diagup2 sin ^n-2~x(1
- sin ^2x) dx = (n - 1)(I~\_
n-2 - I\_n)\%& \\
\end{align*}

d'où I\_n = n-1 \over n I\_n-2. En
tenant compte de I\_0 = \pi~ \over 2 et
I\_1 = 1, on a alors

I\_2p = (2p - 1)(2p -
3)\\ldots~3.1
\over (2p)(2p -
2)\\ldots4.2~  \pi~
\over 2 = (2p)! \over
2^p(p!)^2  \pi~ \over 2

en multipliant numérateur et dénominateur par (2p)(2p -
2)\\ldots~4.2 de
manière à rétablir les facteurs manquant au numérateur. De même

I\_2p+1 = (2p)(2p -
2)\\ldots~4.2
\over (2p + 1)(2p -
1)\\ldots3~ =
2^p(p!)^2 \over (2p + 1)!

On en déduit en utilisant n! ∼ \ell\sqrtn
n^n \over n!

 I\_2p \over I\_2p+1 = (2p +
1)(2p)!^2 \over
2^4pp!^4  \pi~ \over 2 ∼ (2p +
1)\ell^2(2p)(2p)^4pe^4p
\over
2^4pe^4p\ell^4p^2p^4p
 \pi~ \over 2 ∼ 2\pi~ \over
\ell^2

Mais d'autre part, on a \forall~~x \in {[}0, \pi~
\over 2 {]}, 0 \leq sin~
^n+1x \leq sin ^n~x
\leq sin ^n-1~x, soit en intégrant 0 \leq
I\_n+1 \leq I\_n \leq I\_n-1 et en tenant compte de 
I\_n-1 \over I\_n+1 = n+1
\over n , on obtient 1 \leq I\_n
\over I\_n+1 \leq n+1 \over n
soit encore lim I\_n~
\over I\_n+1 = 1. On en déduit que  2\pi~
\over \ell^2 = 1 et comme \ell \textgreater{} 0, \ell
= \sqrt2\pi~ ce qui achève la démonstration.

{[}
{[}
{[}
{[}

\end{document}
