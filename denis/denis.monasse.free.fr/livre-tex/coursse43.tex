\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Complements : developpements asymptotiques, analyse numerique},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.paragraphHead, .likeparagraphHead { margin-top:2em; font-weight: bold;}
.subparagraphHead, .likesubparagraphHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:justify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.subsubsectionToc, .likesubsubsectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Complements : developpements asymptotiques, analyse numerique}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

[
[
[]
[

\subsubsection{7.9 Compléments~: développements asymptotiques, analyse
numérique}

\paragraph{7.9.1 Calcul approché de la somme d'une série}

L'idée naturelle est d'approcher la somme S de la série convergente
\\sum  x_n~ par
une somme partielle S_N =\
\sum  _n=0^Nx_n~.
L'erreur de méthode est évidemment égale à R_N
= \\sum ~
_n=N+1^+\infty~x_n. Bien entendu, à cette erreur de
méthode vient s'ajouter une erreur de calcul de la somme S_N
que l'on peut estimer majorée par N\epsilon où \epsilon est la précision de
l'instrument de calcul. Entre la valeur cherchée S et la valeur calculée
\overlineS_N il y a donc une erreur du type
S
-\overlineS_N\leqR_N
+ N\epsilon = \delta(N) que l'on cherchera donc à minimiser (la fonction \delta tend
manifestement vers + \infty~ quand N croît indéfiniment).

Etudions pour cela deux cas. Dans le premier cas, la série est à
convergence géométrique~: x_n\leq
A\rho^n avec \rho < 1. Alors R_N \leq
B\rho^N et \delta(N) \leq \delta_1(N) = B\rho^N + N\epsilon. On a
\delta_1'(t) = B(log \rho)\rho^t~ + \epsilon
qui s'annule pour t = t_0 = 1 \over \rho
 log~ \left  \epsilon
\over B log \rho~
\right . On a intérêt à choisir N aussi proche
que possible de t_0 où la fonction \delta_1 atteint son
minimum.

Exemple~7.9.1 ~: \epsilon = 10^-8,B = 1,\rho = 9 \over
10 . On trouve un N de l'ordre de 150 pour une erreur de l'ordre de
10^-5. C'est parfaitement raisonnable.

Dans le second cas, la série est à convergence polynomiale~:
x_n\leq A \over
n^\alpha~ avec \alpha~ > 1. Alors R_N \leq B
\over n^\alpha~-1 et \delta(N) \leq \delta_1(N) = B
\over N^\alpha~-1 + N\epsilon. On a \delta_1'(t) =
B(1 - \alpha~)t^-\alpha~ + \epsilon qui s'annule pour t = t_0 =
\left ( B(\alpha~-1) \over \epsilon
\right )^ 1 \over \alpha~ . On a
intérêt à choisir N aussi proche que possible de t_0 où la
fonction \delta_1 atteint son minimum.

Exemple~7.9.2 ~: \epsilon = 10^-8,B = 1,\alpha~ = 11
\over 10 . On trouve un N de l'ordre de 10^7
pour une erreur de l'ordre de 0,25. On voit que la méthode fournit un
résultat très médiocre en un temps très long~; elle demande donc à être
améliorée par une accélération de convergence.

\paragraph{7.9.2 Accélération de la convergence}

Supposons que x_n admet un développement asymptotique de la
forme

x_n = a_o \over n^K +
a_1 \over n^K+1 +
\\ldots~ +
a_N \over n^K+N + \epsilon_n

avec \epsilon_n\leq A \over
n^K+N+1 . Posons u_n = b_o
\over n^K-1 +
\\ldots~ +
b_N \over n^K+N-1 (où
b_o,\\ldots,b_N~
sont des coefficients à déterminer) puis y_n = u_n -
u_n+1 , et cherchons à déterminer les b_i de telle
sorte que x_n - y_n\leq B
\over n^K+N+1 (pour une certaine constante
B), c'est-à-dire, x_n - y_n = O( 1
\over n^K+N+1 ). On a u_n
= \\sum ~
_i=0^N b_i \over
n^K+i-1 , d'où

\begin{align*} y_n& =&
\sum _i=0^Nb_
i\left ( 1 \over n^K+i-1
- 1 \over (n + 1)^K+i-1
\right ) \%& \\ & =&
\sum _i=0^Nb_ i~ 1
\over n^K+i-1 \left (1 - (1
+ 1 \over n )^1-K-i\right
)\%& \\ \end{align*}

On sait que la fonction f_\alpha~(x) = (1 + x)^\alpha~ admet au
voisinage de 0 un développement limité f_\alpha~(x) = 1
+ \\sum ~
_k=1^pc_k^(\alpha~)x^k +
O(x^p+1) avec c_k^(\alpha~) =
\alpha~(\alpha~-1)\\ldots~(\alpha~-k+1)
\over k! . On en déduit que

1 - (1 + 1 \over n )^1-K-i =
-\sum _k=1^N+1-ic_
k^(1-K-i) 1 \over n^k + O( 1
\over n^N+2-i )

soit

\begin{align*} 1 \over
n^K+i-1 \left (1 - (1 + 1
\over n )^1-K-i\right )& =&
-\sum _k=1^N+1-ic_
k^(1-K-i) 1 \over n^k+K+i-1 +
O( 1 \over n^N+K+1 )\%&
\\ & =& -\\sum
_k=i^Nc_ k+1-i^(1-K-i) 1
\over n^k+K + O( 1 \over
n^N+K+1 ) \%& \\
\end{align*}

après changement d'indices. On en déduit

\begin{align*} y_n& =&
-\sum _i=0^Nb_ i~
\sum _k=i^Nc_
k+1-i^(1-K-i) 1 \over n^k+K +
O( 1 \over n^N+K+1 )\%&
\\ & =& -\\sum
_k=0^N 1 \over n^k+K 
\sum _i=0^kb_
ic_k+1-i^(1-K-i) + O( 1 \over
n^N+K+1 )\%& \\
\end{align*}

Donc

x_n - y_n = O( 1 \over
n^K+N+1 ) \Leftrightarrow
\forall~k \in [0,n], a_k~ +
\sum _i=0^kb_
ic_1-k-i^(1-K-i) = 0

Il s'agit d'un système triangulaire en les inconnues b_i qui
admet une unique solution. En faisant le changement d'indice j = k + 1 -
i, on obtient le système

\forall~k \in [0,n], a_k~ +
\sum _j=1^k+1b_
k+1-jc_j^(-K-k+j) = 0

On calcule donc les b_k à l'aide de la formule de récurrence
c_1^(-K-k+1)b_k = -a_k
-\\sum ~
_j=2^k+1b_k+1-jc_j^(-K-k+j) où
les c_j^(t+j) sont définis par récurrence par
c_1^(t+1) = t + 1 et c_j+1^(t+j+1) =
t+j+1 \over j+1 c_j^(t+j). Supposons
les b_i déterminés. Il existe une constante B telle que
x_n - y_n\leq B
\over n^K+N+1 . L'erreur faite en approchant
la somme de la série \\\sum
 (x_n - y_n) par sa somme partielle d'indice n est
donc majorée par  B \over K+N  1
\over n^K+N . Mais la somme partielle
d'indice n de la série est

\sum _k=1^n(x_ k~ -
y_k) = \\sum
_k=1^nx_ k -\\sum
_k=1^n(u_ k - u_k+1) = S_n +
u_1 - u_n+1

et la somme de la série est

\sum _n=1^+\infty~(x_ n~ -
y_n) = \\sum
_n=1^+\infty~x_ n -\\sum
_n=1^+\infty~(u_ n - u_n+1) = S -
u_1

(puisque limu_n~ = 0). On a donc
S - S_n + u_n+1\leq B
\over K+N  1 \over n^K+N
et S_n - u_n+1 est donc une bien meilleure valeur
approchée de S que S_n.

Bien entendu ces méthodes peuvent se généraliser à d'autres types de
développements asymptotiques~: l'idée générale étant de trouver une
suite u_n telle que la série x_n - (u_n -
u_n+1) ait une décroissance vers 0 aussi rapide que possible.
Alors S_n - u_n+1 est donc une bien meilleure valeur
approchée de S que S_n. Cette méthode fournira également des
développements asymptotiques de restes de séries car si x_n -
(u_n - u_n+1) = o(v_n), on aura
R_n(x) + u_n+1 = o(R_n(v)) et donc le
développement R_n(x) = -u_n+1 + o(R_n(v)).

En ce qui concerne les développements asymptotiques de sommes partielles
de séries divergentes, on se ramènera à la situation précédente en
rempla\ccant la série x_n par une série du
type y_n = x_n - (v_n - v_n-1) de
telle sorte que la série
\\sum  y_n~
converge. On aura alors S_n(x) = v_n - v_0 +
S_n(y) = v_n + A + R_n(y) où A = S(y) -
v_0 est une constante (sa valeur ne pourra pas être obtenue
directement par cette méthode). Il suffira ensuite d'appliquer la
méthode précédente pour obtenir un développement asymptotique de
R_n(y) à la précision souhaitée, et donc aussi un développement
asymptotique de R_n(x).

Nous allons traiter deux exemples importants des techniques ci dessus.

Exemple~7.9.3 On recherche un développement asymptotique de
\\sum ~
_k=1^n 1 \over k . Posons x_n
= 1 \over n et y_n =\
log (n) - log~ (n - 1) =
-log (1 - 1 \over n~ ). On a
z_n = x_n - y_n = 1 \over
n - log (1 - 1 \over n~ )
= - 1 \over 2n^2 + O( 1
\over n^3 ). On en déduit que la série
\\sum  z_n~
converge. On a alors

\begin{align*} \\sum
_k=1^nx_ k& =& 1 +
\sum _k=2^nz_ k~ +
\sum _k=2^ny_ k~ = 1 +
\sum _k=2^nz_ k~ +
\sum _k=2^n~(log k - log (k -
1))\%& \\ & =&
log~ n + (1 + \\sum
_k=2^+\infty~z_ k) - R_n(z)
\%&\\ \end{align*}

Mais les théorèmes de comparaison des séries à termes de signes
constants assurent que puisque z_n ∼- 1 \over
2n^2 , on a R_n(z) ∼- 1 \over 2
 \\sum ~
_k=n+1^+\infty~ 1 \over k^2 ∼- 1
\over 2n . Posons alors \gamma = 1
+ \\sum ~
_k=2^+\infty~z_k (la constante d'Euler)~; on obtient

\sum _k=1^n~ 1
\over k = log n + \gamma + 1 \over 2n +
o( 1 \over n )

(en fait il est clair que les techniques ci dessus permettent d'obtenir
un développement à un ordre arbitraire).

Exemple~7.9.4 Nous allons maintenant montrer la formule de Stirling, n!
∼\sqrt2\pi~n n^n \over
e^n . Pour cela posons a_n = n!e^n
\over n^n+1\diagup2 et b_n
= log a_n~ -\
log a_n-1 (pour n ≥ 2). On a

\begin{align*} b_n& =&
log  a_n~ \over
a_n-1 = log  n!e^n~(n
- 1)^n-1\diagup2 \over (n -
1)!e^n-1n^n+1\diagup2 \%&
\\ & =& log~
\left (e (n - 1)^n-1\diagup2 \over
n^n-1\diagup2 \right ) = 1 + (n - 1
\over 2 )log~ (1 - 1
\over n )\%& \\
\end{align*}

d'où b_n = 1 + (n - 1 \over 2 )(- 1
\over n - 1 \over 2n^n
- 1 \over 3n^3 + O( 1
\over n^4 )) = - 1 \over
12n^2 + O( 1 \over n^3 ) On
en déduit que la série \\\sum
 b_n converge. Soit S sa somme. On a alors
\\sum ~
_k=2^nb_k = S - R_n(b), mais comme
b_n ∼- 1 \over 12n^2 , on a
R_n(b) ∼- 1 \over 12
 \\sum ~
_k=n+1^+\infty~ 1 \over k^2 ∼- 1
\over 12n . On a d'autre part
\\sum ~
_k=2^nb_k = log~
a_n - log a_1~, d'où
finalement log a_n~
= \\sum ~
_k=2^nb_k + log~
a_1 = S + log a_1~ + 1
\over 12n + o( 1 \over n ) et donc
a_n = e^S+log~
a_1 exp~ ( 1 \over
12n + o( 1 \over n )) = \ell(1 + 1
\over 12n + o( 1 \over n )) en
posant \ell = e^S+log a_1~
> 0, soit encore

n! = \ell n^n+1\diagup2 \over n!
\left (1 + 1 \over 12n + o( 1
\over n )\right )

La méthode précédente ne permet pas d'obtenir la valeur de \ell~; on
obtient celle ci classiquement à l'aide des intégrales de Wallis~:
I_n =\int ~
_0^\pi~\diagup2 sin ^n~x dx.
Pour n ≥ 2, on écrit à l'aide d'une intégration par parties, en
intégrant sin~ x et en dérivant
sin ^n-1~x

\begin{align*} I_n& =&
\int ~
_0^\pi~\diagup2 sin~
^n-1xsin~ x dx \%&
\\ & =& \left
[-cos x\sin~
^n-1x\right ]_ 0^\pi~\diagup2 + (n -
1)\int ~
_0^\pi~\diagup2 sin~
^n-2xcos ^2~x dx \%&
\\ & =& (n -
1)\int ~
_0^\pi~\diagup2 sin ^n-2~x(1
- sin ^2x) dx = (n - 1)(I_
n-2 - I_n)\%& \\
\end{align*}

d'où I_n = n-1 \over n I_n-2. En
tenant compte de I_0 = \pi~ \over 2 et
I_1 = 1, on a alors

I_2p = (2p - 1)(2p -
3)\\ldots~3.1
\over (2p)(2p -
2)\\ldots4.2~  \pi~
\over 2 = (2p)! \over
2^p(p!)^2  \pi~ \over 2

en multipliant numérateur et dénominateur par (2p)(2p -
2)\\ldots~4.2 de
manière à rétablir les facteurs manquant au numérateur. De même

I_2p+1 = (2p)(2p -
2)\\ldots~4.2
\over (2p + 1)(2p -
1)\\ldots3~ =
2^p(p!)^2 \over (2p + 1)!

On en déduit en utilisant n! ∼ \ell\sqrtn
n^n \over n!

 I_2p \over I_2p+1 = (2p +
1)(2p)!^2 \over
2^4pp!^4  \pi~ \over 2 ∼ (2p +
1)\ell^2(2p)(2p)^4pe^4p
\over
2^4pe^4p\ell^4p^2p^4p
 \pi~ \over 2 ∼ 2\pi~ \over
\ell^2

Mais d'autre part, on a \forall~~x \in [0, \pi~
\over 2 ], 0 \leq sin~
^n+1x \leq sin ^n~x
\leq sin ^n-1~x, soit en intégrant 0 \leq
I_n+1 \leq I_n \leq I_n-1 et en tenant compte de 
I_n-1 \over I_n+1 = n+1
\over n , on obtient 1 \leq I_n
\over I_n+1 \leq n+1 \over n
soit encore lim I_n~
\over I_n+1 = 1. On en déduit que  2\pi~
\over \ell^2 = 1 et comme \ell > 0, \ell
= \sqrt2\pi~ ce qui achève la démonstration.

[
[
[
[

\end{document}
