\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{graphicx}
% Redefine \includegraphics so that, unless explicit options are
% given, the image width will not exceed the width of the page.
% Images get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\ScaleIfNeeded{%
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother
\let\Oldincludegraphics\includegraphics
{%
 \catcode`\@=11\relax%
 \gdef\includegraphics{\@ifnextchar[{\Oldincludegraphics}{\Oldincludegraphics[width=\ScaleIfNeeded]}}%
}%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Determinants},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.subsectionHead, .likesubsectionHead { margin-top:2em; font-weight: bold;}
.sectionHead, .likesectionHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:justify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.sectionToc, .likesectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Determinants}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

[
[
[]
[

\section{2.7 Déterminants}

\subsection{2.7.1 Formes p-linéaires}

Définition~2.7.1 On appelle forme p-linéaire sur le Kespace vectoriel E
toute application \phi : E^p \rightarrow~ K vérifiant
\forall~(a_1,\\\ldots,a_p~)
\in K^p, \forall~~i \in [1,p],
x_i\mapsto~\phi(a_1,\\ldots,a_i-1,x_i,a_i+1,\\\ldots,a_p~)
est linéaire de E dans K.

Définition~2.7.2 Soit f une forme p-linéaire sur E. On dit qu'elle est

\begin{itemize}
\item
  (i) alternée si
  \forall~(x_1\\\ldots,x_p~)
  \in E^p,

  \left (\exists~(i,j) \in
  [1,p]^2,i\neq~j\text
  et x_ i = x_j\right ) \rigtharrow~
  f(x_1,\\ldots,x_p~)
  = 0
\item
  (ii) antisymétrique si
  \forall~(x_1\\\ldots,x_p~)
  \in E^p,

  \forall~\sigma \inS_p~,
  f(x_\sigma(1),\\ldots,x_\sigma(p)~)
  =
  \epsilon(\sigma)f(x_1,\\ldots,x_p~)
\end{itemize}

Proposition~2.7.1 Toute forme alternée est antisymétrique. Si
carK\mathrel\neq~~2, toute forme
antisymétrique est alternée.

Démonstration On remarque que si f est alternée, on a

\begin{align*} 0& =&
f(\\ldots,x_i~
+
x_j,\\ldots,x_i~
+
x_j,\\ldots~)
\%& \\ & =&
f(\\ldots,x_i,\\\ldots,x_j,\\\ldots~)
+
f(\\ldots,x_j,\\\ldots,x_i,\\\ldots~)\%&
\\ \end{align*}

On a donc
f(x_\sigma(1),\\ldots,x_\sigma(p)~)
=
\epsilon(\sigma)f(x_1,\\ldots,x_p~)
lorsque \sigma est la transposition \tau_i,j. Comme les transpositions
engendrent S_p, f est antisymétrique. Inversement si f est
antisymétrique et si x_i = x_j, soit \sigma la
transposition \tau_i,j. On a alors

\begin{align*}
f(\\ldots,x_i,\\\ldotsx_j,\\\ldots~)&
=&
-f(\\ldots,x_j,\\\ldotsx_i,\\\ldots~)\%&
\\ & =&
-f(\\ldots,x_i,\\\ldotsx_j,\\\ldots~)\%&
\\ \end{align*}

soit
f(\\ldots,x_i,\\\ldotsx_j,\\\ldots~)
= 0 si carK\mathrel\neq~~2.

Définition~2.7.3 On note A_p(E) l'espace vectoriel des formes
p-linéaires alternées sur E.

Théorème~2.7.2 Toute forme alternée est nulle sur une famille liée.

Démonstration Il suffit d'écrire un terme comme combinaison linéaire des
autres et de développer. Dans tous les termes obtenus figurent deux
termes identiques et donc chaque terme est nul~: si x_k
= \\sum ~
_j\neq~k\alpha_jx_j, on a

[} \varphi (x_1,\ldots
,x_k,\ldots
,x_p)=\sum_j\ne
k\alpha _j\varphi
(x_1,\ldots,\overbracex_j^j,\ldots
,\overbracex_j^k,\ldots ,x_p)=0
]}

où l'on a indiqué au dessus de x_j l'indice de sa position.

\subsection{2.7.2 Déterminant d'une famille de vecteurs}

Définition~2.7.4 Soit E un K-espace vectoriel de dimension n, \mathcal{E} =
(e_1,\\ldots,e_n~)
une base de E de base duale \mathcal{E}^∗ =
(e_1^∗,\\ldots,e_n^∗~),
et
(x_1,\\ldots,x_n~)
une famille de vecteurs de E. On pose
\mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n~)
= \\sum ~
_\sigma\inS_n\epsilon(\sigma)\\∏
 _j=1^ne_j^∗(x_\sigma(j)).

Théorème~2.7.3 \mathrm{det}~
_\mathcal{E}\in A_n(E). L'espace vectoriel A_n(E) est de
dimension 1~: pour toute f \in A_n(E), on a f =
\lambda_f \mathrm{det}~
_\mathcal{E} avec \lambda_f =
f(e_1,\\ldots,e_n~).
L'application \mathrm{det}~
_\mathcal{E} est l'unique forme n-linéaire alternée vérifiant
f(e_1,\\ldots,e_n~)
= 1.

Démonstration \mathrm{det}~
_\mathcal{E} est clairement n-linéaire. Supposons que x_i =
x_j et écrivons S_n = A \cup \tau_i,jA où A est
l'ensemble des permutations de signature +1. On a donc, en tenant compte
de \epsilon(\tau_i,j\sigma) = -\epsilon(\sigma)

\begin{align*}
\mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n~)&
=& \\sum
_\sigma\inA\epsilon(\sigma)\∏
_k=1^ne_ k^∗(x_ \sigma(k)) \%&
\\ & -& \\sum
_\sigma\inA\epsilon(\sigma)\∏
_k=1^ne_ k^∗(x_
\tau_i,j\sigma(k))\%& \\
\end{align*}

Mais on a pour tout k \in [1,n], x_\tau_i,j\sigma(k) =
x_\sigma(k)~: c'est évident si
\sigma(k)∉\i,j\
car alors \tau_i,j\sigma(k) = \sigma(k), et si par exemple \sigma(k) = i, on a
x_\tau_i,j\sigma(k) = x_j = x_i =
x_\sigma(k). On en déduit que dans la différence précédente, les
deux sommes sont égales, et donc
\mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n~)
= 0. Ceci montre le caractère alterné de
\mathrm{det} _\mathcal{E}~.

Soit maintenant f \in A_n(E). On pose x_j
= \\sum ~
_i=1^n\xi_i,je_i. On a alors

f(x_1,\\ldots,x_n~)
= \\sum
_i_1,\ldots,i_n\in\mathbb{N}~\xi_i_1,1\\ldots\xi_i_n,nf(e_i_1,\\ldots,e_i_n~)

En fait dans cette somme on peut se limiter aux
i_1,\\ldots,i_n~
distincts, car sinon
f(e_i_1,\\ldots,e_i_n~)
= 0. Posant i_1 =
\sigma(1),\\ldots,i_n~
= \sigma(n) où \sigma est une permutation de [1,n], on obtient (compte tenu de
f(e_i_1,\\ldots,e_i_n~)
=
f(e_\sigma(1),\\ldots,e_\sigma(n)~)
=
\epsilon(\sigma)f(e_1,\\ldots,e_n~))

\begin{align*}
f(x_1,\\ldots,x_n~)&
=&
f(e_1,\\ldots,e_n~)\\sum
_\sigma\inS_n\epsilon(\sigma)\xi_\sigma(1),1\ldots\xi_\sigma(n),n~\%&
\\ & =&
f(e_1,\\ldots,e_n)f_0(x_1,\\\ldots,x_n~)
\%& \\ \end{align*}

avec une définition évidente de f_0.

Ceci montre clairement que dim A_n~(E)
\leq 1. Comme d'autre part,
\mathrm{det} _\mathcal{E}~ est
non nul (car on vérifie immédiatement que
\mathrm{det}~
_\mathcal{E}(e_1,\\ldots,e_n~)
= 1~: il y a un seul terme non nul dans la somme), c'est qu'il est bien
de dimension 1. Le reste en résulte immédiatement (ainsi que le fait que
f_0 = \mathrm{det}~
_\mathcal{E}).

Théorème~2.7.4 Soit \mathcal{E} =
(e_1,\\ldots,e_n~)
une base de E et
(x_1,\\ldots,x_n~)
une famille de E. Alors c'est une base de E si et seulement si
\mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n)\mathrel\neq~~0.

Démonstration En effet, si c'est une base X, on a
\mathrm{det} _\mathcal{E}~
= \mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n)\\mathrm{det}~
_X et comme
\mathrm{det}~
_\mathcal{E}\neq~0, on a
\mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n)\mathrel\neq~~0~;
si ce n'est pas une base, c'est que la famille est liée (son cardinal
est n) et donc \mathrm{det}~
_\mathcal{E}(x_1,\\ldots,x_n~)
= 0.

\subsection{2.7.3 Déterminant d'un endomorphisme}

Théorème~2.7.5 Soit u \in L(E). Il existe un unique scalaire noté
\mathrm{det}~ u vérifiant
\forall~f \in A_n~(E),
\forall~(x_1,\\\ldots,x_n~)
\in E^n~;

f(u(x_1),\\ldots,u(x_n~))
= \mathrm{det}~ u
f(x_1,\\ldots,x_n~)

Démonstration Soit \phi_u : A_n(E) \rightarrow~ A_n(E)
définie par
\phi_u(f)(x_1,\\ldots,x_n~)
=
f(u(x_1),\\ldots,u(x_n~)).
C'est un endomorphisme d'un espace vectoriel de dimension 1, donc une
homothétie~; on note
\mathrm{det}~ u son rapport.

Proposition~2.7.6

\begin{itemize}
\item
  (i) Soit \mathcal{E} =
  (e_1,\\ldots,e_n~)
  une base de E, alors

  \mathrm{det}~ u
  = \mathrm{det}~
  _\mathcal{E}(u(e_1),\\ldots,u(e_n~))
\item
  (ii) \mathrm{det}~
  \mathrmId = 1,
  \mathrm{det}~ \lambda~u =
  \lambda~^n \mathrm{det}~
  u, \mathrm{det}~
  ^tu = \mathrm{det}~
  u
\item
  (iii) \mathrm{det}~ v \cdot u
  = \mathrm{det}~
  v\mathrm{det}~ u
\item
  (iv) u est un automorphisme de E si et seulement si
  \mathrm{det}~
  u\neq~0.
\end{itemize}

Démonstration Tout est presque immédiat~; (iii) découle de
\phi_v\cdotu = \phi_u \cdot \phi_v et du fait que le rapport
du produit de deux homothéties est le produit des rapports.

\subsection{2.7.4 Déterminant d'une matrice}

Définition~2.7.5 Soit A \in M_k(n). On appelle déterminant de A
le déterminant de la famille de ses vecteurs colonnes dans la base
canonique. On a donc
\mathrm{det}~ A
= \\sum ~
_\sigma\inS_n\epsilon(\sigma)\\∏
 _j=1^na_j,\sigma(j).

Proposition~2.7.7

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) Soit \mathcal{E} =
  (e_1,\\ldots,e_n~)
  une base de E, u \in L(E), alors
  \mathrm{det}~ u
  = \mathrm{det}~
  \mathrmMat~ (u,\mathcal{E})
\item
  (ii) \mathrm{det}~
  I_n = 1,
  \mathrm{det}~ \lambda~A =
  \lambda~^n \mathrm{det}~
  A, \mathrm{det}~
  ^tA = \mathrm{det}~
  A
\item
  (iii) \mathrm{det}~ AB
  = \mathrm{det}~
  A\mathrm{det}~ B
\item
  (iv) A est inversible si et seulement si
  \mathrm{det}~
  A\neq~0.
\end{itemize}

Démonstration On a
\mathrm{det}~ u
= \mathrm{det}~
_\mathcal{E}(u(e_1),\\ldots,u(e_n~))
ce qui n'est autre que
\mathrm{det}~
\mathrmMat~ (u,\mathcal{E}) puisque
les vecteurs colonnes de cette matrice sont constitués des coordonnées
des u(e_j) dans la base \mathcal{E}, ce qui montre (i). La formule
\mathrm{det} ^t~A
= \mathrm{det}~ A résulte de
la remarque que nous avons faite lors de la démonstration du fait que
A_n(E) est de dimension 1~: f_0
= \mathrm{det} _\mathcal{E}~,
soit encore

\\sum
_\sigma\inS_n\epsilon(\sigma)\xi_\sigma(1),1\ldots\xi_\sigma(n),n~
= \\sum
_\sigma\inS_n\epsilon(\sigma)\xi_1,\sigma(1)\ldots\xi_n,\sigma(n)~

Tout le reste s'obtient en traduisant les résultats similaires sur les
endomorphismes.

Proposition~2.7.8 Le déterminant d'une matrice dépend linéairement de
chacun de ses vecteurs colonnes (resp lignes), le déterminant d'une
matrice ne change pas si on ajoute à une colonne (resp. ligne) une
combinaison linéaire des autres colonnes (resp. lignes). Si on effectue
une permutation \sigma sur les colonnes (resp. lignes) d'une matrice, son
déterminant est multiplié par \epsilon(\sigma). Application Calcul par la méthode du
pivot.

Démonstration Evident de par la définition du déterminant d'une matrice
comme déterminant de la famille de ses vecteurs colonnes (ou de ses
vecteurs lignes par transposition)

Théorème~2.7.9 (calcul des déterminants par blocs).
\mathrm{det}~
\left ( \includegraphics{cours3x.png}
\,\right ) = \
\mathrm{det}
A.\mathrm{det}~ C.

Démonstration Notons M = (a_i,j)_1\leqi,j\leqn =
\left
(\matrix\,A&B\cr 0
&C\right ), si bien que l'on a a_i,j = 0 si i
≥ p + 1 et j \leq p. On a alors
\mathrm{det}~ M
= \\sum ~
_\sigma\inS_n\epsilon(\sigma)\\∏
 _k=1^na_k,\sigma(k). Soit \sigma \inS_n~; s'il
existe k_0 \in [p + 1,n] tel que \sigma(k_0) \in [1,p],
on a alors a_k_0,\sigma(k_0) = 0 et donc
\∏ ~
_k=1^na_k,\sigma(k) = 0. Autrement dit, les seules
permutations qui peuvent donner une contribution non nulle au
déterminant sont les permutations \sigma \inS_n telles que \sigma([p +
1,n]) \subset~ [p + 1,n], c'est-à-dire vérifiant \sigma([p + 1,n]) = [p
+ 1,n] et donc aussi \sigma([1,p]) = [1,p]. Une telle permutation \sigma
est entièrement définie par ses restrictions \sigma_1 à [1,p] et
\sigma_2 à [p + 1,n], l'application
\sigma\mapsto~(\sigma_1,\sigma_2) étant bijective
de l'ensemble de ces permutations sur S_p \timesS_n-p.
D'autre part, on voit immédiatement que toute décomposition de
\sigma_1 et \sigma_2 en produit de transpositions, fournit une
décomposition de \sigma en produit de transpositions, ce qui montre que \epsilon(\sigma)
= \epsilon(\sigma_1)\epsilon(\sigma_2). On obtient donc~:

\begin{align*}
\mathrm{det}~ M& =&
\sum _ \sigma_1\inS_p~
\atop \sigma_2\inS_n-p
\epsilon(\sigma_1)\epsilon(\sigma_2)\∏
_k=1^pa_ k,\sigma_1(k)
∏ _k=p+1^na_
k,\sigma_2(k) \%& \\ & =&
\\sum
_\sigma_1\inS_p\epsilon(\sigma_1)\∏
_k=1^pa_ k,\sigma_1(k)
\\sum
_\sigma_2\inS_n-p\epsilon(\sigma_2)\∏
_k=p+1^na_ k,\sigma_2(k)\%&
\\ & =&
\mathrm{det}~
A.\mathrm{det}~ C \%&
\\ \end{align*}

Corollaire~2.7.10 Le déterminant d'une matrice triangulaire par blocs
est égal au produit des déterminants des blocs diagonaux. Le déterminant
d'une matrice triangulaire est égal au produit de ses éléments
diagonaux.

Démonstration Récurrence évidente.

Définition~2.7.6 Soit A = (a_i,j) \in M_K(n). On notera
A_k,l = (-1)^k+l\
\mathrm{det}
(a_i,j)_i\neq~k,j\mathrel\neq~l
(cofacteur d'indice (k,l)). La matrice (A_i,j) \in
M_K(n) est appelée la comatrice de la matrice A.

Théorème~2.7.11 (développement d'un déterminant). On a

\forall~~i \in [1,n],\quad
\mathrm{det}~ A =
\sum _k=1^na_
i,kA_i,k

(développement suivant la i-ième ligne)

\forall~~j \in [1,n],\quad
\mathrm{det}~ A =
\sum _k=1^na_
k,jA_k,j

(développement suivant la j-ième colonne)

Démonstration Par exemple sur les colonnes~; soit
(\epsilon_1,\\ldots,\epsilon_n~)
la base canonique de K^n,
(c_1,\\ldotsc_n~)
les vecteurs colonnes de la matrice A. On a c_j
= \\sum ~
_k=1^na_k,j\epsilon_k, d'où

\begin{align*}
\mathrm{det}~ A& =&
\mathrm{det}~
(c_1,\\ldots,c_n~)
\%& \\ & =& \\sum
_k=1^na_ k,j \mathrm{det}
(c_1,\ldots,c_j-1,\epsilon_k,c_j+1,\\ldots,c_n~)\%&
\\ & =& \\sum
_k=1^na_ k,j\Delta_k,j \%&
\\ \end{align*}

Par combinaisons linéaires de colonnes (pour éliminer les termes de la
k-ième ligne) puis par échange de lignes et de colonnes, on obtient

\Delta_k,l = (-1)^k+l\left
\matrix\,1&0\\ldots~0
\cr \matrix\,0
\cr \⋮~
\cr
0&(a_i,j)_i\neq~k,j\mathrel\neq~l\right
 = (-1)^k+lA_ k,l

Corollaire~2.7.12 A^t com~A =
^t com~A A =
(\mathrm{det} A)I_n~.
Si A est inversible, A^-1 = 1 \over
\mathrm{det} A~
^t com~A.

Démonstration En effet (A^t\
comA)_i,j =\ \\sum
 _k=1^na_i,kA_j,k. Mais ceci n'est
autre que le développement suivant la j-ième ligne du déterminant de la
matrice B obtenue à partir de A en rempla\ccant la
j-ième ligne par la i-ième. Si i = j, c'est donc
\mathrm{det}~ A. Si
i\neq~j, la matrice B a deux lignes identiques,
donc son déterminant est nul.

\subsection{2.7.5 Application des déterminants à la recherche du rang}

Lemme~2.7.13 Soit A = (a_i,j) \in M_K(m,n) et soit B =
(a_i,j)_i\inI,j\inJ une sous-matrice de A (avec I \subset~
[1,m] et J \subset~ [1,n]. Alors
\mathrmrg~B
\leq\mathrmrg~A.

Démonstration Soit C = (a_i,j)_i\in[1,m],j\inJ et soit
c_1,\\ldots,c_n~
les vecteurs colonnes de A. Alors on a
\mathrmrg~C
= \mathrmrg(c_j~, j
\in J)
\leq\mathrmrg(c_1,\\\ldots,c_n~)
= \mathrmrg~A. Mais soit
d'autre part
l_1',\\ldots,l_m~'
les vecteurs lignes de la matrice C. On a
\mathrmrg~B
= \mathrmrg(l_i~',
i \in I)
\leq\mathrmrg(l_1',\\\ldots,l_m~')
= \mathrmrg~C, d'où
\mathrmrg~B
\leq\mathrmrg~A.

Soit alors r le rang de A. D'après le lemme précédent, toute
sous-matrice inversible B de A a une taille (un ordre) plus petit que r.
On a alors le théorème suivant

Théorème~2.7.14 Soit A = (a_i,j) \in M_K(m,n) de rang r
et soit B = (a_i,j)_i\inI,j\inJ une sous-matrice de A
carrée inversible avec I = J
< r. Alors il existe i_0 \in [1,m] \diagdown I,j_0
\in [1,n] \diagdown J tels que la matrice B' =
(a_i,j)_i\inI\cup\i_0\,j\inJ\cup\j_0\
(matrice bordante de B dans A) soit encore inversible.

Démonstration Soit C = (a_i,j)_i\in[1,m],j\inJ et soit
c_1,\\ldots,c_n~
les vecteurs colonnes de A. On a
\mathrmrg~C
\leqJ (car C a J vecteurs colonnes)
et \mathrmrg~C
≥\mathrmrg~B =
J (car B est une sous-matrice de C). Donc
\mathrmrg~C =
J < r. Ceci montre que la famille
(c_j)_j\inJ est libre. D'autre part dans V
=\
\mathrmVect(c_1,\\ldots,c_n~),
la famille
(c_1,\\ldots,c_n~)
est génératrice. Par le théorème de la base incomplète, il existe J' tel
que J \subset~ J' \subset~ [1,n] avec (c_j)_j\inJ' base de V .
Mais J' = dim~ V = r
> J donc on peut prendre un
j_0 \in J' \diagdown J et la famille
(c_j)_j\inJ\cup\j_0\
est encore libre. Soit D =
(a_i,j)_i\in[1,m],j\inJ\cup\j_0\.
Le rang de D est donc I + 1. Soit
l_1',\\ldots,l_m~'
les vecteurs colonnes de la matrice D. La matrice
(a_i,j)_i\inI,j\inJ\cup\j_0\
est de rang I (elle a I lignes
et contient la matrice B de rang I), donc la famille
(l_i')_i\inI est de rang I alors que
la famille (l_i')_i\in[1,m] est de rang
I + 1. Le même argument à base de théorème de la
base incomplète montre que l'on peut trouver i_0 \in [1,m] \diagdown
I tel que la famille
(l_i')_i\inI\cup\i_0\
soit encore libre. La matrice B' =
(a_i,j)_i\inI\cup\i_0\,j\inJ\cup\j_0\
est donc inversible.

Remarque~2.7.1 Le théorème précédent montre donc que toute sous-matrice
inversible de taille strictement inférieure à r peut être complétée en
une autre sous-matrice inversible. On en déduit

Théorème~2.7.15 Soit A = (a_i,j) \in M_K(m,n) de rang r.
Alors A contient des sous-matrices carrées inversibles de rang r
(sous-matrices principales). Une sous-matrice carrée inversible est une
sous-matrice principale si et seulement si toutes ses matrices bordantes
sont non inversibles.

Remarque~2.7.2 Ceci permet de rechercher théoriquement le rang d'une
matrice à l'aide de déterminants, en augmentant au fur et à mesure la
taille des sous-matrices inversibles.

\subsection{2.7.6 Formes p-linéaires alternées}

Proposition~2.7.16 Soit E un K-espace vectoriel,
f_1,\\ldots,f_p~
\in E^∗. Alors f_1
∧\\ldots~ ∧
f_p : E^p \rightarrow~ K définie par
(x_1,\\ldots,x_p)\mapsto~\\mathrm{det}~
(f_i(x_j))_1\leqi\leqp,1\leqj\leqp est une forme p-
linéaire alternée sur E.

Ceci va nous permettre d'exhiber une base de A_p(E) en
utilisant les deux lemmes suivants. Pour cela soit E un K-espace
vectoriel de dimension n et \mathcal{E} =
(e_1,\\ldots,e_n~)
une base de E de base duale \mathcal{E}^∗ =
(e_1^∗,\\ldots,e_n^∗~).

Lemme~2.7.17 Soit f,g \in A_p(E) telles que pour toute famille
(i_1,\\ldots,i_p~)
vérifiant 1 \leq i_1 < i_2 <
\\ldots~ <
i_p \leq n, on ait
f(e_i_1,\\ldots,e_i_p~)
=
g(e_i_1,\\ldots,e_i_p~).
Alors f = g.

Démonstration La relation
f(e_i_1,\\ldots,e_i_p~)
=
g(e_i_1,\\ldots,e_i_p~)
reste encore vraie si
i_1,\\ldots,i_p~
sont distincts mais non ordonnés (il suffit de les réordonner par une
permutation \sigma, ce qui ne fait que multiplier les deux côtés par \epsilon(\sigma)).
Elle est triviale si
i_1,\\ldots,i_p~
ne sont pas distincts car alors les deux membres valent 0. Mais alors,
on a en posant x_j =\
\sum ~
_i=1^n\xi_i,je_i

f(x_1,\\ldots,x_p~)
= \\sum
_i_1,\ldots,i_p\in\mathbb{N}~\xi_i_1,1\\ldots\xi_i_p,pf(e_i_1,\\ldots,e_i_p~)

et la même chose pour g. Donc f = g.

Lemme~2.7.18 Soit 1 \leq i_1 < i_2 <
\\ldots~ <
i_p \leq n et 1 \leq j_1 < j_2
< \\ldots~
< j_p \leq n. Alors

e_i_1^∗∧\\ldots~
∧ e_ i_p^∗(e_
j_1,\\ldots,e_j_p~)
=
\delta_i_1,\\ldots,i_p^j_1,\\\ldots,j_p~


(symbole de Kronecker)

Démonstration Il est clair que
e_i_1^∗∧\\ldots~
∧
e_i_1^∗(e_i_1,\\ldots,e_i_p~)
= 1 (la matrice ''f_i(x_j)'' est l'identité).
Supposons donc que i_1 =
j_1,\\ldots,i_k-1~
= j_k-1,i_k\neq~j_k.
Si i_k < j_k, on a pour tout l \in
[1,p],i_k\neq~j_l soit
e_j_l^∗(e_i_k) = 0. La
matrice ''f_i(x_j)'' a donc sa k-ième ligne nulle et
son déterminant est donc nul. Si j_k < i_k,
de manière similaire, la k-ième colonne de la matrice est nulle. Dans
les deux cas, on trouve donc 0 comme résultat.

Théorème~2.7.19 La famille des
(e_i_1^∗∧\\ldots~
∧
e_i_p^∗)_1\leqi_1<i_2<\\ldots<i_p\leqn~
est une base de A_p(E) (qui est donc de dimension
C_n^p).

Démonstration Montrons que la famille est génératrice. Soit f \in
A_p(E) et

g = \\sum ~
_1\leqj_1<j_2<\\ldots<j_p\leqnf(e_j_1,\\\ldots,e_j_p)e_j_1^∗∧\\\ldots~
∧ e_j_p^∗. Grâce au lemme 2, si 1 \leq
i_1 < i_2 <
\\ldots~ <
i_p \leq n, on a
g(e_i_1,\\ldots,e_i_p~)
=
f(e_i_1,\\ldots,e_i_p~).
D'après le lemme 1, on a f = g. Il reste à montrer que la famille est
libre. Supposons que \\\sum

_1\leqj_1<j_2<\\ldots<j_p\leqn\lambda_j_1,\\\ldots,j_pe_j_1^∗∧\\\ldots~
∧ e_j_p^∗ = 0. Grâce au lemme 2, si 1 \leq
i_1 < i_2 <
\\ldots~ <
i_p \leq n on a

\begin{align*} 0& =&
0(e_i_1,\\ldots,e_i_p~)
\%& \\ & =& \\sum
_1\leqj_1<j_2<\ldots<j_p\leqn\lambda_j_1,\\ldots,j_pe_j_1^∗∧\\ldots~
∧ e_ j_p^∗(e_
i_1,\ldots,e_i_p~)
=
\lambda_i_1,\ldots,i_p~\%&
\\ \end{align*}

ce qui montre que la famille est libre.

[
[
[
[

\end{document}
