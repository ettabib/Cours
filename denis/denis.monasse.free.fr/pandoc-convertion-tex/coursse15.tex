\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{graphicx}
% Redefine \includegraphics so that, unless explicit options are
% given, the image width will not exceed the width of the page.
% Images get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\ScaleIfNeeded{%
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother
\let\Oldincludegraphics\includegraphics
{%
 \catcode`\@=11\relax%
 \gdef\includegraphics{\@ifnextchar[{\Oldincludegraphics}{\Oldincludegraphics[width=\ScaleIfNeeded]}}%
}%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Valeurs propres. Vecteurs propres},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
 
/* start css.sty */
.cmr-5{font-size:50%;}
.cmr-7{font-size:70%;}
.cmmi-5{font-size:50%;font-style: italic;}
.cmmi-7{font-size:70%;font-style: italic;}
.cmmi-10{font-style: italic;}
.cmsy-5{font-size:50%;}
.cmsy-7{font-size:70%;}
.cmex-7{font-size:70%;}
.cmex-7x-x-71{font-size:49%;}
.msbm-7{font-size:70%;}
.cmtt-10{font-family: monospace;}
.cmti-10{ font-style: italic;}
.cmbx-10{ font-weight: bold;}
.cmr-17x-x-120{font-size:204%;}
.cmsl-10{font-style: oblique;}
.cmti-7x-x-71{font-size:49%; font-style: italic;}
.cmbxti-10{ font-weight: bold; font-style: italic;}
p.noindent { text-indent: 0em }
td p.noindent { text-indent: 0em; margin-top:0em; }
p.nopar { text-indent: 0em; }
p.indent{ text-indent: 1.5em }
@media print {div.crosslinks {visibility:hidden;}}
a img { border-top: 0; border-left: 0; border-right: 0; }
center { margin-top:1em; margin-bottom:1em; }
td center { margin-top:0em; margin-bottom:0em; }
.Canvas { position:relative; }
li p.indent { text-indent: 0em }
.enumerate1 {list-style-type:decimal;}
.enumerate2 {list-style-type:lower-alpha;}
.enumerate3 {list-style-type:lower-roman;}
.enumerate4 {list-style-type:upper-alpha;}
div.newtheorem { margin-bottom: 2em; margin-top: 2em;}
.obeylines-h,.obeylines-v {white-space: nowrap; }
div.obeylines-v p { margin-top:0; margin-bottom:0; }
.overline{ text-decoration:overline; }
.overline img{ border-top: 1px solid black; }
td.displaylines {text-align:center; white-space:nowrap;}
.centerline {text-align:center;}
.rightline {text-align:right;}
div.verbatim {font-family: monospace; white-space: nowrap; text-align:left; clear:both; }
.fbox {padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.fbox {display:table}
div.center div.fbox {text-align:center; clear:both; padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
div.minipage{width:100%;}
div.center, div.center div.center {text-align: center; margin-left:1em; margin-right:1em;}
div.center div {text-align: left;}
div.flushright, div.flushright div.flushright {text-align: right;}
div.flushright div {text-align: left;}
div.flushleft {text-align: left;}
.underline{ text-decoration:underline; }
.underline img{ border-bottom: 1px solid black; margin-bottom:1pt; }
.framebox-c, .framebox-l, .framebox-r { padding-left:3.0pt; padding-right:3.0pt; text-indent:0pt; border:solid black 0.4pt; }
.framebox-c {text-align:center;}
.framebox-l {text-align:left;}
.framebox-r {text-align:right;}
span.thank-mark{ vertical-align: super }
span.footnote-mark sup.textsuperscript, span.footnote-mark a sup.textsuperscript{ font-size:80%; }
div.tabular, div.center div.tabular {text-align: center; margin-top:0.5em; margin-bottom:0.5em; }
table.tabular td p{margin-top:0em;}
table.tabular {margin-left: auto; margin-right: auto;}
div.td00{ margin-left:0pt; margin-right:0pt; }
div.td01{ margin-left:0pt; margin-right:5pt; }
div.td10{ margin-left:5pt; margin-right:0pt; }
div.td11{ margin-left:5pt; margin-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
td.td00{ padding-left:0pt; padding-right:0pt; }
td.td01{ padding-left:0pt; padding-right:5pt; }
td.td10{ padding-left:5pt; padding-right:0pt; }
td.td11{ padding-left:5pt; padding-right:5pt; }
table[rules] {border-left:solid black 0.4pt; border-right:solid black 0.4pt; }
.hline hr, .cline hr{ height : 1px; margin:0px; }
.tabbing-right {text-align:right;}
span.TEX {letter-spacing: -0.125em; }
span.TEX span.E{ position:relative;top:0.5ex;left:-0.0417em;}
a span.TEX span.E {text-decoration: none; }
span.LATEX span.A{ position:relative; top:-0.5ex; left:-0.4em; font-size:85%;}
span.LATEX span.TEX{ position:relative; left: -0.4em; }
div.float img, div.float .caption {text-align:center;}
div.figure img, div.figure .caption {text-align:center;}
.marginpar {width:20%; float:right; text-align:left; margin-left:auto; margin-top:0.5em; font-size:85%; text-decoration:underline;}
.marginpar p{margin-top:0.4em; margin-bottom:0.4em;}
.equation td{text-align:center; vertical-align:middle; }
td.eq-no{ width:5%; }
table.equation { width:100%; } 
div.math-display, div.par-math-display{text-align:center;}
math .texttt { font-family: monospace; }
math .textit { font-style: italic; }
math .textsl { font-style: oblique; }
math .textsf { font-family: sans-serif; }
math .textbf { font-weight: bold; }
.partToc a, .partToc, .likepartToc a, .likepartToc {line-height: 200%; font-weight:bold; font-size:110%;}
.chapterToc a, .chapterToc, .likechapterToc a, .likechapterToc, .appendixToc a, .appendixToc {line-height: 200%; font-weight:bold;}
.index-item, .index-subitem, .index-subsubitem {display:block}
.caption td.id{font-weight: bold; white-space: nowrap; }
table.caption {text-align:center;}
h1.partHead{text-align: center}
p.bibitem { text-indent: -2em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
p.bibitem-p { text-indent: 0em; margin-left: 2em; margin-top:0.6em; margin-bottom:0.6em; }
.subsectionHead, .likesubsectionHead { margin-top:2em; font-weight: bold;}
.sectionHead, .likesectionHead { font-weight: bold;}
.quote {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; margin-right:1em; text-align:justify;}
.verse{white-space:nowrap; margin-left:2em}
div.maketitle {text-align:center;}
h2.titleHead{text-align:center;}
div.maketitle{ margin-bottom: 2em; }
div.author, div.date {text-align:center;}
div.thanks{text-align:left; margin-left:10%; font-size:85%; font-style:italic; }
div.author{white-space: nowrap;}
.quotation {margin-bottom:0.25em; margin-top:0.25em; margin-left:1em; }
h1.partHead{text-align: center}
.sectionToc, .likesectionToc {margin-left:2em;}
.subsectionToc, .likesubsectionToc {margin-left:4em;}
.sectionToc, .likesectionToc {margin-left:6em;}
.frenchb-nbsp{font-size:75%;}
.frenchb-thinspace{font-size:75%;}
.figure img.graphics {margin-left:10%;}
/* end css.sty */

\title{Valeurs propres. Vecteurs propres}
\author{}
\date{}

\begin{document}
\maketitle

\textbf{Warning: 
requires JavaScript to process the mathematics on this page.\\ If your
browser supports JavaScript, be sure it is enabled.}

\begin{center}\rule{3in}{0.4pt}\end{center}

[
[]
[

\section{3.1 Valeurs propres. Vecteurs propres}

\subsection{3.1.1 Sous-espaces stables}

Définition~3.1.1 Soit E un K-espace vectoriel , u \in L(E). On dit qu'un
sous-espace F de E est stable si u(F) \subset~ F.

Remarque~3.1.1 Dans ce cas on peut considérer l'application (évidemment
linéaire) v : F \rightarrow~ F, x\mapsto~u(x). C'est un
endomorphisme de F appelé l'endomorphisme induit par u.

Proposition~3.1.1 Soit E un K-espace vectoriel de dimension finie, F un
sous-espace de E,
(e_1,\\ldots,e_p~)
une base de F complétée en une base \mathcal{E} =
(e_1,\\ldots,e_n~)
de E. Soit u \in L(E). Alors F est stable par u si et seulement si la
matrice de u dans la base \mathcal{E} est de la forme \left (
\includegraphics{cours4x.png} \,\right ).

Démonstration En effet F est stable par u si et seulement si
\forall~j \in [1,p], u(e_j~)
\in\mathrmVect(e_1,\\\ldots,e_p~),
ce que traduit exactement la forme de la matrice.

Remarque~3.1.2 Dans ce cas la matrice A n'est autre que la matrice dans
la base
(e_1,\\ldots,e_p~)
de l'endomorphisme v de F induit par u.

Proposition~3.1.2 Soit E un K-espace vectoriel de dimension finie,
E_1,\\ldots,E_p~
une famille de sous-espaces vectoriels de E tels que E = E_1
\oplus~⋯ \oplus~ E_p, soit \mathcal{E} une base de E
adaptée à cette décomposition en somme directe. Alors chacun des
E_i est stable par u si et seulement si la matrice de u dans la
base \mathcal{E} est de la forme

\left
(\matrix\,A_1& &0
\cr &⋱& \cr 0
& &A_p\right )

Démonstration La même~; la forme de la matrice traduit exactement que

\forall~i \in [1,p], u(\mathcal{E}_i~)
\subset~\mathrmVect(\mathcal{E}_i~)
= E_i

où l'on désigne par \mathcal{E}_i la base de E_i extraite de \mathcal{E}.

Définition~3.1.2 Soit E un K-espace vectoriel de dimension finie n~; on
appelle drapeau de E une suite \0\ =
E_0 \subset~ E_1 \subset~⋯ \subset~ E_n
= E de sous-espaces de E tels que dim~
E_i = i.

Proposition~3.1.3 Soit E un K-espace vectoriel de dimension finie n,
\0\ = E_0 \subset~ E_1
\subset~⋯ \subset~ E_n = E un drapeau de E et \mathcal{E} =
(e_1,\\ldots,e_n~)
une base de E adaptée à ce drapeau (c'est-à-dire que pour tout i \in
[1,n],
(e_1,\\ldots,e_i~)
est une base de E_i). Soit u \in L(E). Alors on a équivalence
de~:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \forall~i \in [1,n], u(E_i~) \subset~
  E_i
\item
  la matrice de u dans la base \mathcal{E} est triangulaire supérieure.
\end{itemize}

Démonstration En effet, on a évidemment au vu des inclusions
E_i-1 \subset~ E_i

\begin{align*} \forall~~i \in
[1,n], u(E_i) \subset~ E_i&& \%&
\\ & \Leftrightarrow &
\forall~i \in [1,p], u(e_i) \in E_i~
=\
\mathrmVect(e_1,\\ldots,e_i~)\%&
\\ \end{align*}

ce que traduit exactement la forme de la matrice.

\subsection{3.1.2 Valeurs propres, vecteurs propres}

Définition~3.1.3 Soit E un K-espace vectoriel et u \in L(E). On dit que \lambda~
\in K est valeur propre de u s'il existe x \in E,
x\neq~0 tel que u(x) = \lambda~x. On dit alors que x est
un vecteur propre de u associé à la valeur propre \lambda~. L'ensemble des
valeurs propres de u est appelé le spectre de u et noté
\mathrm{Sp}~(u).

Remarque~3.1.3 On a u(x) = \lambda~x \Leftrightarrow (u -
\lambda~\mathrmId_E)(x) = 0. On en déduit que \lambda~ est
valeur propre de u si et seulement si u -
\lambda~\mathrmId_E est non injectif. Ceci amène
aussi à la définition suivante

Définition~3.1.4 Soit \lambda~
\in\mathrm{Sp}~(u). On appelle
sous-espace propre associé à \lambda~ le sous espace vectoriel E_u(\lambda~)
= \mathrmKer~(u -
\lambda~\mathrmId_E) (composé des vecteurs propres
associés à \lambda~ et du vecteur nul).

Remarque~3.1.4 On remarque bien entendu qu'un vecteur propre est associé
à une seule valeur propre (soit E_u(\lambda~) \bigcap E_u(\mu) =
\0\). En fait ce résultat peut être
précisé à l'aide du théorème essentiel suivant

Théorème~3.1.4 Soit E un K-espace vectoriel et u \in L(E). Soit
\lambda_1,\\ldots,\lambda_k~
des valeurs propres distinctes de u. Alors les sous-espaces
E_u(\lambda_i) sont en somme directe.

Démonstration On va démontrer par récurrence sur n que x_1 +
\\ldots~ +
x_n = 0 \rigtharrow~\forall~i, x_i~ = 0 si
x_i \in E_u(\lambda_i). C'est vrai pour n = 1. On
suppose le résultat vrai pour n - 1 et soit x_1 +
\\ldots~ +
x_n = 0. Appliquant u on obtient

\begin{align*} 0& =& u(x_1) +
\\ldots~ +
u(x_n) = \lambda_1x_1 +
\\ldots~ +
\lambda_nx_n\%& \\ & =&
\lambda_1x_1 +
\\ldots~ +
\lambda_nx_n - \lambda_n(x_1 +
\\ldots~ +
x_n) \%& \\ & =& (\lambda_1
- \lambda_n)x_1 +
\\ldots~ +
(\lambda_n-1 - \lambda_n)x_n-1 \%&
\\ \end{align*}

L'hypothèse de récurrence implique que \forall~~i \in
[1,n - 1], (\lambda_i - \lambda_n)x_i = 0 soit
x_i = 0 (car
\lambda_i\neq~\lambda_n). La relation de
départ donne en plus x_n = 0.

On en déduit immédiatement

Corollaire~3.1.5 Soit (x_i)_i\inI une famille de
vecteurs propres de u associés à des valeurs propres \lambda_i deux à
deux distinctes. Alors la famille est libre.

Exemple~3.1.1 La famille d'applications C^\infty~, f_\lambda~ : \mathbb{R}~
\rightarrow~ \mathbb{C}, t\mapsto~e^\lambda~t est composée de
vecteurs propres de l'opérateur de dérivation (dans l'espace vectoriel
des fonctions C^\infty~ de \mathbb{R}~ dans \mathbb{C})~: Df_\lambda~ =
\lambda~f_\lambda~. On en déduit qu'elle est libre.

Enfin le résultat suivant est souvent fort utile

Proposition~3.1.6 Soit u et v deux endomorphismes de E tels que u \cdot v =
v \cdot u. Alors tout sous-espace propre de u est stable par v.

Démonstration Si u(x) = \lambda~x, alors u(v(x)) = v(u(x)) = \lambda~v(x), donc v(x) \in
E_u(\lambda~).

\subsection{3.1.3 Polynôme caractéristique}

Remarque~3.1.5 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). On a vu que \lambda~ est valeur propre de u si et seulement si
\lambda~\mathrmId_E - u est non injectif, ce qui en
dimension finie signifie que
\mathrm{det}~
(\lambda~\mathrmId_E - u) = 0. On va donc
introduire un polynôme \chi_u(X) tel que
\forall~\lambda~ \in K,\chi_u~(\lambda~)
= \mathrm{det}~
(\lambda~\mathrmId_E - u).

Définition~3.1.5 Soit M \in M_K(n). On appelle polynôme
caractéristique de la matrice M le déterminant \chi_M(X) de la
matrice XI_n - M \in M_K[X](n).

Proposition~3.1.7

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) si M et M' sont deux matrices semblables, alors \chi_M' =
  \chi_M
\item
  (ii) \chi_^tM = \chi_M
\item
  (iii) \chi_M(X) = X^n
  -\mathrm{tr}(M)X^n-1~
  + \\ldots~ +
  (-1)^n\
  \mathrm{det} M
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) Si M' = P^-1MP, alors XI_n - M' =
  XI_n - P^-1MP = P^-1(XI_n -
  M)P et donc \mathrm{det}~
  (XI_n - M) =\
  \mathrm{det} (XI_n - M').
\item
  (ii) découle de la même fa\ccon de
  ^t(XI_n - M) = XI_n -^tM
\item
  (iii) Le coefficient du terme constant est \chi_M(0)
  = \mathrm{det}~ (-M) =
  (-1)^n\
  \mathrm{det} M. Pour les coefficients de plus haut
  degré, on écrit \chi_M(X) =\
  \sum ~
  _\sigma\inS_n\epsilon(\sigma)\\∏
   _i=1^n(\delta_i^\sigma(i)X -
  a_i,\sigma(i)). Or le degré de
  \∏ ~
  _i=1^n(\delta_i^\sigma(i)X - a_i,\sigma(i))
  est le nombre de points fixes de \sigma, c'est-à-dire soit n pour \sigma =
  \mathrmId, soit inférieur ou égal à n - 2. Donc
  \chi_M(X) =\ \∏
   _i=1^n(X - a_i,i) + R(X) avec
  deg~ R \leq n - 2. Le résultat en découle
  immédiatement.
\end{itemize}

Remarque~3.1.6 La partie (i) nous montre que si u \in L(E) et si \mathcal{E} est une
base de E, le polynôme caractéristique de la matrice
\mathrmMat~ (u,\mathcal{E}) est
indépendant du choix de \mathcal{E}.

Définition~3.1.6 Soit u \in L(E) où dim~ E
< +\infty~. On appelle polynôme caractéristique de u le polynôme
caractéristique de sa matrice dans n'importe quelle base de E.

Proposition~3.1.8

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) \chi_u(X) = X^n
  -\mathrm{tr}(u)X^n-1~
  + \\ldots~ +
  (-1)^n\
  \mathrm{det} u
\item
  (ii) \chi_^tu = \chi_u
\item
  (iii) \lambda~ \in\mathrm{Sp}~(u)
  \Leftrightarrow \chi_u(\lambda~) = 0
\end{itemize}

Définition~3.1.7 Soit \lambda~ une valeur propre de u. On appelle multiplicité
de \lambda~ le nombre m_u(\lambda~) égal à la multiplicité de \lambda~ comme racine
de \chi_u.

Lemme~3.1.9 Soit u \in L(E) et F un sous-espace vectoriel de E stable par
u. Soit u' : F \rightarrow~ F défini par u'(x) = u(x) pour x \in F. Alors
\chi_u'(X) divise \chi_u(X).

Démonstration Soit ℱ =
(e_1,\\ldots,e_p~)
une base de F que l'on complète en \mathcal{E} =
(e_1,\\ldots,e_n~)
base de E. Alors M =\
\mathrmMat (u,\mathcal{E}) = \left
(\matrix\,A&B\cr 0
&C\right ) où A =\
\mathrmMat (u',ℱ). On a alors par un calcul de
déterminants par blocs \chi_M(X) = \chi_A(X)\chi_C(X)
ce qui montre que \chi_u'(X) = \chi_A(X) divise
\chi_u(X) = \chi_M(X).

Théorème~3.1.10 Soit u \in L(E), \lambda~
\in\mathrm{Sp}~(u),
m_u(\lambda~) la multiplicité de la valeur propre \lambda~ et E_u(\lambda~)
le sous-espace propre associé à \lambda~. Alors 1 \leq\
dim E_u(\lambda~) \leq m_u(\lambda~).

Démonstration E_u(\lambda~) est stable par u et la restriction u' de u
à E_u(\lambda~) est l'homothétie de rapport \lambda~ dont le polynôme
caractéristique est \chi_u'(X) = (X -
\lambda~)^dim E_u(\lambda~)~. Le lemme
précédent implique donc que dim~
E_u(\lambda~) \leq m_u(\lambda~). De plus
E_u(\lambda~)\neq~\0\,
donc 1 \leq dim E_u~(\lambda~).

Remarque~3.1.7 On a donc m_u(\lambda~) = 1 \rigtharrow~\
dim E_u(\lambda~) = 1.

\subsection{3.1.4 Endomorphismes diagonalisables}

Définition~3.1.8 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). On dit que u est diagonalisable s'il vérifie les conditions
équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) il existe une base \mathcal{E} de E telle que
  \mathrmMat~ (u,\mathcal{E}) soit
  diagonale
\item
  (ii) il existe une base \mathcal{E} de E formée de vecteurs propres de u
\item
  (iii) E est somme (directe) des sous-espaces propres de u
\end{itemize}

Démonstration (i) et (ii) sont évidemment équivalents. Réunissant des
bases des sous-espaces propres de u, on a bien évidemment (iii) \rigtharrow~(ii).
Supposons maintenant que (i) est vrai. Quitte à permuter la base, on
peut supposer que
\mathrmMat~ (u,\mathcal{E})
=\
\mathrmdiag(\lambda_1,\\ldots,\lambda_1,\lambda_2,\\\ldots,\lambda_2,\\\ldots,\\\ldots,\lambda_k,\\\ldots,\lambda_k~)
avec
\lambda_1,\\ldots,\lambda_k~
deux à deux distincts, \lambda_i figurant m_i fois. On a
alors dim E_u(\lambda_i~) ≥
m_i (on a m_i vecteurs de base dans cet espace), soit

dim~ \\oplus~
_\lambda~\in\mathrm{Sp}(u)E_u(\lambda~) =
\sum _i=1^k dim E_
u(\lambda_i) ≥\\sum
_i=1^km_ i = dim E

et donc E = \\oplus~ ~
_\lambda~\in\mathrm{Sp}(u)E_u~(\lambda~).
Donc (i) \rigtharrow~(iii).

Théorème~3.1.11 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors les conditions suivantes sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) u est diagonalisable
\item
  (ii) \chi_u(X) est scindé sur K et pour toute valeur propre \lambda~ de
  u la dimension du sous-espace propre associé est égale à la
  multiplicité de la valeur propre.
\end{itemize}

Démonstration Supposons u diagonalisable et soit \mathcal{E} une base de E telle
que \mathrmMat~ (u,\mathcal{E}) = D
=\
\mathrmdiag(\lambda_1,\\ldots,\lambda_1,\lambda_2,\\\ldots,\lambda_2,\\\ldots,\\\ldots,\lambda_k,\\\ldots,\lambda_k~)
avec
\lambda_1,\\ldots,\lambda_k~
deux à deux distincts, \lambda_i figurant m_i fois. Alors
\chi_u(X) = \chi_D(X) =\
∏  _i=1^k~(X -
\lambda_i)^m_i ce qui montre déjà que \chi_u
est scindé et que les valeurs propres de u sont exactement
\lambda_1,\\ldots,\lambda_k~.
De plus dim E_u(\lambda_i~) ≥
m_i = m_u(\lambda_i) puisque
E_u(\lambda_i) contient une famille libre de cardinal
m_i. On a donc dim~
E_u(\lambda_i) = m_u(\lambda_i), soit (i) \rigtharrow~(ii).
Inversement supposons (ii) vérifié. On a alors

\begin{align*} dim~
\oplus~ _i=1^kE_
u(\lambda_i)& =& \\sum
_i=1^km_ u(\lambda_i) = deg
\chi_u(X)\%& \\ & =&
dim~ E \%& \\
\end{align*}

puisque le polynôme est scindé. Soit E =\
\oplus~ ~
_i=1^kE_u(\lambda_i).

Corollaire~3.1.12 Soit E un K-espace vectoriel de dimension finie et u \in
L(E) tel que \chi_u soit scindé à racines simples. Alors u est
diagonalisable.

Remarque~3.1.8 Pratique de la diagonalisation

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) calculer le polynôme caractéristique et en chercher les racines
  avec leurs multiplicités
\item
  (ii) pour chaque racine déterminer le sous-espace propre
  correspondant, défini par l'équation (u -
  \lambda~\mathrmId)(x) = 0~; comparer dimension du
  sous-espace propre et multiplicité de la valeur propre
\item
  (iii) déterminer une base de chaque sous-espace propre et les réunir
  en une base de E.
\end{itemize}

\subsection{3.1.5 Matrices diagonalisables}

Définition~3.1.9 Soit M \in M_K(n). On définit de manière
évidente les valeurs propres et vecteurs propres de M~: MX = \lambda~X avec
X\neq~0.

Définition~3.1.10 Soit M \in M_K(n). On dit que M est
diagonalisable si elle vérifie les conditions équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est la matrice d'un endomorphisme diagonalisable dans une
  certaine base
\item
  (ii) M est semblable à une matrice diagonale
\item
  (iii) il existe une base de K^n ∼ M_K(n,1) formée
  de vecteurs propres de M
\item
  (iii) K^n ∼ M_K(n,1) est somme directe des
  sous-espaces propres de M
\end{itemize}

Démonstration Tout ceci est élémentaire.

On a immédiatement

Théorème~3.1.13 Soit M \in M_K(n). Alors les conditions suivantes
sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est diagonalisable
\item
  (ii) \chi_M(X) est scindé sur K et pour toute valeur propre \lambda~ de
  M la dimension du sous-espace propre associé est égale à la
  multiplicité de la valeur propre.
\end{itemize}

Corollaire~3.1.14 Soit M \in M_K(n) telle que \chi_M soit
scindé à racines simples. Alors M est diagonalisable.

Remarque~3.1.9 Pratique de la diagonalisation

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) calculer le polynôme caractéristique et en chercher les racines
\item
  (ii) pour chaque racine déterminer le sous-espace propre
  correspondant, défini par l'équation (M - \lambda~I_n)X = 0~; ceci
  conduit à un système homogène de rang r_M(\lambda~)~; on a
  dim E_M(\lambda~) = n - r_M~(\lambda~)~;
  comparer dimension du sous-espace propre et multiplicité de la valeur
  propre
\item
  (iii) déterminer une base de chaque sous-espace propre~; soit P la
  matrice qui admet ces vecteurs propres comme vecteurs colonnes~; alors
  P^-1MP est diagonale.
\end{itemize}

\subsection{3.1.6 Endomorphismes et matrices trigonalisables}

Définition~3.1.11 Soit E un K-espace vectoriel de dimension finie. On
dit que u est trigonalisable s'il vérifie les conditions équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) il existe une base \mathcal{E} de E telle que
  \mathrmMat~ (u,\mathcal{E}) soit
  triangulaire (supérieure)
\item
  (ii) il existe une base \mathcal{E} de E telle que \forall~~i,
  u(e_i)
  \in\mathrmVect(e_1,\\\ldots,e_i~)
\item
  (iii) il existe une suite \0\ =
  F_0 \subset~ F_1 \subset~⋯ \subset~
  F_n = E de sous-espaces de E tels que
  dim F_i = i et u(F_i~) \subset~
  F_i.
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) et (ii) sont trivialement équivalents
\item
  (i) \rigtharrow~(iii)~: prendre F_i =\
  \mathrmVect(e_1,\\ldots,e_i~)
\item
  (iii) \rigtharrow~(i)~: construire par applications successives du théorème de la
  base incomplète une base
  (e_1,\\ldots,e_n~)
  telle que F_i =\
  \mathrmVect(e_1,\\ldots,e_i~).
\end{itemize}

Théorème~3.1.15 Soit E un K-espace vectoriel de dimension finie et u \in
L(E). Alors les conditions suivantes sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) u est trigonalisable
\item
  (ii) \chi_u(X) est scindé sur K (ce qui est automatiquement
  vérifié si K est algébriquement clos)
\end{itemize}

Démonstration

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) \rigtharrow~(ii)~: si M =\
  \mathrmMat (u,\mathcal{E}) = \left
  (\matrix\,a_1,1&\\ldots~
  &\\ldots&\\\ldots~
  \cr 0
  &a_2,2&\\ldots&\\\ldots~
  \cr &
  &⋱&\\ldots~
  \cr 0
  &\\ldots~
  &0&a_n,n\right ), on a \chi_u(X) =
  \chi_M(X) =\ \∏
   _i=1^n(X - a_i,i). Donc \chi_u est
  scindé.
\item
  (ii) \rigtharrow~ (i). Par récurrence sur n~; il n'y a rien à démontrer si n = 1.
  Supposons \chi_u scindé, et soit \lambda~ une racine de \chi_u.
  Soit e_1 un vecteur propre associé à \lambda~, que l'on complète en
  (e_1,\\ldots,e_n~)
  base de E. Soit F =\
  \mathrmVect(e_2,\\ldots,e_n~),
  p la projection sur F parallèlement à Ke_1 et v : F \rightarrow~ F
  défini par v(x) = p(u(x)) si x \in F. Alors M =\
  \mathrmMat (u,\mathcal{E}) = \left
  (\matrix\,\lambda~&∗∗∗ \cr
  \matrix\,0 \cr
  \⋮~
  \cr 0&A \right ) avec A
  = \mathrmMat~
  (v,(e_2,\\ldots,e_n~)).
  On en déduit que \chi_u(X) = (X - \lambda~)\chi_v(X). Donc
  \chi_v est aussi scindé. Par hypothèse de récurrence, il existe
  une base
  (\epsilon_2,\\ldots,\epsilon_n~)
  de F telle que \mathrmMat~
  (v,(\epsilon_2,\\ldots,\epsilon_n~))
  soit triangulaire supérieure et alors
  \mathrmMat~
  (u,(e_1,\epsilon_2,\\ldots,\epsilon_n~))
  = \left (\matrix\,\lambda~&∗
  \cr \matrix\,0
  \cr
  \⋮~
  \cr
  0&\mathrmMat~
  (v,(\epsilon_2,\\ldots,\epsilon_n))~\right
  ) est triangulaire supérieure.
\end{itemize}

Remarque~3.1.10 Comme pour la diagonalisation, ces notions passent
immédiatement aux matrices

Définition~3.1.12 Soit M \in M_K(n). On dit que M est
trigonalisable si elle vérifie les conditions équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est la matrice d'un endomorphisme trigonalisable dans une
  certaine base
\item
  (ii) M est semblable à une matrice triangulaire (supérieure).
\end{itemize}

Théorème~3.1.16 Soit M \in M_K(n). Alors les conditions suivantes
sont équivalentes

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  (i) M est trigonalisable
\item
  (ii) \chi_M(X) est scindé sur K (ce qui est automatiquement
  vérifié si K est algébriquement clos)
\end{itemize}

Corollaire~3.1.17 L'ensemble des matrices diagonalisables est dense dans
M_\mathbb{C}(n).

Démonstration Soit M \in M_\mathbb{C}(n) et P inversible telle que

P^-1MP = T = \left
(\matrix\,a_1,1&\\ldots~
&\\ldots&\\\ldots~
\cr 0
&a_2,2&\\ldots&\\\ldots~
\cr &
&⋱&\\ldots~
\cr 0
&\\ldots~
&0&a_n,n\right )

et posons pour p \in \mathbb{N}~,

T_p = \left
(\matrix\,a_1,1 + 1
\over p
&\\ldots~
&\\ldots&\\\ldots~
\cr 0 &a_2,2 + 1 \over
p^2
&\\ldots&\\\ldots~
\cr &
&⋱&\\ldots~
\cr 0
&\\ldots~
&0&a_n,n + 1 \over p^n
\right )

Il n'y a qu'un nombre fini de p pour lesquels on peut avoir
a_i,i + 1 \over p^i =
a_j,j + 1 \over p^j (il s'agit en
effet d'une équation polynomiale en  1 \over p ). On
en déduit que pour tous les p sauf en nombre fini, T_p a un
polynôme caractéristique scindé à racines simples, donc est
diagonalisable. Il en est donc de même de M_p =
PT_pP^-1. Or
lim_p\rightarrow~+\infty~M_p~ =
PTP^-1 = M. Donc M est limite d'une suite de matrices
diagonalisables.

[
[

\end{document}
