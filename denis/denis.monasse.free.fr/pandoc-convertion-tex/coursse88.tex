Voici le texte avec les environnements demandés :
\section{Equations différentielles linéaires d'ordre 1}
\subsection{Généralités}
Soit $E$ un espace vectoriel normé de dimension finie, $I$ un intervalle de
$\mathbb{R}$, $\ell : I \rightarrow L(E)$ continue et $g : I \rightarrow E$ continue. On considère l'équation
différentielle linéaire d'ordre 1 vectorielle $y' = \ell(t).y + g(t)$ ; une
solution est un couple $(J,\phi)$ constitué d'un intervalle $J$ de $\mathbb{R}$ inclus
dans $I$ et d'une application $\phi : J \rightarrow E$ de classe $\mathcal{C}^1$ telle que
$\forall t \in J, \phi'(t) = \ell(t).\phi(t) + g(t)$, où l'on note
$\ell(t).x$ à la place de $(\ell(t))(x)$ pour alléger l'écriture. A une telle équation différentielle
linéaire nous associerons l'équation différentielle homogène $y' = \ell(t).y$.
\begin{thm}
L'ensemble $S_H(J)$ des solutions de l'équation
homogène $y' = \ell(t).y$ définies sur un intervalle $J$ est un $K$-espace
vectoriel. On obtient la solution générale sur $J$ de l'équation linéaire
$y' = \ell(t).y + g(t)$ en ajoutant à une solution particulière de cette
équation la solution générale de l'équation homogène.
\end{thm}
\begin{proof}
La fonction nulle est bien évidemment solution de
l'équation homogène et si $\phi_1$ et $\phi_2$ sont deux
solutions définies sur $J$, il est clair que $\alpha \phi_1 + \beta \phi_2$
est encore une solution définie sur $J$ ; donc $S_H$ est bien un $K$-espace vectoriel. Si maintenant $\phi_0$ est une solution sur $J$ de
l'équation linéaire et si $\phi$ est une fonction de classe $\mathcal{C}^1$ de
$J$ dans $E$, alors $\phi$ est solution de l'équation linéaire si et seulement
si $\phi'(t) = \ell(t).\phi(t) + g(t) = \ell(t).\phi(t) + \phi_0'(t) -
\ell(t).\phi_0(t)$, soit encore $(\phi - \phi_0)'(t) = \ell(t).(\phi(t) -
\phi_0(t))$, c'est-à-dire $\phi - \phi_0 \in S_H$ ce qui
montre le résultat.
\end{proof}
On peut également voir ce problème sous forme matricielle. Pour cela
donnons-nous $\mathcal{E}$ une base de $E$, soit $A(t)$ la matrice de $\ell(t)$ dans la base
$\mathcal{E}$, avec $A(t) = (a_{i,j}(t))_{1 \leq i,j \leq n}$ ; soit $B(t)$ le
vecteur colonne des coordonnées de $g(t)$ dans la base $\mathcal{E}$ et $Y(t)$ le
vecteur colonne des coordonnées de la fonction inconnue $y(t)$ dans la
base $\mathcal{E}$. L'équation différentielle linéaire d'ordre 1 vectorielle s'écrit
encore sous la forme $Y' = A(t)Y + B(t)$, ou encore sous forme d'un
système différentiel linéaire
$\begin{pmatrix}
y_1' = a_{1,1}(t)y_1 + \ldots + a_{1,n}(t)y_n + b_1(t) \
\vdots \
y_n' = a_{n,1}(t)y_1 + \ldots + a_{n,n}(t)y_n + b_n(t)
\end{pmatrix}$
les $a_{i,j}$ et les $b_i$ étant des fonctions continues de
$I$ dans le corps de base $K$. Une solution de ce système est alors la
donnée d'un intervalle $J$ de $\mathbb{R}$ inclus dans $I$ et de $n$ fonctions
$\phi_j : J \rightarrow K$ de classe $\mathcal{C}^1$ vérifiant
$\forall t \in J,
\begin{pmatrix}
\phi_1'(t) = a_{1,1}(t)\phi_1(t) + \ldots + a_{1,n}(t)\phi_n(t) + b_1(t) \
\vdots \
\phi_n'(t) = a_{n,1}(t)\phi_1(t) + \ldots + a_{n,n}(t)\phi_n(t) + b_n(t)
\end{pmatrix}$
Bien entendu le système homogène associé est alors le système $Y' = A(t)Y$ ou encore
$\begin{pmatrix}
y_1' = a_{1,1}(t)y_1 + \ldots + a_{1,n}(t)y_n \
\vdots \
y_n' = a_{n,1}(t)y_1 + \ldots + a_{n,n}(t)y_n
\end{pmatrix}$
\subsection{Equation différentielle linéaire scalaire d'ordre 1}
Dans ce subsectione, nous allons obtenir dans ce cas particulier, une
preuve élémentaire du théorème de Cauchy-Lipschitz.
Ici, on a $n = 1$ et donc l'équation différentielle linéaire s'écrit $y' =
a(t)y + b(t)$, où $a$ et $b$ sont deux fonctions continues de $I$ dans le corps
de base $K$ (égal à $\mathbb{R}$ ou $\mathbb{C}$). L'équation homogène associée est alors
l'équation $y' = a(t)y$. C'est cette équation que nous allons d'abord
résoudre. Soit $J$ un intervalle inclus dans $I$ et soit $A$ une primitive de
$a$ sur $I$. Soit $\phi : J \rightarrow K$ de classe $\mathcal{C}^1$. On a alors
\begin{align*}
\forall t \in J, \phi'(t) - a(t)\phi(t) = 0 &\Leftrightarrow \forall t \in J, (\phi'(t) - a(t)\phi(t))e^{-A(t)} = 0 \
&\Leftrightarrow \forall t \in J, (\phi e^{-A})'(t) = 0 \Leftrightarrow \phi e^{-A} \text{ est constante}
\end{align*}
On en déduit que les solutions définies sur $J$ sont les fonctions de la
forme $t \mapsto \lambda e^{A(t)}$. Les solutions
maximales sont donc définies sur $I$ et ce sont les solutions
$(I,t \mapsto \lambda e^{A(t)})$. On constate
qu'elles forment un $K$-espace vectoriel de dimension 1. La solution
maximale vérifiant $y(t_0) = y_0$ est bien entendu
$(I, t \mapsto y_0 e^{\int_{t_0}^t a(u) du})$ ; elle est visiblement
unique. On obtient donc
\begin{thm}
Soit $a : I \rightarrow K$ une application continue. Toute solution
maximale de l'équation homogène $y' = a(t)y$ est définie sur $I$. L'ensemble
de ces solutions maximales est un $K$-espace vectoriel de dimension 1
engendré par la fonction $e^A$ où $A$ est une primitive de $a$ sur
$I$. Pour $t_0 \in I$ et $y_0 \in K$, il existe une unique
solution maximale vérifiant la condition initiale $y(t_0) = y_0$ à savoir
$(I, t \mapsto y_0 e^{\int_{t_0}^t a(u) du})$.
\end{thm}
Pour résoudre l'équation linéaire, faisons le changement de fonction
inconnue $z = ye^{-A}$ soit encore $y = ze^A$. On a
alors $y' = z'e^A + aze^A = z'e^A + ay$
si bien que
$y' = a(t)y + b(t) \Leftrightarrow z'e^{A(t)} = b(t) \Leftrightarrow z' = b(t)e^{-A(t)}$
ce qui conduit à un simple calcul de primitive de la fonction
$be^{-A}$ pour déterminer la fonction inconnue $z$ et donc la
fonction inconnue $y$.
Remarquons que la solution générale de l'équation homogène était écrite
sous la forme $\phi(t) = \lambda e^{A(t)}$ où $\lambda$ est une constante, et qu'à
un changement de notation près (celui de $z$ en $\lambda$), la résolution de
l'équation linéaire se fait en posant $\phi(t) = \lambda(t)e^{A(t)}$,
autrement dit en remplaçant la constante $\lambda$ par une
fonction inconnue $\lambda$. Cette méthode porte le nom de méthode de variation
de la constante. On déduit immédiatement de l'étude précédente le
théorème suivant
\begin{thm}
Soit $a,b : I \rightarrow K$ deux applications continues. Toute
solution maximale de l'équation linéaire $y' = a(t)y + b(t)$ est définie
sur $I$. L'ensemble de ces solutions maximales est une droite affine ayant
pour direction la droite vectorielle des solutions de l'équation
homogène associée. Pour $t_0 \in I$ et $y_0 \in K$, il existe
une unique solution maximale vérifiant la condition initiale
$y(t_0) = y_0$.
\end{thm}
\begin{proof}
La méthode de variation de la constante montre que toute
solution maximale est définie sur $I$. La structure de droite affine
résulte immédiatement du théorème de structure de l'ensemble des
solutions d'une équation différentielle linéaire. Si $\phi_0$ est
une solution particulière, toute solution est du type $\phi(t) =
\phi_0(t) + \lambda e^{A(t)}$ et la condition $\phi(t_0) =
y_0$ fournit immédiatement $\lambda = e^{-A(t_0)}(y_0 - \phi_0(t_0))$.
\end{proof}
\begin{rem}
La condition de continuité des applications $a$ et $b$ est
essentielle pour la validité du résultat. Si $a$ et $b$ ne sont pas
continues, les solutions maximales ne sont plus nécessairement définies
sur $I$, et, pour un problème à condition initiale $y(t_0) = y_0$, soit l'existence soit l'unicité de la solution maximale
peut être prise en défaut. En particulier, les équations différentielles
linéaires se présentent souvent sous forme non normale $\alpha(t)y' + \beta(t)y = \gamma(t)$ et la mise sous forme normale exige la division par $\alpha(t)$. Si la
fonction $\alpha$ peut s'annuler, les fonctions $a(t) = - \frac{\beta(t)}{\alpha(t)}$ et $b(t) = \frac{\gamma(t)}{\alpha(t)}$
ne sont pas nécessairement continues. Dans ce cas, on utilisera la
théorie précédente sur des intervalles maximaux sur lesquels la fonction
$\alpha$ ne s'annule pas, en essayant ensuite éventuellement de recoller les
solutions ainsi obtenues pour obtenir des solutions maximales.
\end{rem}
\begin{example}
Considérons l'équation différentielle $ty' - 2y = 1$.
L'équation homogène associée s'écrit $ty' - 2y = 0$ soit encore sur $]-\infty,0[$ ou $]0,+\infty[$, $y' = \frac{2}{t} y$ qui admet
évidemment pour solution $y(t) = \lambda t^2$. On voit alors que
toutes les fonctions $\phi_{\lambda,\mu} : \mathbb{R} \rightarrow \mathbb{R}$ définies par
$\phi_{\lambda,\mu}(t) = \begin{cases}
\lambda t^2 & \text{si } t > 0 \
0 & \text{si } t = 0 \
\mu t^2 & \text{si } t < 0
\end{cases}$ sont de classe $\mathcal{C}^1$
et solutions de l'équation homogène. En remarquant que
$t \mapsto -\frac{1}{2}$ est
solution particulière de l'équation linéaire, on obtient les solutions
maximales de l'équation linéaire sous la forme
$\phi_{\lambda,\mu}(t) = \begin{cases}
\lambda t^2 - \frac{1}{2} & \text{si } t > 0 \
-\frac{1}{2} & \text{si } t = 0 \
\mu t^2 - \frac{1}{2} & \text{si } t < 0
\end{cases}$
et elles forment un espace affine de dimension 2.
Il n'existe aucune solution vérifiant la condition $y(0) = y_0$
pour $y_0 \neq -\frac{1}{2}$ par contre, il existe une infinité de solutions maximales vérifiant
$y(0) = -\frac{1}{2}$.
\end{example}
\subsection{Théorie de Cauchy-Lipschitz pour les équations linéaires}
\begin{lem}
Soit $E$ un espace vectoriel normé de dimension finie, $I$ un
intervalle de $\mathbb{R}$, $\ell : I \rightarrow L(E)$ continue et $g : I \rightarrow E$ continue. Alors
l'équation différentielle linéaire $y' = \ell(t).y + g(t)$ satisfait à la
condition d'unicité au problème de Cauchy-Lipschitz.
\end{lem}
\begin{proof}
Soit $(I_1,\phi_1)$ et
$(I_2,\phi_2)$ deux solutions de l'équation différentielle
linéaire vérifiant $\phi_1(t_0) = \phi_2(t_0) = y_0$. Alors, si $t \in I_1 \cap I_2$, on a à la fois $\phi_1(t) = y_0 + \int_{t_0}^t (\ell(u).\phi_1(u) + g(u)) du$ et
$\phi_2(t) = y_0 + \int_{t_0}^t (\ell(u).\phi_2(u) + g(u)) du$, d'où, si
$\phi = \phi_1 - \phi_2$, $\phi(t) = \int_{t_0}^t \ell(u).\phi(u) du$. Pour $t \geq t_0$, on a
donc

\begin{align*}
\|\phi(t)\| &\leq \int_{t_0}^t \|\ell(u).\phi(u)\| du \\
&\leq \int_{t_0}^t \|\ell(u)\| \|\phi(u)\| du
\end{align*}

et le lemme de Gronwall (dans le cas où la constante $c$ est nulle)
implique que $\phi$ est nulle sur $I_1 \cap I_2 \cap [t_0,+\infty[$. Une démonstration similaire montre que $\phi$ est
nulle sur $I_1 \cap I_2 \cap ]-\infty,t_0]$, et donc
$\phi_1$ et $\phi_2$ coïncident sur $I_1 \cap I_2$.
\end{proof}

\begin{lem}
Soit $E$ un espace vectoriel normé de dimension finie, $I$ un
intervalle de $\mathbb{R}$, $\ell : I \rightarrow L(E)$ continue et $g : I \rightarrow E$ continue. Alors
l'équation différentielle linéaire $y' = \ell(t).y + g(t)$ satisfait à la
condition d'existence locale au problème de Cauchy-Lipschitz. De
façon plus précise, pour tout $t_0 \in I$,
pour tout $y_0 \in E$ et pour tout segment $J$ tel que $t_0 \in J \subset I$, il existe une solution $(J,\phi)$ de l'équation différentielle
vérifiant $\phi(t_0) = y_0$.
\end{lem}

\begin{proof}
Soit donc $J$ un segment inclus dans $I$ ; l'application $\ell$ est
continue sur le compact $J$, donc bornée et on peut poser $L = \sup_{t \in J} \|\ell(t)\|$.
On définit une suite $(\phi_n)$ d'applications continues de $J$ dans $E$
par $\phi_0(t) = y_0$ et pour $n \geq 0$, $\phi_{n+1}(t) =
y_0 + \int_{t_0}^t (\ell(u).\phi_n(u) + g(u)) du$. En posant $M = \sup_{t \in J} \|\ell(t).y_0 + g(t)\|$, on montre par récurrence sur $n$ que

$\forall n \in \mathbb{N}, \forall t \in J,
\|\phi_{n+1}(t) - \phi_n(t)\| \leq \frac{M}{L} \frac{L^{n+1}|t - t_0|^{n+1}}{(n + 1)!}$

C'est vrai pour $n = 1$ d'après la définition même de $M$ :

\begin{align*}
\|\phi_1(t) - \phi_0(t)\| &= \|\phi_1(t) - y_0\| \\
&= \|\int_{t_0}^t (\ell(u).y_0 + g(u)) du\| \leq M|t - t_0|
\end{align*}

ce qui est bien la formule voulue. Si maintenant l'inégalité est
vérifiée pour $n - 1$, on a alors pour $t \in [t_0,+\infty[ \cap J$

\begin{align*}
\|\phi_{n+1}(t) - \phi_n(t)\| &= \|\int_{t_0}^t (\ell(u).\phi_n(u) - \ell(u).\phi_{n-1}(u)) du\| \\
&\leq \int_{t_0}^t \|\ell(u)\| \|\phi_n(u) - \phi_{n-1}(u)\| du \\
&\leq \int_{t_0}^t L \|\phi_n(u) - \phi_{n-1}(u)\| du \\
&\leq L \int_{t_0}^t \frac{M}{L} \frac{L^n |u - t_0|^n}{n!} du \\
&= \frac{M}{L} \frac{L^{n+1}|t - t_0|^{n+1}}{(n + 1)!}
\end{align*}

Un calcul similaire conduit à la même inégalité pour $t \in ]-\infty,t_0] \cap J$.

Désignons par $\eta$ la longueur du segment $J$. On a donc

$\forall n \in \mathbb{N}, \forall t \in J,
\|\phi_{n+1}(t) - \phi_n(t)\| \leq \frac{M}{L} \frac{L^{n+1}\eta^{n+1}}{(n + 1)!}$

Alors pour $q > p$, on a, $\forall t \in J$

\begin{align*}
\|\phi_q(t) - \phi_p(t)\| &\leq \sum_{n=p}^{q-1} \|\phi_{n+1}(t) - \phi_n(t)\| \\
&\leq \frac{M}{L} \sum_{n=p}^{q-1} \frac{L^{n+1}\eta^{n+1}}{(n + 1)!} \\
&\leq \frac{M}{L} \sum_{n=p}^{+\infty} \frac{L^{n+1}\eta^{n+1}}{(n + 1)!}
\end{align*}

Comme la série $\sum_n \frac{L^{n+1}\eta^{n+1}}{(n+1)!}$ est une série convergente (exponentielle d'un nombre réel), son
reste tend vers $0$ ; étant donné $\epsilon > 0$, il existe $N \in \mathbb{N}$ tel
que $p \geq N \Rightarrow \frac{M}{L} \sum_{n=p}^{+\infty} \frac{L^{n+1}\eta^{n+1}}{(n+1)!} < \epsilon$. Alors

$q > p \geq N \Rightarrow \forall t \in J, \|\phi_q(t) - \phi_p(t)\| < \epsilon$

La suite $(\phi_n)$ vérifie donc le critère de Cauchy uniforme. En
conséquence, elle converge uniformément vers une fonction $\phi : J \rightarrow E$ qui
est elle-même continue.

L'inégalité $\|(\ell(u).\phi(u) + g(u)) - (\ell(u).\phi_n(u) + g(u))\| \leq L\|\phi(u) - \phi_n(u)\|$, montre que la suite
$(\ell(u).\phi_n(u) + g(u))$
converge uniformément vers $\ell(u).\phi(u) + g(u)$ ; ceci nous permet de passer
à la limite sous le signe d'intégration et d'obtenir

\begin{align*} 
y_0 + \int_{t_0}^t (\ell(u).\phi(u) + g(u)) du &= y_0 + \lim_{n \rightarrow +\infty} \int_{t_0}^t (\ell(u).\phi_n(u) + g(u)) du \\
&= \lim_{n \rightarrow +\infty} \phi_{n+1}(t) = \phi(t)
\end{align*}

Comme $\phi$ est continue, ceci montre que $\phi$ est la solution cherchée sur $J$
de l'équation $y' = \ell(t).y + g(t)$ vérifiant $\phi(t_0) = y_0$.
\end{proof}

\begin{thm}
Soit $E$ un espace vectoriel normé de dimension finie, $I$
un intervalle de $\mathbb{R}$, $\ell : I \rightarrow L(E)$ continue et $g : I \rightarrow E$ continue. Alors
toute solution maximale de l'équation différentielle linéaire $y' =
\ell(t).y + g(t)$ est définie sur $I$. Pour tout $t_0 \in I$ et tout
$y_0 \in E$, il existe une et une seule solution $(I,\phi)$ de
l'équation différentielle linéaire $y' = \ell(t).y + g(t)$ vérifiant
$\phi(t_0) = y_0$ ; pour toute solution $(J,\psi)$ de l'équation
différentielle vérifiant $\psi(t_0) = y_0$, on a :

$J \subset I$ et $\psi$ est la restriction de $\phi$ à $J$.
\end{thm}

\begin{proof}
Puisque l'équation différentielle linéaire vérifie les
conditions d'unicité et d'existence locale au problème de
Cauchy-Lipschitz, on sait qu'il existe une unique solution maximale pour
une condition initiale donnée et que cette solution prolonge toutes les
autres solutions vérifiant cette même condition initiale. Mais le lemme
précédent, montre que l'intervalle de définition de cette solution doit
contenir tout segment $J$ contenant $t_0$ et inclus dans $I$ ; ce ne
peut donc être que $I$ lui-même. Le théorème en résulte.
\end{proof}

\subsection{Structure des solutions de l'équation homogène}

\begin{thm}
Soit $E$ un espace vectoriel normé de dimension finie, $I$
un intervalle de $\mathbb{R}$, $\ell : I \rightarrow L(E)$ continue. L'ensemble $S_H$ des
solutions définies sur $I$ de l'équation différentielle homogène $y' =
\ell(t).y$ est un espace vectoriel de dimension finie égale à
$\dim E$. Plus précisément, pour tout
$t_0 \in I$, l'application
$\phi \mapsto \phi(t_0)$ est un isomorphisme
d'espaces vectoriels de $S_H$ sur $E$.
\end{thm}

\begin{proof}
On sait déjà que $S_H$ est un espace vectoriel.
L'application $S_H \rightarrow E$,
$\phi \mapsto \phi(t_0)$ est visiblement linéaire et
le théorème de Cauchy-Lipschitz assure que cette application est
bijective (puisque pour tout $y_0 \in E$, il existe une unique
solution définie sur $I$ vérifiant $\phi(t_0) = y_0$).
L'application en question est donc un isomorphisme d'espaces vectoriels
et les deux espaces vectoriels ont donc même dimension.
\end{proof}

\begin{cor}
Soit $E$ un espace vectoriel normé de dimension finie, $I$
un intervalle de $\mathbb{R}$, $\ell : I \rightarrow L(E)$ continue. Soit
$(\phi_1,\ldots,\phi_k)$
des solutions définies sur $I$ de l'équation différentielle $y' = \ell(t).y$.
Alors,

$\forall t_0 \in I,
\text{rang}(\phi_1,\ldots,\phi_k) = \text{rang}(\phi_1(t_0),\ldots,\phi_k(t_0))$
\end{cor}

\begin{proof}
En effet, un isomorphisme conserve le rang.
\end{proof}

\begin{rem}
En particulier, pour $k = 1$, une solution non
identiquement nulle de l'équation homogène $y' = \ell(t).y$ ne peut pas
s'annuler.
\end{rem}

\subsection{Méthode de variation des constantes}

Soit $E$ un espace vectoriel normé de dimension finie $n$, $I$ un intervalle
de $\mathbb{R}$, $\ell : I \rightarrow L(E)$ continue et $g : I \rightarrow E$ continue. Soit $\mathcal{E}$ une base de $E$,
$A(t)$ la matrice de $\ell(t)$ dans la base $\mathcal{E}$, avec $A(t) =
(a_{i,j}(t))_{1 \leq i,j \leq n}$ ; soit $B(t)$ le vecteur colonne des
coordonnées de $g(t)$ dans la base $\mathcal{E}$ et $Y(t)$ le vecteur colonne des
coordonnées de la fonction inconnue $y(t)$ dans la base $\mathcal{E}$. L'équation
différentielle linéaire d'ordre 1 vectorielle s'écrit encore sous la
forme $Y' = A(t)Y + B(t)$ et l'équation homogène associée s'écrit $Y' =
A(t)Y$.

On sait que l'espace vectoriel $S_H$ des solutions de l'équation
homogène est de dimension $n$. Supposons connue une base
$(\phi_1,\ldots,\phi_n)$
et soit $\Phi_j(t) = \begin{pmatrix} \phi_{1,j}(t) \\ \vdots \\ \phi_{n,j}(t) \end{pmatrix}$ le vecteur
colonne des coordonnées de $\phi_j(t)$ dans la base $E$. On a alors
$\Phi_j'(t) = A(t)\Phi_j(t)$ ce qui se traduit encore par

$\forall i,j, \phi_{i,j}'(t) = \sum_{k=1}^n a_{i,k}(t)\phi_{k,j}(t)$

ou encore, en introduisant la matrice carrée $R(t) =
(\phi_{i,j}(t))_{1 \leq i,j \leq n}$, par $R'(t) = A(t)R(t)$.

Remarquons que cette matrice a pour vecteurs colonnes les vecteurs
$\Phi_j(t)$ ; or on sait que $\forall t \in J$,
$\text{rang}(\Phi_1(t),\ldots,\Phi_n(t)) = \text{rang}(\Phi_1,\ldots,\Phi_n) = \text{rang}(\phi_1,\ldots,\phi_n) = n$. On en déduit que cette matrice est inversible. Faisons alors le
changement de fonction inconnue $Z = R(t)^{-1}Y$, autrement dit
$Y = R(t)Z$. L'application $t \mapsto R(t)$ étant
visiblement de classe $\mathcal{C}^1$, il en est de même de
$t \mapsto R(t)^{-1}$, ce qui rend valide ce
changement de fonction inconnue. On a alors $Y' = R'(t)Z + R(t)Z' =
A(t)R(t)Z + R(t)Z' = A(t)Y + R(t)Z'$, si bien que

\begin{align*} 
Y' = A(t)Y + B(t) &\Leftrightarrow R(t)Z' = B(t) \\
&\Leftrightarrow Z' = R(t)^{-1}B(t)
\end{align*}

et la résolution de l'équation linéaire se ramène à un simple calcul de
primitive de la fonction vectorielle
$t \mapsto R(t)^{-1}B(t)$ sur l'intervalle $I$.

Voyons une autre interprétation de cette méthode. Puisque
$(\Phi_1,\ldots,\Phi_n)$
est une base de l'espace des solutions du système homogène $Y' = A(t)Y$,
toute solution s'écrit de manière unique sous la forme $Y(t) =
\lambda_1\Phi_1(t) + \ldots + \lambda_n\Phi_n(t)$,
où
$\lambda_1,\ldots,\lambda_n$
sont des constantes. Posons alors $Z(t) = \begin{pmatrix} \lambda_1(t) \\ \vdots \\ \lambda_n(t) \end{pmatrix}$. Le
changement de fonction inconnue $Y = R(t)Z$ revient à poser
$y_i(t) = \sum_{j=1}^n \phi_{i,j}(t)\lambda_j(t)$, soit encore $Y(t) = \sum_{j=1}^n \lambda_j(t)\Phi_j(t) = \lambda_1(t)\Phi_1(t) + \ldots + \lambda_n(t)\Phi_n(t)$ ; ce changement de fonction inconnue
consiste donc à substituer dans la solution générale de l'équation
homogène, aux constantes
$\lambda_1,\ldots,\lambda_n$
des fonctions de classe $\mathcal{C}^1$
$\lambda_1(t),\ldots,\lambda_n(t)$,
d'où le nom de méthode de variation des constantes. La résolution se
fait alors en écrivant

\begin{align*} 
Y'(t) &= \lambda_1'(t)\Phi_1(t) + \ldots + \lambda_n'(t)\Phi_n(t) \\
&\quad + \lambda_1(t)\Phi_1'(t) + \ldots + \lambda_n(t)\Phi_n'(t) \\
&= \lambda_1'(t)\Phi_1(t) + \ldots + \lambda_n'(t)\Phi_n(t) \\
&\quad + \lambda_1(t)A(t)\Phi_1(t) + \ldots + \lambda_n(t)A(t)\Phi_n(t) \\
&= \lambda_1'(t)\Phi_1(t) + \ldots + \lambda_n'(t)\Phi_n(t) \\
&\quad + A(t)(\lambda_1(t)\Phi_1(t) + \ldots + \lambda_n(t)\Phi_n(t)) \\
&= \lambda_1'(t)\Phi_1(t) + \ldots + \lambda_n'(t)\Phi_n(t) + A(t)Y(t)
\end{align*}

si bien que

\begin{align*} 
Y'(t) = A(t)Y(t) + B(t) &\Leftrightarrow \lambda_1'(t)\Phi_1(t) + \ldots + \lambda_n'(t)\Phi_n(t) = B(t)
\end{align*}

C'est un système de Cramer en les inconnues
$\lambda_1'(t),\ldots,\lambda_n'(t)$, la matrice de ce système étant la matrice inversible $R(t)$ ; la
résolution de ce système permet alors de déterminer les fonctions
$\lambda_j'$ ; on détermine ensuite les fonctions $\lambda_j$ par $n$
calculs de primitives de fonctions à valeurs dans le corps de base $K$.

\subsection{Systèmes différentiels à coefficients constants}

C'est le cas où l'application $t \mapsto \ell(t)$ est
constante. On préférera dans ce cas l'interprétation matricielle en
posant $\forall t \in I, A(t) = A$ si bien que le système
linéaire s'écrit sous la forme $Y' = AY + B(t)$, le système homogène
associé étant le système $Y' = AY$.

Rappelons à ce propos un théorème démontré dans le chapitre sur les
séries entières qui permet théoriquement de résoudre l'équation homogène

\begin{thm}
Soit $Y_0 \in M_K(n,1)$. L'unique
solution du système homogène $Y' = AY$ vérifiant $Y(0) = Y_0$
est l'application
$t \mapsto \exp(tA)Y_0$.
\end{thm}

\begin{proof}
Cette application convient évidemment puisque $\frac{d}{dt}(\exp(tA)Y_0) = A\exp(tA)Y_0$. Soit
$t \mapsto Y(t)$ une autre solution et soit $Z(t) = \exp(-tA)Y(t)$. On a $Z'(t) = -\exp(-tA)AY(t) + \exp(-tA)Y'(t) = \exp(-tA)(Y'(t) - AY(t)) = 0$. On en déduit que $Z$ est constante égale à $Z(0)$. Mais $Z(0) = Y_0$. On a donc $Z(t) = Y_0$ soit encore $Y(t) = \exp(tA)Y_0$.
\end{proof}

\begin{rem}
Le même changement de fonction inconnue $Y = \exp(tA)Z$ permet d'ailleurs de résoudre
théoriquement l'équation linéaire $Y' = AY + B(t)$, puisque l'on a alors
$Y' = A\exp(tA)Z + \exp(tA)Z' = AY + \exp(tA)Z'$ et donc

\begin{align*} 
Y' = AY + B(t) &\Leftrightarrow \exp(tA)Z'(t) = B(t) \\
&\Leftrightarrow Z'(t) = \exp(-tA)B(t)
\end{align*}

ce qui ramène le problème de la résolution de l'équation linéaire à
celui d'un calcul de primitive de la fonction
$t \mapsto \exp(-tA)B(t)$.
\end{rem}

En fait la méthode précédente bute sur le problème non évident du calcul
de l'exponentielle $\exp(tA)$, si bien que, dans
la pratique, d'autres méthodes peuvent être préférées.

En premier lieu, supposons que la matrice $A$ est diagonalisable et soit
$(V_1,\ldots,V_n)$ une base de vecteurs propres de $A$ associés aux valeurs
propres
$\lambda_1,\ldots,\lambda_n$.
Posons $\Phi_i : t \mapsto e^{\lambda_i t}V_i$. On
a alors $\Phi_i'(t) = \lambda_i e^{\lambda_i t}V_i = e^{\lambda_i t}AV_i = A\Phi_i(t)$
si bien que
$\Phi_1,\ldots,\Phi_n$
sont solutions de l'équation homogène $Y' = AY$. Mais, comme
$(\Phi_1(0),\ldots,\Phi_n(0)) = (V_1,\ldots,V_n)$ est une famille libre, la famille
$(\Phi_1,\ldots,\Phi_n)$
est également une famille libre. Comme l'espace vectoriel des solutions
de l'équation homogène est de dimension $n$, cette famille en est une base
et donc la solution générale de l'équation homogène est de la forme

$Y(t) = \alpha_1 e^{\lambda_1 t}V_1 + \ldots + \alpha_n e^{\lambda_n t}V_n$

On peut ensuite résoudre l'équation linéaire en faisant varier les
constantes
$\alpha_1,\ldots,\alpha_n$.

Dans le cas général, soit $P$ une matrice inversible et faisons le
changement de fonction inconnue $Y = PZ$. On a alors

\begin{align*} 
Y' = AY + B(t) &\Leftrightarrow PZ' = APZ + B(t) \\
&\Leftrightarrow Z' = P^{-1}AP + P^{-1}B(t)
\end{align*}

Quitte à passer sur le corps des complexes, on peut par exemple
s'arranger pour que la matrice $P^{-1}AP$ soit triangulaire
supérieure. En notant $(\alpha_{i,j})$ cette matrice et
$P^{-1}B(t) = \begin{pmatrix} \beta_1(t) \\ \vdots \\ \beta_n(t) \end{pmatrix}$, ceci conduit à un système différentiel

$\begin{cases}
z_1' = \alpha_{1,1}z_1 + \alpha_{1,2}z_2 + \ldots + \alpha_{1,n}z_n + \beta_1(t) \\
\vdots \\
z_{n-1}' = \alpha_{n-1,n-1}z_{n-1} + \alpha_{n-1,n}z_n + \beta_{n-1}(t) \\
z_n' = \alpha_{n,n}z_n + \beta_n(t)
\end{cases}$

qui se résout en cascade à partir du bas en résolvant $n$ équations
différentielles linéaires scalaires d'ordre 1 : lorsque
$z_n,\ldots,z_{i+1}$
sont connues, $z_i$ est solution de l'équation différentielle
linéaire scalaire d'ordre 1

$z_i' = \alpha_{i,i}z_i + \sum_{k=i+1}^n \alpha_{i,k}z_k(t) + \beta_i(t)$

Ceci permet un calcul de $Z$ et donc de $Y$.